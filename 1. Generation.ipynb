{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egzIVY7VioI2"
      },
      "source": [
        "# Connecting and Uploading"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading Last Datasets:\n",
        "import pandas as pd\n",
        "dataset_5A = pd.read_pickle(\"Dataset (5 Authors).pkl\")"
      ],
      "metadata": {
        "id": "hPcbIsEnBniL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kshYIePGvZji"
      },
      "outputs": [],
      "source": [
        "# taging:\n",
        "\n",
        "dataset_5A['Merged'] = \"<\" + dataset_5A['Label_(number)'].astype(str) + \"> \" + dataset_5A['Sentence'] + \" <end>\"# Sentence Merge with their Label\n",
        "\n",
        "display(dataset_5A.head(3))\n",
        "print()\n",
        "\n",
        "#Number of sample in each group:\n",
        "\n",
        "num_samples = dataset_5A['Author'].value_counts()\n",
        "\n",
        "display(num_samples)\n",
        "\n",
        "print(\"\\nSum = \", num_samples.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBhrfyc6Iend"
      },
      "outputs": [],
      "source": [
        "# Example:\n",
        "print(dataset_5A[\"Sentence\"][10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibasclIDmUV2"
      },
      "source": [
        "# Library Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEAsg39hnQ1-"
      },
      "outputs": [],
      "source": [
        "#!pip install requests==2.31.0\n",
        "!pip install requests==2.32.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xa6qGs5BnO6T"
      },
      "outputs": [],
      "source": [
        "#!pip install pyarrow==14.0.1\n",
        "!pip install -U pyarrow==19.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oosOpy1GnW5F"
      },
      "outputs": [],
      "source": [
        "!pip install datasets==2.10.0 #\"datasets>=2.19,<3.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibfAowmonQ4x"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwF-2pntnQ79"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Bxjxe7LnWzJ"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49sZKshanW2K"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOf45tBYnW8K"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==2.0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P726EILYnW-u"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KL2dEmWBpmRE"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyeFdbCQ8_Se"
      },
      "source": [
        "# Generator (GPT-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3EtjInGCNHM"
      },
      "source": [
        "## ---- Configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vmmot1_9vrxZ"
      },
      "outputs": [],
      "source": [
        "dataset_text = dataset_5A[['Merged']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzhpx4Y2vuFe"
      },
      "outputs": [],
      "source": [
        "dataset_text = dataset_text.rename(columns={'Merged': 'text'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Fto7notjVTQ"
      },
      "outputs": [],
      "source": [
        "# <0> = Charles Dickens\n",
        "# <1> = Jane Austen\n",
        "# <2> = Mark Twain\n",
        "# <3> = Louisa May Alcott\n",
        "# <4> = Herman Melville"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0Tda7DIHEbL"
      },
      "outputs": [],
      "source": [
        "dataset_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IY98yCSB8_Sf"
      },
      "outputs": [],
      "source": [
        "# Example:\n",
        "dataset_text[\"text\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xcRuLOl8_Sg"
      },
      "outputs": [],
      "source": [
        "len(dataset_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChvpysjazG6C"
      },
      "outputs": [],
      "source": [
        "#pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqg0ZVjLzTAq"
      },
      "outputs": [],
      "source": [
        "#pip install evaluate torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4G2DzI40e8J"
      },
      "outputs": [],
      "source": [
        "#pip install --upgrade pyarrow evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1DFXjfn5VDG"
      },
      "outputs": [],
      "source": [
        "#pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yePefoI3Vy6"
      },
      "outputs": [],
      "source": [
        "#!pip install datasets==2.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wu9KlIZ-38o"
      },
      "outputs": [],
      "source": [
        "import pyarrow as pa\n",
        "print(pa.__version__)  #pip install \"pyarrow<21\"\n",
        "print(hasattr(pa, \"PyExtensionType\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uEM8SSd7AZh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import Dataset as HFDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oDOE3QjGA2F"
      },
      "outputs": [],
      "source": [
        "#pip install numpy==1.26.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ykRSSlKFdm2"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPTNeoForCausalLM, Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLGIuMdnBJVM"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained model and tokenizer\n",
        "model_name = 'EleutherAI/gpt-neo-1.3B'\n",
        "#model_name = 'EleutherAI/gpt-neo-125m'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wole7bl39q4I"
      },
      "outputs": [],
      "source": [
        "# Add a padding token if it does not exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ni89vfEDMlZn"
      },
      "outputs": [],
      "source": [
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    encoding = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
        "    encoding['labels'] = encoding['input_ids'].copy()\n",
        "    return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ly9ksV_8_Sg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert pandas DataFrame to Hugging Face Dataset\n",
        "hf_dataset_5A = HFDataset.from_pandas(dataset_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOiNj2haHhwL"
      },
      "outputs": [],
      "source": [
        "hf_dataset_5A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZjoMkObCShd"
      },
      "source": [
        "## ---- Training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI3TsAqSHlKi"
      },
      "outputs": [],
      "source": [
        "# Apply the tokenize function to the dataset\n",
        "tokenized_datasets = hf_dataset_5A.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpu-eU43gX7i"
      },
      "outputs": [],
      "source": [
        "# Split dataset into train and eval\n",
        "train_size = int(0.8 * len(tokenized_datasets))\n",
        "train_dataset = tokenized_datasets.select(range(train_size))\n",
        "eval_dataset = tokenized_datasets.select(range(train_size, len(tokenized_datasets)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wWOscfLpJey"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "base_dir = '/stylometry'\n",
        "run_name             = \"writing_style_v1\"\n",
        "current_date = datetime.now().strftime(\"%Y.%m.%d\")\n",
        "\n",
        "# Define save paths in Google Drive with date\n",
        "drive_model_path     = f'{base_dir}/fine_tuned_model_gptNeo_1.3B/saved_model_{current_date}'\n",
        "drive_tokenizer_path = f'{base_dir}/fine_tuned_model_gptNeo_1.3B/saved_tokenizer_{current_date}'\n",
        "output_dir           = f'{base_dir}/fine_tuned_model_gptNeo_1.3B/results_{current_date}'\n",
        "logging_dir          = f'{base_dir}/fine_tuned_model_gptNeo_1.3B/logs_{current_date}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLD0OAbbseqX"
      },
      "outputs": [],
      "source": [
        "print(run_name,\":\")\n",
        "print(drive_model_path)\n",
        "print(drive_tokenizer_path)\n",
        "print(output_dir)\n",
        "print(logging_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vpe5FdWjit2"
      },
      "outputs": [],
      "source": [
        "#pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SlQUq_ErcC1"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkrlvQCerbou"
      },
      "outputs": [],
      "source": [
        "#!WANDB_START_METHOD=thread"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leZARoT0rj3A"
      },
      "outputs": [],
      "source": [
        "#!WANDB_HTTP_TIMEOUT=300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9voeLIQpz2n"
      },
      "outputs": [],
      "source": [
        "# Initialize the wandb session\n",
        "wandb.init(project=run_name, entity=\"niu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vA5D7-lSp617"
      },
      "outputs": [],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHGp3MntIBWY"
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    run_name = run_name,               # name for run\n",
        "    output_dir = output_dir,           # output directory\n",
        "    num_train_epochs = 3,             # number of training epochs (10)\n",
        "    per_device_train_batch_size = 16,  # batch size for training\n",
        "    per_device_eval_batch_size = 16,   # batch size for evaluation\n",
        "    warmup_steps = 1000,               # number of warmup steps for learning rate scheduler\n",
        "    weight_decay = 0.01,               # strength of weight decay\n",
        "    logging_dir = logging_dir,         # directory for storing logs\n",
        "    report_to=\"wandb\",                 # report metrics to wandb\n",
        "    save_steps = 2000,                 # save checkpoint every 2000 steps\n",
        "    save_total_limit = 10,             # keep only the last 5 checkpoints\n",
        "    save_strategy = \"steps\",           # save based on steps (other option: \"epoch\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g59BL48-mAu"
      },
      "outputs": [],
      "source": [
        "# Use Trainer to fine-tune the model\n",
        "trainer = Trainer(\n",
        "    model         = model,\n",
        "    args          = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset  = eval_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLVGuJMiMhby"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
        "from torchinfo import summary\n",
        "\n",
        "# Check if CUDA is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the device\n",
        "model.to(device)\n",
        "\n",
        "# Tokenize a sample input and move to the same device\n",
        "input_text = \"Hello, this is a test input for model summary.\"\n",
        "input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n",
        "\n",
        "# Print the detailed summary\n",
        "summary(model, input_data=input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlpKlKvIIFEY"
      },
      "outputs": [],
      "source": [
        "# Fine-tune the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVyNQFHb2M-D"
      },
      "outputs": [],
      "source": [
        "# Saving ===================================================\n",
        "model.save_pretrained(drive_model_path)\n",
        "tokenizer.save_pretrained(drive_tokenizer_path)\n",
        "\n",
        "print(f\"Model saved to: {drive_model_path}\")\n",
        "print(f\"Tokenizer saved to: {drive_tokenizer_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PI3iYUwu3hNT"
      },
      "outputs": [],
      "source": [
        "# Ensure the padding token is set if it was added\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP-7PCKKCXK6"
      },
      "source": [
        "## ---- Loading:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRbZiupl_qAj"
      },
      "outputs": [],
      "source": [
        "drive_model_path     = 'fine_tuned_model_gptNeo_1.3B/saved_model_2025.01.03'\n",
        "drive_tokenizer_path = 'fine_tuned_model_gptNeo_1.3B/saved_tokenizer_2025.01.03'\n",
        "output_dir           = 'fine_tuned_model_gptNeo_1.3B/results_2025.01.03'\n",
        "logging_dir          = 'fine_tuned_model_gptNeo_1.3B/logs_2025.01.03'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1V3lfCb9Hj-"
      },
      "outputs": [],
      "source": [
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = GPTNeoForCausalLM.from_pretrained(drive_model_path)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(drive_tokenizer_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0tO7H_K3khJ"
      },
      "outputs": [],
      "source": [
        "# Generate text\n",
        "def generate_text(input_text, model, tokenizer, max_length=50, num_return_sequences=1):\n",
        "    inputs = tokenizer.encode(input_text, return_tensors='pt')\n",
        "    attention_mask = torch.ones(inputs.shape, dtype=torch.long)\n",
        "    outputs = model.generate(inputs,\n",
        "                             max_length=max_length,\n",
        "                             num_return_sequences=num_return_sequences,\n",
        "                             pad_token_id=tokenizer.eos_token_id,\n",
        "                             attention_mask=attention_mask)\n",
        "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "    return generated_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDShx6znEssC"
      },
      "outputs": [],
      "source": [
        "#Labels of Authors:\n",
        "\n",
        "tages = [\n",
        "         \"<0> \",  # Charles Dickens\n",
        "         \"<1> \",  # Jane Austen\n",
        "         \"<2> \",  # Mark Twain\n",
        "         \"<3> \",  # Louisa May Alcott\n",
        "         \"<4> \"   # Herman Melville\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFL4txAz3tDm"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "\n",
        "start_with = \"Hi\"\n",
        "\n",
        "for tag in tages:\n",
        "    input_text = tag + start_with\n",
        "    generated_texts = generate_text(input_text, model, tokenizer)\n",
        "    print(generated_texts[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Qldsfpye1__"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "grouped = dataset_5A.groupby('Label_(number)')\n",
        "\n",
        "def sample_and_extract(group):\n",
        "  sampled_sentences = group.sample(n=1000)  # Randomly select 1000 sentences form each author\n",
        "  first_words = sampled_sentences['Sentence'].apply(lambda x: x.split()[0])  # Extract first word\n",
        "  # Create a new DataFrame from the extracted words and labels\n",
        "  result_df = pd.DataFrame({'First_Word': first_words, 'Label': group.name})\n",
        "  return result_df\n",
        "\n",
        "randome_start_words_gpt3 = grouped.apply(sample_and_extract, include_groups=False)\n",
        "randome_start_words_gpt3.reset_index(inplace=True) #ungrouping\n",
        "randome_start_words_gpt3.drop(['Label_(number)','level_1'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fDH7zbUFwp-"
      },
      "outputs": [],
      "source": [
        "randome_start_words_gpt3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyuinJrBa3V7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device).eval()\n",
        "\n",
        "# Build prompts robustly\n",
        "prompts = [f\"{tag}{s.split()[0]}\"\n",
        "           for s in randome_start_words_gpt3[\"First_Word\"].astype(str)\n",
        "           for tag in tages]\n",
        "\n",
        "batch_size = 64        # you tested 64 OK on A100\n",
        "max_new_tokens = 64\n",
        "\n",
        "generated = []\n",
        "with torch.inference_mode(), torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
        "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Generating (batched)\", unit=\"batch\"):\n",
        "        batch = prompts[i:i+batch_size]\n",
        "        enc = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)  # <<< move to CUDA\n",
        "        out = model.generate(\n",
        "            **enc,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "        texts = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
        "        generated.extend(texts)\n",
        "\n",
        "generated_texts_gpt3 = pd.DataFrame({\"Text\": generated})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WucelGUleI1C"
      },
      "outputs": [],
      "source": [
        "generated_texts_gpt3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syhoS09OCRdV"
      },
      "outputs": [],
      "source": [
        "for text in generated_texts_gpt3[\"Text\"].head(50):\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ou3fWZPwwbVv"
      },
      "outputs": [],
      "source": [
        "# Save to CSV (without index column)\n",
        "generated_texts_gpt3.to_csv(\"generated_texts_gpt3.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HrhLftUKdKl"
      },
      "source": [
        "#  Generator (GPT-3 + LoRA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbBQPeG8MLOT"
      },
      "source": [
        "## ---- Configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIhDvLWTKjsJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPTNeoForCausalLM, Trainer, TrainingArguments\n",
        "from datasets import Dataset as HFDataset\n",
        "from datetime import datetime\n",
        "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
        "import pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1224Ct-Me6_"
      },
      "outputs": [],
      "source": [
        "#Number of sample in each group:\n",
        "\n",
        "num_samples = dataset_5A['Author'].value_counts()\n",
        "\n",
        "display(num_samples)\n",
        "\n",
        "print(\"\\nSum = \", num_samples.sum())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNX6qpdWPqJo"
      },
      "outputs": [],
      "source": [
        "# Prepare dataset for fine-tuning\n",
        "dataset_text = dataset_5A[['Merged']].rename(columns={'Merged': 'text'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oz4oFhxDMTN1"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained model and tokenizer\n",
        "model_name = 'EleutherAI/gpt-neo-1.3B'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLpdB-WOQCiD"
      },
      "outputs": [],
      "source": [
        "# Add a padding token if it does not exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZd4mJbyQYlW"
      },
      "outputs": [],
      "source": [
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    encoding = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
        "    encoding['labels'] = encoding['input_ids'].copy()\n",
        "    return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrl1smygRfJI"
      },
      "outputs": [],
      "source": [
        "# Convert pandas DataFrame to Hugging Face Dataset\n",
        "hf_dataset_5A = HFDataset.from_pandas(dataset_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrcTS3ZwRiDK"
      },
      "outputs": [],
      "source": [
        "# Apply tokenization\n",
        "tokenized_datasets = hf_dataset_5A.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpZyrkVBRu5w"
      },
      "source": [
        "## ---- LoRA Configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwCCf4dbRp-D"
      },
      "outputs": [],
      "source": [
        "# Define LoRA configuration\n",
        "lora_config = LoraConfig( task_type=TaskType.CAUSAL_LM,\n",
        "                          r=8,  # Rank of the update matrices\n",
        "                          lora_alpha=32,  # Alpha parameter for LoRA scaling\n",
        "                          lora_dropout=0.1,  # Dropout probability for LoRA layers\n",
        "                          bias=\"none\",  # Don't train bias parameters\n",
        "                          target_modules=[\"attn.attention.q_proj\", \"attn.attention.v_proj\", \"attn.attention.k_proj\", \"attn.attention.out_proj\"]  # Target attention and MLP modules\n",
        "                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYMMvVENSEUq"
      },
      "outputs": [],
      "source": [
        "# Prepare the model with LoRA adapters\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bk3jIXyUSaAw"
      },
      "outputs": [],
      "source": [
        "# Print trainable parameters info\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_NoSr86SQUZ"
      },
      "source": [
        "## ---- Training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbkqEDfRRjxE"
      },
      "outputs": [],
      "source": [
        "# Split dataset into train and eval\n",
        "train_size = int(0.8 * len(tokenized_datasets))\n",
        "train_dataset = tokenized_datasets.select(range(train_size))\n",
        "eval_dataset = tokenized_datasets.select(range(train_size, len(tokenized_datasets)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKjX4MvMSgiR"
      },
      "outputs": [],
      "source": [
        "# Define paths and filenames\n",
        "base_dir = 'stylometry' # Local directory for saving\n",
        "run_name             = \"writing_style_lora_v1\"\n",
        "current_date = datetime.now().strftime(\"%Y.%m.%d\")\n",
        "\n",
        "# Define save paths in Google Drive with date\n",
        "lora_model_path     = f'{base_dir}/fine_tuned_model_gptNeo_1.3B_lora/saved_model_{current_date}'\n",
        "drive_tokenizer_path = f'{base_dir}/fine_tuned_model_gptNeo_1.3B_lora/saved_tokenizer_{current_date}'\n",
        "output_dir           = f'{base_dir}/fine_tuned_model_gptNeo_1.3B_lora/results_{current_date}'\n",
        "logging_dir          = f'{base_dir}/fine_tuned_model_gptNeo_1.3B_lora/logs_{current_date}'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWwWneqzTRO4"
      },
      "outputs": [],
      "source": [
        "print(run_name,\":\")\n",
        "print(lora_model_path)\n",
        "print(drive_tokenizer_path)\n",
        "print(output_dir)\n",
        "print(logging_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_THGavWUjFs"
      },
      "outputs": [],
      "source": [
        "#pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l60rbTNMUjFs"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9TCDnK9UjFs"
      },
      "outputs": [],
      "source": [
        "#!WANDB_START_METHOD=thread"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETeAeljrUjFt"
      },
      "outputs": [],
      "source": [
        "#!WANDB_HTTP_TIMEOUT=300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-uqHngeUjFt"
      },
      "outputs": [],
      "source": [
        "# Initialize the wandb session\n",
        "wandb.init(project=run_name, entity=\"niu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdG_nl8xUjFu"
      },
      "outputs": [],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inCSFzqZUMsP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define training arguments - reduced batch size for memory efficiency\n",
        "training_args = TrainingArguments(\n",
        "    run_name=run_name,\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=3,  # Keep same as original\n",
        "    per_device_train_batch_size=8,  # Reduced for memory efficiency\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=1000,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=logging_dir,\n",
        "    save_steps=2000,\n",
        "    save_total_limit=5,\n",
        "    save_strategy=\"steps\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=5000,\n",
        "    fp16=False,\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPLbWU19Ufsz"
      },
      "outputs": [],
      "source": [
        "# Set up trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlcwoDToVYlj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
        "from torchinfo import summary\n",
        "\n",
        "# Check if CUDA is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the device\n",
        "model.to(device)\n",
        "\n",
        "# Tokenize a sample input and move to the same device\n",
        "input_text = \"Hello, this is a test input for model summary.\"\n",
        "input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n",
        "\n",
        "# Print the detailed summary\n",
        "summary(model, input_data=input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dILJqpKXVKRQ"
      },
      "outputs": [],
      "source": [
        "# Fine-tune the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cT9xkKogV3SK"
      },
      "outputs": [],
      "source": [
        "# Saving ===================================================\n",
        "model.save_pretrained(lora_model_path)\n",
        "tokenizer.save_pretrained(drive_tokenizer_path)\n",
        "\n",
        "print(f\"Model saved to: {lora_model_path}\")\n",
        "print(f\"Tokenizer saved to: {drive_tokenizer_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjHtvSggWtpM"
      },
      "source": [
        "## ---- Loading:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHVVxQF-WuE9"
      },
      "outputs": [],
      "source": [
        "lora_model_path      = 'fine_tuned_model_gptNeo_1.3B_lora/saved_model_2025.08.09'\n",
        "drive_tokenizer_path = 'fine_tuned_model_gptNeo_1.3B_lora/saved_tokenizer_2025.08.09'\n",
        "output_dir           = 'fine_tuned_model_gptNeo_1.3B_lora/results_2025.08.09'\n",
        "logging_dir          = 'fine_tuned_model_gptNeo_1.3B_lora/logs_2025.08.09'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNly5-Bef9in"
      },
      "outputs": [],
      "source": [
        "#output_dir           = ''\n",
        "#logging_dir          = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl3qgZUlTHzK"
      },
      "outputs": [],
      "source": [
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "# Load the model and tokenizer\n",
        "peft_config = PeftConfig.from_pretrained(lora_model_path)\n",
        "base_model = GPTNeoForCausalLM.from_pretrained(peft_config.base_model_name_or_path)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(drive_tokenizer_path)\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "model = PeftModel.from_pretrained(base_model, lora_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlIEaQ3LTKIB"
      },
      "outputs": [],
      "source": [
        "# Generate text\n",
        "def generate_text(input_text, model, tokenizer, max_length=50, num_return_sequences=1):\n",
        "    inputs = tokenizer.encode(input_text, return_tensors='pt')\n",
        "    attention_mask = torch.ones(inputs.shape, dtype=torch.long)\n",
        "    outputs = model.generate(inputs,\n",
        "                             max_length=max_length,\n",
        "                             num_return_sequences=num_return_sequences,\n",
        "                             pad_token_id=tokenizer.eos_token_id,\n",
        "                             attention_mask=attention_mask)\n",
        "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "    return generated_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6_NArajTLB9"
      },
      "outputs": [],
      "source": [
        "#Labels of Authors:\n",
        "\n",
        "tages = [\n",
        "         \"<0> \",  # Charles Dickens\n",
        "         \"<1> \",  # Jane Austen\n",
        "         \"<2> \",  # Mark Twain\n",
        "         \"<3> \",  # Louisa May Alcott\n",
        "         \"<4> \"   # Herman Melville\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKlX_liLTLEv"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "\n",
        "start_with = \"Hi\"\n",
        "\n",
        "for tag in tages:\n",
        "    input_text = tag + start_with\n",
        "    generated_texts = generate_text(input_text, model, tokenizer)\n",
        "    print(generated_texts[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcWgJIK0TLHa"
      },
      "outputs": [],
      "source": [
        "# \"First word sampleing\" has been removed to have \"randome_start_words_gpt3\" same as GPT-3 FFT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ612ukHTPuW"
      },
      "outputs": [],
      "source": [
        "randome_start_words_gpt3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3tAlGOTmfXf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device).eval()\n",
        "\n",
        "# Build prompts robustly\n",
        "prompts = [f\"{tag}{s.split()[0]}\"\n",
        "           for s in randome_start_words_gpt3[\"First_Word\"].astype(str)\n",
        "           for tag in tages]\n",
        "\n",
        "batch_size = 64        # you tested 64 OK on A100\n",
        "max_new_tokens = 64\n",
        "\n",
        "generated = []\n",
        "with torch.inference_mode(), torch.amp.autocast(\"cuda\", dtype=torch.float16):\n",
        "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Generating (batched)\", unit=\"batch\"):\n",
        "        batch = prompts[i:i+batch_size]\n",
        "        enc = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(device)  # <<< move to CUDA\n",
        "        out = model.generate(\n",
        "            **enc,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "        texts = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
        "        generated.extend(texts)\n",
        "\n",
        "generated_texts_gpt3_lora = pd.DataFrame({\"Text\": generated})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNj1YLwnWuE-"
      },
      "outputs": [],
      "source": [
        "generated_texts_gpt3_lora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MndTi1S2WuE-"
      },
      "outputs": [],
      "source": [
        "for text in generated_texts_gpt3_lora[\"Text\"].head(50):\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jVyzD88whFq"
      },
      "outputs": [],
      "source": [
        "# Save to CSV (without index column)\n",
        "generated_texts_gpt3_lora.to_csv(\"generated_texts_gpt3_lora.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install notebook ipywidgets==8.1.2"
      ],
      "metadata": {
        "id": "6rASer0k5OPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbextension enable --py widgetsnbextension"
      ],
      "metadata": {
        "id": "FJ4HhiLS5P2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m1Q4EyCJ5YHg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "egzIVY7VioI2",
        "ibasclIDmUV2",
        "FyeFdbCQ8_Se",
        "u3EtjInGCNHM",
        "HZjoMkObCShd",
        "OP-7PCKKCXK6",
        "5HrhLftUKdKl",
        "NbBQPeG8MLOT",
        "bpZyrkVBRu5w",
        "y_NoSr86SQUZ"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
