{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egzIVY7VioI2"
      },
      "source": [
        "# Connecting and Uploading"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading Last Datasets:\n",
        "import pandas as pd\n",
        "dataset_5A = pd.read_pickle(\"Dataset (5 Authors).pkl\")"
      ],
      "metadata": {
        "id": "hPcbIsEnBniL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kshYIePGvZji"
      },
      "outputs": [],
      "source": [
        "# taging:\n",
        "\n",
        "dataset_5A['Merged'] = \"<\" + dataset_5A['Label_(number)'].astype(str) + \"> \" + dataset_5A['Sentence'] + \" <end>\"# Sentence Merge with their Label\n",
        "\n",
        "display(dataset_5A.head(3))\n",
        "print()\n",
        "\n",
        "#Number of sample in each group:\n",
        "\n",
        "num_samples = dataset_5A['Author'].value_counts()\n",
        "\n",
        "display(num_samples)\n",
        "\n",
        "print(\"\\nSum = \", num_samples.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBhrfyc6Iend"
      },
      "outputs": [],
      "source": [
        "# Example:\n",
        "print(dataset_5A[\"Sentence\"][10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKWJVVyueWz9"
      },
      "source": [
        "# Evaluator (RoBERTa):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dY3iUswnt1K"
      },
      "source": [
        "## ---- Data Preperation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJZozGC1nt1K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rLpCWXint1K"
      },
      "outputs": [],
      "source": [
        "!pip install transformers dataset ipython-sql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iGlHyWpnt1L"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn16bB_dcXXl"
      },
      "outputs": [],
      "source": [
        "import transformers, datasets, evaluate, pyarrow\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"datasets:\", datasets.__version__)\n",
        "print(\"evaluate:\", evaluate.__version__)\n",
        "print(\"pyarrow:\", pyarrow.__version__)\n",
        "\n",
        "from transformers import Trainer, TrainingArguments, RobertaTokenizer, RobertaForSequenceClassification\n",
        "from datasets import Dataset as HFDataset\n",
        "print(\"Imports OK\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADbUKtVOnt1L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset as HFDataset\n",
        "import evaluate\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKjShlN9nt1M"
      },
      "outputs": [],
      "source": [
        "dataset_5A.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6_7F4clnt1N"
      },
      "outputs": [],
      "source": [
        "display(dataset_5A['Author'].value_counts())\n",
        "\n",
        "print(\"\\nSum = \", dataset_5A['Author'].value_counts().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZhwfW00nt1N"
      },
      "outputs": [],
      "source": [
        "dataset_BERT = dataset_5A[['Sentence', 'Author']].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1l6497noGQ9j"
      },
      "outputs": [],
      "source": [
        "dataset_BERT.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTHcr8B6GBI5"
      },
      "outputs": [],
      "source": [
        "# <0> = Charles Dickens\n",
        "# <1> = Jane Austen\n",
        "# <2> = Mark Twain\n",
        "# <3> = Louisa May Alcott\n",
        "# <4> = Herman Melville"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KAIk3ojH16h"
      },
      "outputs": [],
      "source": [
        "# Mapping from year to label\n",
        "custom_label2id = {\n",
        "    \"Charles Dickens\": 0,\n",
        "    \"Jane Austen\": 1,\n",
        "    \"Mark Twain\": 2,\n",
        "    \"Louisa May Alcott\": 3,\n",
        "    \"Herman Melville\": 4}\n",
        "\n",
        "dataset_BERT['Author'] = dataset_BERT['Author'].map(custom_label2id)\n",
        "\n",
        "dataset_BERT.rename(columns={'Author': 'label'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wb66dBPZGW7O"
      },
      "outputs": [],
      "source": [
        "dataset_BERT.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUgRN2c0nt1O"
      },
      "outputs": [],
      "source": [
        "display(dataset_BERT['label'].value_counts())\n",
        "\n",
        "print(\"\\n\\nTotal Number:\", sum(dataset_BERT['label'].value_counts()),\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qb2LxhcJnt1O"
      },
      "outputs": [],
      "source": [
        "dataset_BERT.rename(columns={'Sentence': 'text'}, inplace=True)\n",
        "dataset_BERT.rename(columns={'label': 'label'}, inplace=True)\n",
        "dataset_BERT[\"label\"] = dataset_BERT[\"label\"].astype(\"int32\")\n",
        "dataset_BERT.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kILHEOaVnt1P"
      },
      "outputs": [],
      "source": [
        "# Check class distribution\n",
        "num_classes = dataset_BERT[\"label\"].nunique()\n",
        "print(f\"Number of unique Authors (classes): {num_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSuM4pgsnt1P"
      },
      "outputs": [],
      "source": [
        "# Split dataset into train and test sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    dataset_BERT[\"text\"].tolist(), dataset_BERT[\"label\"].tolist(), test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9Rx2Emknt1P"
      },
      "source": [
        "## ---- Tokenizing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45g-TT6LV_W0"
      },
      "outputs": [],
      "source": [
        "model_name = \"roberta-large\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYIkbo-Ont1P"
      },
      "outputs": [],
      "source": [
        "# Load RoBERTa tokenizer\n",
        "###tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "max_length = 256\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eu4gaVnnt1Q"
      },
      "outputs": [],
      "source": [
        "# Convert to Hugging Face Dataset format\n",
        "train_data = HFDataset.from_dict({\"text\": train_texts, \"label\": train_labels})\n",
        "test_data  = HFDataset.from_dict({\"text\": test_texts,  \"label\": test_labels })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sj-GtH6gnt1Q"
      },
      "outputs": [],
      "source": [
        "# Tokenize datasets\n",
        "train_data = train_data.map(tokenize_function, batched=True)\n",
        "test_data  = test_data.map(tokenize_function,  batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6onOi8Snt1Q"
      },
      "source": [
        "## ---- Training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haK8Dft3AAlj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    # Some models return (logits, ...) — keep only the first\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = logits[0]\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, preds, average=\"weighted\", zero_division=0\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OzXnj2Jnt1R"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained RoBERTa model for classification\n",
        "\n",
        "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQf2lMsCIp7d"
      },
      "outputs": [],
      "source": [
        "# Define paths and filenames\n",
        "base_dir = '/content/drive/MyDrive/Colab/LLMs Project' # Local directory for saving\n",
        "current_date = datetime.now().strftime(\"%Y.%m.%d\")\n",
        "\n",
        "# Define save paths in Google Drive with date\n",
        "model_path     = f'{base_dir}/roberta_author_classifier/saved_model_{current_date}'\n",
        "tokenizer_path = f'{base_dir}/roberta_author_classifier/saved_tokenizer_{current_date}'\n",
        "output_dir           = f'{base_dir}/roberta_author_classifier/results_{current_date}'\n",
        "logging_dir          = f'{base_dir}/roberta_author_classifier/logs_{current_date}'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amnmT46twFWY"
      },
      "outputs": [],
      "source": [
        "print(model_name,\":\")\n",
        "print(model_path)\n",
        "print(tokenizer_path)\n",
        "print(output_dir)\n",
        "print(logging_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CACEtPwunt1S"
      },
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    gradient_accumulation_steps=2,\n",
        "    gradient_checkpointing=False,     # huge memory saver when True\n",
        "    bf16=True, # A100\n",
        "    tf32=True, # A100\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=logging_dir,\n",
        "    logging_steps=10,\n",
        "    logging_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better = True,\n",
        "    report_to=\"wandb\",\n",
        "    run_name=\"deberta-v3-large (5-authors)\",\n",
        "    optim=\"adamw_torch_fused\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3TaMgzLnt1T"
      },
      "outputs": [],
      "source": [
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_icYm8trnt1T"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ov0qHR7-nt1U"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUZ6BvFEJKvm"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXGi78DaJK2j"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(tokenizer_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcsSZ2ChJLA7"
      },
      "outputs": [],
      "source": [
        "# Evaluate on the test set\n",
        "# Evaluate on the test set\n",
        "results = trainer.evaluate()\n",
        "\n",
        "# Ensure W&B gets plain scalars (not numpy types)\n",
        "wandb.log({k: float(v) for k, v in results.items()})\n",
        "\n",
        "print(f\"Test Accuracy: {results.get('eval_accuracy', float('nan')):.4f}\")\n",
        "print(f\"Test F1:       {results.get('eval_f1', float('nan')):.4f}\")\n",
        "print(f\"Precision:     {results.get('eval_precision', float('nan')):.4f}\")\n",
        "print(f\"Recall:        {results.get('eval_recall', float('nan')):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrWHQX5JJLLX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Get predictions on the test set\n",
        "predictions_output = trainer.predict(test_data)\n",
        "preds = np.argmax(predictions_output.predictions, axis=-1)\n",
        "labels = predictions_output.label_ids\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(labels, preds)\n",
        "\n",
        "# Print or plot the confusion matrix\n",
        "#print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# Define the sorted list of years as labels\n",
        "year_labels = [\"Charles Dickens\",\n",
        "               \"Jane Austen\",\n",
        "               \"Mark Twain\",\n",
        "               \"Louisa May Alcott\",\n",
        "               \"Herman Melville\"]\n",
        "\n",
        "# Optional: use seaborn to plot a nice heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=year_labels, yticklabels=year_labels)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlKbynBgK4rk"
      },
      "source": [
        "## ---- Loading:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyKWeMQfl8TR"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTN5fFpKl1yi"
      },
      "outputs": [],
      "source": [
        "roberta_model_path      = 'roberta_author_classifier/saved_model_2025.08.10'\n",
        "roberta_tokenizer_path  = 'roberta_author_classifier/saved_tokenizer_2025.08.10'\n",
        "roberta_output_dir      = 'roberta_author_classifier/results_2025.08.10'\n",
        "#roberta_logging_dir    = 'roberta_author_classifier/logs_2025.05.07'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-wtJYMAKqGo"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "import torch\n",
        "\n",
        "model = RobertaForSequenceClassification.from_pretrained(roberta_model_path)\n",
        "tokenizer = RobertaTokenizer.from_pretrained(roberta_tokenizer_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A76IhbiUIJOo"
      },
      "source": [
        "# Evaluator (Deberta-v3-Large):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj4DUuUjIJOp"
      },
      "source": [
        "## ---- Data Preperation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsgJnIe6IJOp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAIhbACjIJOp"
      },
      "outputs": [],
      "source": [
        "!pip install transformers dataset ipython-sql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZcnepJPIJOq"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSxqEw-kIJOq"
      },
      "outputs": [],
      "source": [
        "import transformers, datasets, evaluate, pyarrow\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"datasets:\", datasets.__version__)\n",
        "print(\"evaluate:\", evaluate.__version__)\n",
        "print(\"pyarrow:\", pyarrow.__version__)\n",
        "\n",
        "from transformers import Trainer, TrainingArguments, RobertaTokenizer, RobertaForSequenceClassification\n",
        "from datasets import Dataset as HFDataset\n",
        "print(\"Imports OK\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isWzcAbhIJOq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset as HFDataset\n",
        "import evaluate\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS2aPSJ9IJOq"
      },
      "outputs": [],
      "source": [
        "dataset_5A.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEn2lgkfIJOq"
      },
      "outputs": [],
      "source": [
        "display(dataset_5A['Author'].value_counts())\n",
        "\n",
        "print(\"\\nSum = \", dataset_5A['Author'].value_counts().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Km1odJHLIJOq"
      },
      "outputs": [],
      "source": [
        "dataset_BERT = dataset_5A[['Sentence', 'Author']].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BI3rOhBxIJOq"
      },
      "outputs": [],
      "source": [
        "dataset_BERT.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4-0vA9dIJOq"
      },
      "outputs": [],
      "source": [
        "# <0> = Charles Dickens\n",
        "# <1> = Jane Austen\n",
        "# <2> = Mark Twain\n",
        "# <3> = Louisa May Alcott\n",
        "# <4> = Herman Melville"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0s6qLvVIJOq"
      },
      "outputs": [],
      "source": [
        "# Mapping from year to label\n",
        "custom_label2id = {\n",
        "    \"Charles Dickens\": 0,\n",
        "    \"Jane Austen\": 1,\n",
        "    \"Mark Twain\": 2,\n",
        "    \"Louisa May Alcott\": 3,\n",
        "    \"Herman Melville\": 4}\n",
        "\n",
        "dataset_BERT['Author'] = dataset_BERT['Author'].map(custom_label2id)\n",
        "\n",
        "dataset_BERT.rename(columns={'Author': 'label'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phMOzDOLIJOq"
      },
      "outputs": [],
      "source": [
        "dataset_BERT.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQL-6cloIJOq"
      },
      "outputs": [],
      "source": [
        "display(dataset_BERT['label'].value_counts())\n",
        "\n",
        "print(\"\\n\\nTotal Number:\", sum(dataset_BERT['label'].value_counts()),\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HrBphG1IJOq"
      },
      "outputs": [],
      "source": [
        "dataset_BERT.rename(columns={'Sentence': 'text'}, inplace=True)\n",
        "dataset_BERT.rename(columns={'label': 'label'}, inplace=True)\n",
        "dataset_BERT[\"label\"] = dataset_BERT[\"label\"].astype(\"int32\")\n",
        "dataset_BERT.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTK3r-9lIJOr"
      },
      "outputs": [],
      "source": [
        "# Check class distribution\n",
        "num_classes = dataset_BERT[\"label\"].nunique()\n",
        "print(f\"Number of unique Authors (classes): {num_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5BqptmWIJOr"
      },
      "outputs": [],
      "source": [
        "# Split dataset into train and test sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    dataset_BERT[\"text\"].tolist(), dataset_BERT[\"label\"].tolist(), test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"microsoft/deberta-v3-large\""
      ],
      "metadata": {
        "id": "ZaUgnixFC84Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbCHvludIJOr"
      },
      "source": [
        "## ---- Tokenizing:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, torch\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "torch.set_float32_matmul_precision(\"high\")  # enables TF32"
      ],
      "metadata": {
        "id": "IiVSCgCY7TwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "821DRyTVIJOr"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding, Trainer, TrainingArguments,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "\n",
        "model_name = \"microsoft/deberta-v3-large\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gndV8zMWIJOr"
      },
      "outputs": [],
      "source": [
        "# Load RoBERTa tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "max_length = 512 ### 512\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ni5nj5HcIJOr"
      },
      "outputs": [],
      "source": [
        "# Convert to Hugging Face Dataset format\n",
        "train_data = HFDataset.from_dict({\"text\": train_texts, \"label\": train_labels})\n",
        "test_data  = HFDataset.from_dict({\"text\": test_texts,  \"label\": test_labels })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzzqne42IJOr"
      },
      "outputs": [],
      "source": [
        "# Tokenize datasets\n",
        "train_data = train_data.map(tokenize_function, batched=True)\n",
        "test_data  = test_data.map(tokenize_function,  batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIDn7bRFIJOr"
      },
      "source": [
        "## ---- Training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHLrx4AuIJOr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    # Some models return (logits, ...) — keep only the first\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = logits[0]\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, preds, average=\"weighted\", zero_division=0\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyopvuPvIJOs"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n",
        "\n",
        "import numpy as np\n",
        "print(\"Label set (train):\", set(np.unique(train_data[\"label\"])))\n",
        "print(\"Label set (test): \", set(np.unique(test_data[\"label\"])))\n",
        "assert max(train_data[\"label\"] + test_data[\"label\"]) < num_classes, \"Found label >= num_labels\"\n",
        "assert min(train_data[\"label\"] + test_data[\"label\"]) >= 0, \"Found negative label\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVkME5MPIJOs"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained RoBERTa model for classification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_classes,\n",
        ")\n",
        "\n",
        "# Important with gradient checkpointing:\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Use non-reentrant checkpointing if your Transformers supports it (4.38+)\n",
        "if hasattr(model, \"gradient_checkpointing_enable\"):\n",
        "    try:\n",
        "        model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
        "    except TypeError:\n",
        "        # older versions don't accept kwargs; will still enable GC (reentrant)\n",
        "        model.gradient_checkpointing_enable()\n",
        "\n",
        "# Required on some stacks so the first layer gets grads with GC:\n",
        "if hasattr(model, \"enable_input_require_grads\"):\n",
        "    model.enable_input_require_grads()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snpw56ZXIJOs"
      },
      "outputs": [],
      "source": [
        "# Define paths and filenames\n",
        "base_dir = '/content/drive/MyDrive/Colab/LLMs Project' # Local directory for saving\n",
        "current_date = datetime.now().strftime(\"%Y.%m.%d\")\n",
        "\n",
        "# Define save paths in Google Drive with date\n",
        "deberta_model_path     = f'{base_dir}/Deberta_v3_large_author_classifier/saved_model_{current_date}'\n",
        "deberta_tokenizer_path = f'{base_dir}/Deberta_v3_large_author_classifier/saved_tokenizer_{current_date}'\n",
        "deberta_output_dir           = f'{base_dir}/Deberta_v3_large_author_classifier/results_{current_date}'\n",
        "deberta_logging_dir          = f'{base_dir}/Deberta_v3_large_author_classifier/logs_{current_date}'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DU4nCiBIJOs"
      },
      "outputs": [],
      "source": [
        "print(model_name,\":\")\n",
        "print(model_path)\n",
        "print(tokenizer_path)\n",
        "print(output_dir)\n",
        "print(logging_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PQvrNd7IJOs"
      },
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir= deberta_output_dir,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=16,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=False,     # huge memory saver when True\n",
        "    bf16=True, # A100\n",
        "    tf32=True, # A100\n",
        "    num_train_epochs=20,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=deberta_logging_dir,\n",
        "    logging_steps=10,\n",
        "    logging_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better = True,\n",
        "    report_to=\"wandb\",\n",
        "    run_name=\"deberta-v3-large (5-authors)\",\n",
        "    optim=\"adamw_torch_fused\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nv9vbH1VIJOs"
      },
      "outputs": [],
      "source": [
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    data_collator=data_collator,\n",
        "    eval_dataset=test_data,\n",
        "    processing_class=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(2)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdVkg7X0IJOs"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEvLozEHIJOs"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "MYFTeB4t6gXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xaof345FIJOs"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(tokenizer_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAaC6mm3IJOs"
      },
      "outputs": [],
      "source": [
        "# Evaluate on the test set\n",
        "# Evaluate on the test set\n",
        "results = trainer.evaluate()\n",
        "\n",
        "# Ensure W&B gets plain scalars (not numpy types)\n",
        "wandb.log({k: float(v) for k, v in results.items()})\n",
        "\n",
        "print(f\"Test Accuracy: {results.get('eval_accuracy', float('nan')):.4f}\")\n",
        "print(f\"Test F1:       {results.get('eval_f1', float('nan')):.4f}\")\n",
        "print(f\"Precision:     {results.get('eval_precision', float('nan')):.4f}\")\n",
        "print(f\"Recall:        {results.get('eval_recall', float('nan')):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyYABWkfIJOs"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Get predictions on the test set\n",
        "predictions_output = trainer.predict(test_data)\n",
        "preds = np.argmax(predictions_output.predictions, axis=-1)\n",
        "labels = predictions_output.label_ids\n",
        "\n",
        "# 2. Compute confusion matrix\n",
        "cm = confusion_matrix(labels, preds)\n",
        "\n",
        "# 3. Print or plot the confusion matrix\n",
        "#print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# Define the sorted list of years as labels\n",
        "year_labels = [\"Charles Dickens\",\n",
        "               \"Jane Austen\",\n",
        "               \"Mark Twain\",\n",
        "               \"Louisa May Alcott\",\n",
        "               \"Herman Melville\"]\n",
        "\n",
        "# Optional: use seaborn to plot a nice heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=year_labels, yticklabels=year_labels)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reliability Diagram (Calibration Curve)\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Get logits/labels on the test set\n",
        "pred_out = trainer.predict(test_data)\n",
        "logits = pred_out.predictions[0] if isinstance(pred_out.predictions, tuple) else pred_out.predictions  # [N, C]\n",
        "labels = pred_out.label_ids  # [N]\n",
        "\n",
        "# Convert to probabilities, confidences, predictions\n",
        "probs = torch.softmax(torch.tensor(logits), dim=1).cpu().numpy()  # [N, C]\n",
        "conf = probs.max(axis=1)                                          # [N]\n",
        "pred = probs.argmax(axis=1)                                       # [N]\n",
        "correct = (pred == labels).astype(np.float32)                     # [N]\n",
        "\n",
        "# Reliability diagram bins (+ ECE/MCE)\n",
        "def reliability_bins(confidences, correct, n_bins=15):\n",
        "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
        "    bin_ids = np.digitize(confidences, bins) - 1  # 0..n_bins-1\n",
        "    bin_acc = np.full(n_bins, np.nan, dtype=np.float32)\n",
        "    bin_conf = np.full(n_bins, np.nan, dtype=np.float32)\n",
        "    bin_cnt = np.zeros(n_bins, dtype=np.int64)\n",
        "\n",
        "    for b in range(n_bins):\n",
        "        m = (bin_ids == b)\n",
        "        if np.any(m):\n",
        "            bin_acc[b] = correct[m].mean()\n",
        "            bin_conf[b] = confidences[m].mean()\n",
        "            bin_cnt[b] = m.sum()\n",
        "        else:\n",
        "            bin_conf[b] = 0.5 * (bins[b] + bins[b+1])\n",
        "\n",
        "    ece = np.nansum(np.abs(bin_acc - bin_conf) * (bin_cnt / len(confidences)))\n",
        "    mce = np.nanmax(np.abs(bin_acc - bin_conf))\n",
        "    return bins, bin_conf, bin_acc, bin_cnt, ece, mce\n",
        "\n",
        "bins, bin_conf, bin_acc, bin_cnt, ece, mce = reliability_bins(conf, correct, n_bins=15)\n",
        "\n",
        "# Brier score (multiclass) + NLL\n",
        "eye = np.eye(probs.shape[1], dtype=np.float32)\n",
        "one_hot = eye[labels]                     # [N, C]\n",
        "brier = np.mean(np.sum((probs - one_hot) ** 2, axis=1))\n",
        "try:\n",
        "    nll = log_loss(labels, probs, labels=list(range(probs.shape[1])))\n",
        "except Exception:\n",
        "    nll = float(\"nan\")\n",
        "\n",
        "print(f\"ECE:  {ece:.4f}\")\n",
        "print(f\"MCE:  {mce:.4f}\")\n",
        "print(f\"Brier:{brier:.4f}\")\n",
        "print(f\"NLL:  {nll:.4f}\")\n",
        "\n",
        "# Plot reliability diagram (single plot; no specific colors/styles)\n",
        "valid = ~np.isnan(bin_acc)\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\")                    # perfect calibration\n",
        "plt.plot(bin_conf[valid], bin_acc[valid], marker=\"o\")       # model calibration\n",
        "plt.xlabel(\"Confidence (predicted probability)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(f\"Reliability Diagram — ECE={ece:.3f}\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# (Optional) also print a small table of bins\n",
        "import pandas as pd\n",
        "calib_df = pd.DataFrame({\n",
        "    \"bin_left\": bins[:-1],\n",
        "    \"bin_right\": bins[1:],\n",
        "    \"bin_count\": bin_cnt,\n",
        "    \"avg_confidence\": bin_conf,\n",
        "    \"avg_accuracy\": bin_acc,\n",
        "})\n",
        "display(calib_df)\n"
      ],
      "metadata": {
        "id": "kbZCyR-O2zby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ---- Load Best Checkpoint:"
      ],
      "metadata": {
        "id": "s9jswrgpzW8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "3QcMGuNQz4KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deberta_model_path      = 'Deberta_v3_large_author_classifier/saved_model_2025.08.12'\n",
        "deberta_tokenizer_path  = 'Deberta_v3_large_author_classifier/saved_tokenizer_2025.08.12'\n",
        "deberta_output_dir      = 'Deberta_v3_large_author_classifier/results_2025.08.12'\n",
        "deberta_logging_dir    = 'Deberta_v3_large_author_classifier/logs_2025.08.12'"
      ],
      "metadata": {
        "id": "kuNLXwkjz4M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, glob\n",
        "\n",
        "# Pick the best checkpoint if Trainer recorded it; else fall back to latest\n",
        "trainer_state_path = os.path.join(deberta_output_dir, \"trainer_state.json\")\n",
        "best_ckpt = None\n",
        "if os.path.isfile(trainer_state_path):\n",
        "    with open(trainer_state_path, \"r\") as f:\n",
        "        st = json.load(f)\n",
        "    best_ckpt = st.get(\"best_model_checkpoint\", None)\n",
        "\n",
        "if not best_ckpt:\n",
        "    ckpts = glob.glob(os.path.join(deberta_output_dir, \"checkpoint-*\"))\n",
        "    assert ckpts, \"No checkpoints found in results folder.\"\n",
        "    ckpts.sort(key=lambda p: int(p.split(\"-\")[-1]))\n",
        "    best_ckpt = ckpts[-1]\n",
        "\n",
        "print(\"Loading checkpoint:\", best_ckpt)"
      ],
      "metadata": {
        "id": "G-McXe34vfhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding, Trainer, TrainingArguments,\n",
        "    EarlyStoppingCallback\n",
        ")"
      ],
      "metadata": {
        "id": "dlwVwAOL6xoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(best_ckpt)"
      ],
      "metadata": {
        "id": "q0OHUtoFz4Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer (you didn’t save one yet, so load from base model id)\n",
        "\n",
        "base_model_id = \"microsoft/deberta-v3-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)"
      ],
      "metadata": {
        "id": "B63ggg1azWeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Re)build data collator if needed\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)"
      ],
      "metadata": {
        "id": "NamYTeHD0TsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=deberta_output_dir,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=16,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=False,     # huge memory saver when True\n",
        "    bf16=True, # A100\n",
        "    tf32=True, # A100\n",
        "    num_train_epochs=20,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=deberta_logging_dir,\n",
        "    logging_steps=10,\n",
        "    logging_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better = True,\n",
        "    report_to=\"wandb\",\n",
        "    run_name=\"deberta-v3-large (5-authors)\",\n",
        "    optim=\"adamw_torch_fused\"\n",
        ")"
      ],
      "metadata": {
        "id": "oZORz_Rh0T1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,          # reuse your existing TrainingArguments\n",
        "    train_dataset=train_data,    # or omit if you only want evaluate()\n",
        "    eval_dataset=test_data,\n",
        "    data_collator=data_collator,\n",
        "    processing_class=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "IH2gzQqV7KvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(trainer.evaluate(metric_key_prefix=\"test\"))"
      ],
      "metadata": {
        "id": "DccstHJx0T49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(deberta_model_path)\n",
        "tokenizer.save_pretrained(deberta_tokenizer_path)\n",
        "\n",
        "print(\"Saved model to:\", deberta_model_path)\n",
        "print(\"Saved tokenizer to:\", deberta_tokenizer_path)"
      ],
      "metadata": {
        "id": "smjwRZ6I0mkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#resume training\n",
        "\n",
        "from dataclasses import replace\n",
        "from transformers import Trainer\n",
        "\n",
        "new_args = replace(\n",
        "    training_args,\n",
        "    num_train_epochs=training_args.num_train_epochs + 10,  # resume with 10 more epochs\n",
        "    per_device_train_batch_size=32,      # <= 8 is safe on A100 with DeBERTa-large\n",
        "    per_device_eval_batch_size=16,\n",
        "    gradient_accumulation_steps=4,      # 8 * 4 = 32 effective batch\n",
        "    gradient_checkpointing=False,\n",
        "    run_name=(training_args.run_name + \"-extra10\")\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=new_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    data_collator=data_collator,\n",
        "    processing_class=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(2)],\n",
        ")\n"
      ],
      "metadata": {
        "id": "79HNm9uw2RQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train(resume_from_checkpoint=best_ckpt)"
      ],
      "metadata": {
        "id": "x8imhfjD2xE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(deberta_model_path)\n",
        "tokenizer.save_pretrained(deberta_tokenizer_path)\n",
        "\n",
        "print(\"Saved model to:\", deberta_model_path)\n",
        "print(\"Saved tokenizer to:\", deberta_tokenizer_path)"
      ],
      "metadata": {
        "id": "mBo0m4As3Skw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR7YVgvGIJOt"
      },
      "source": [
        "## ---- Loading:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZR6B_3FIJOt"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXWnCYmgIJOt"
      },
      "outputs": [],
      "source": [
        "deberta_model_path      = 'Deberta_v3_large_author_classifier/saved_model_2025.08.12'\n",
        "deberta_tokenizer_path  = 'Deberta_v3_large_author_classifier/saved_tokenizer_2025.08.12'\n",
        "deberta_output_dir      = 'Deberta_v3_large_author_classifier/results_2025.08.12'\n",
        "#roberta_logging_dir    = 'Deberta_v3_large_author_classifier/logs_2025.08.12'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(deberta_tokenizer_path, use_fast=True, local_files_only=True)\n",
        "model     = AutoModelForSequenceClassification.from_pretrained(deberta_model_path, local_files_only=True)\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "QkzuyApLQuOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfIVYW9ALCf-"
      },
      "source": [
        "# AI-Evaluate-IA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mLS-xhQfGqH"
      },
      "source": [
        "## ---- Evaluation By RoBERTa:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qbga3E5fYUh"
      },
      "outputs": [],
      "source": [
        "# First Run Below Subsections in AI-Evaluate-AI Section:\n",
        "## ---- Data Preperation:\n",
        "## ---- Loading:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvGQvcxlbFXl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "generated_texts_gpt3      = pd.read_csv(\"generated_texts_gpt3.csv\")\n",
        "generated_texts_gpt3_lora = pd.read_csv(\"generated_texts_gpt3_lora.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aKirrfInkHS"
      },
      "outputs": [],
      "source": [
        "generated_texts_gpt3['Model'] = 'gpt3'\n",
        "generated_texts_gpt3_lora['Model'] = 'gpt3_lora'\n",
        "\n",
        "generated_texts_merged = pd.concat([generated_texts_gpt3, generated_texts_gpt3_lora], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TAaWea-pNm4"
      },
      "outputs": [],
      "source": [
        "generated_texts_merged['expected_label'] = generated_texts_merged['Text'].str.extract(r'^<(\\d+)>').astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egxYJt61pr3s"
      },
      "outputs": [],
      "source": [
        "generated_texts_merged['Text'] = generated_texts_merged['Text'].str.replace(r'^<\\d+>', '', regex=True)\n",
        "generated_texts_merged['Text'] = generated_texts_merged['Text'].str.replace(r'<end>.*', '', regex=True).str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YW0G8-dVpr69"
      },
      "outputs": [],
      "source": [
        "generated_texts_merged"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1000,1100):\n",
        "  print(f\"{i}: {generated_texts_merged.loc[i,\"Model\"]} [{generated_texts_merged.loc[i,\"expected_label\"]}] {generated_texts_merged.loc[i,\"Text\"]}\")"
      ],
      "metadata": {
        "id": "mAiydQ6H1_D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaBqTg0zmc_F"
      },
      "outputs": [],
      "source": [
        "list_generated_texts_merged = generated_texts_merged[\"Text\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdLGGv4_nE42"
      },
      "outputs": [],
      "source": [
        "len(list_generated_texts_merged)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZqXXH18nE8M"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer( list_generated_texts_merged,\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    return_tensors=\"pt\"\n",
        "                  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "carUMWiEnFAr"
      },
      "outputs": [],
      "source": [
        "# Set model to eval mode\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaFZgYnPls1d"
      },
      "outputs": [],
      "source": [
        "# Run predictions\n",
        "\n",
        "label_map = {\n",
        "    0: \"Charles Dickens\",\n",
        "    1: \"Jane Austen\",\n",
        "    2: \"Mark Twain\",\n",
        "    3: \"Louisa May Alcott\",\n",
        "    4: \"Herman Melville\"\n",
        "}\n",
        "\n",
        "# inputs is a dict of tensors like {'input_ids': ..., 'attention_mask': ...}\n",
        "N = inputs[\"input_ids\"].shape[0]\n",
        "bs = 64  # 16/32/64\n",
        "\n",
        "all_preds = []\n",
        "all_probs = []   # will hold per-class probabilities\n",
        "\n",
        "with torch.inference_mode(), torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "    for i in tqdm(range(0, N, bs), desc=\"Predicting\", total=(N + bs - 1)//bs):\n",
        "        batch = {k: v[i:i+bs].to(device, non_blocking=True) for k, v in inputs.items()}\n",
        "        logits = model(**batch).logits                         # [B, 5]\n",
        "        probs  = torch.softmax(logits.float(), dim=1).cpu()    # [B, 5] on CPU\n",
        "        preds  = probs.argmax(dim=1)\n",
        "\n",
        "        all_probs.append(probs)\n",
        "        all_preds.append(preds)\n",
        "\n",
        "# concat to full arrays\n",
        "predictions = torch.cat(all_preds)                 # [N]\n",
        "probs_np = torch.cat(all_probs).numpy()            # [N, 5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgRf-26EqfJl"
      },
      "outputs": [],
      "source": [
        "# optional: tidy column names\n",
        "prob_cols = [f\"prob_{label_map[i].replace(' ', '_')}\" for i in range(5)]\n",
        "\n",
        "for i, col in enumerate(prob_cols):\n",
        "    generated_texts_merged[col] = probs_np[:, i]\n",
        "\n",
        "# keep your predicted labels too\n",
        "generated_texts_merged[\"predicted_Label_roberta\"] = predictions.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jooeTSiqfNg"
      },
      "outputs": [],
      "source": [
        "generated_texts_merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exp8dzo73fMs"
      },
      "source": [
        "## ---- Visulization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juZOvcdaLGc_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Wedge\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Map labels to author names\n",
        "label_map = {\n",
        "    0: \"Charles Dickens\",\n",
        "    1: \"Jane Austen\",\n",
        "    2: \"Mark Twain\",\n",
        "    3: \"Louisa May Alcott\",\n",
        "    4: \"Herman Melville\"\n",
        "}\n",
        "\n",
        "generated_texts_merged[\"Expected Author\"] = generated_texts_merged[\"expected_label\"].map(label_map)\n",
        "generated_texts_merged[\"Predicted Author\"] = generated_texts_merged[\"predicted_Label_roberta\"].map(label_map)\n",
        "\n",
        "# Prepare pie-chart data at each (expected, predicted) coordinate\n",
        "grouped = (\n",
        "    generated_texts_merged.groupby([\"Expected Author\", \"Predicted Author\", \"Model\"])\n",
        "    .size()\n",
        "    .reset_index(name=\"Count\")\n",
        ")\n",
        "\n",
        "# Create a pivot table to get model proportions at each coordinate\n",
        "pivot = grouped.pivot_table(index=[\"Expected Author\", \"Predicted Author\"],\n",
        "                             columns=\"Model\",\n",
        "                             values=\"Count\",\n",
        "                             fill_value=0)\n",
        "\n",
        "# Color map for models\n",
        "model_colors = dict(zip(pivot.columns, plt.cm.Set2.colors[:len(pivot.columns)]))\n",
        "\n",
        "# Plot pies at each grid location\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "x_labels = sorted(pivot.index.get_level_values(0).unique())\n",
        "y_labels = sorted(pivot.index.get_level_values(1).unique())\n",
        "\n",
        "# Create mappings for positioning\n",
        "x_map = {name: i for i, name in enumerate(x_labels)}\n",
        "y_map = {name: i for i, name in enumerate(y_labels)}\n",
        "\n",
        "# Draw pies\n",
        "for (x_label, y_label), row in pivot.iterrows():\n",
        "    total = row.sum()\n",
        "    if total == 0:\n",
        "        continue\n",
        "\n",
        "    x = x_map[x_label]\n",
        "    y = y_map[y_label]\n",
        "    radius = 0.05 + 0.1 * (total / pivot.values.max())  # Adjust pie size\n",
        "\n",
        "    start_angle = 0\n",
        "    for model, count in row.items():\n",
        "        if count == 0:\n",
        "            continue\n",
        "        angle = 360 * count / total\n",
        "        wedge = Wedge(center=(x, y), r=radius,\n",
        "                      theta1=start_angle, theta2=start_angle + angle,\n",
        "                      facecolor=model_colors[model],\n",
        "                      edgecolor='gray', linewidth=0.5)\n",
        "        ax.add_patch(wedge)\n",
        "        start_angle += angle\n",
        "\n",
        "# Configure axis\n",
        "ax.set_xlim(-0.5 - 0.3, len(x_map) - 0.5 + 0.3)\n",
        "ax.set_ylim(-0.5 - 0.3, len(y_map) - 0.5 + 0.3)\n",
        "ax.set_xticks(list(x_map.values()))\n",
        "ax.set_yticks(list(y_map.values()))\n",
        "ax.set_xticklabels(x_labels, rotation=30)\n",
        "ax.set_yticklabels(y_labels)\n",
        "ax.set_xlabel(\"Expected Author\")\n",
        "ax.set_ylabel(\"Predicted Author\")\n",
        "ax.set_title(\"Expected vs. Predicted (Pie chart by Model)\")\n",
        "\n",
        "for (x_label, y_label), row in pivot.iterrows():\n",
        "    total = row.sum()\n",
        "    if total == 0:\n",
        "        continue\n",
        "\n",
        "    x = x_map[x_label]\n",
        "    y = y_map[y_label]\n",
        "    radius = 0.1 + 0.2 * (total / pivot.values.max())  # smaller pies\n",
        "\n",
        "    start_angle = 0\n",
        "    for model, count in row.items():\n",
        "        if count == 0:\n",
        "            continue\n",
        "        angle = 360 * count / total\n",
        "        wedge = Wedge(center=(x, y), r=radius,\n",
        "                      theta1=start_angle, theta2=start_angle + angle,\n",
        "                      facecolor=model_colors[model],\n",
        "                      edgecolor='gray', linewidth=0.5)\n",
        "        ax.add_patch(wedge)\n",
        "        start_angle += angle\n",
        "\n",
        "    ax.text(\n",
        "        x, y, str(int(total)),\n",
        "        ha='center', va='center',\n",
        "        fontsize=9, weight='bold', color='black'\n",
        "    )\n",
        "\n",
        "\n",
        "# Legend\n",
        "legend_patches = [plt.Line2D([0], [0], marker='o', color='w',\n",
        "                label=model, markerfacecolor=color, markersize=10)\n",
        "                for model, color in model_colors.items()]\n",
        "ax.legend(handles=legend_patches, title=\"Model\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "\n",
        "ax.set_aspect('equal')\n",
        "ax.grid(True, linestyle=\"--\", alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j21n3KbMsbmV"
      },
      "outputs": [],
      "source": [
        "def plot_pie_chart_for_model(data, model_filter, title_suffix):\n",
        "    # Filter for a single model\n",
        "    data = data[data[\"Model\"] == model_filter]\n",
        "\n",
        "    # Group and pivot\n",
        "    grouped = (\n",
        "        data.groupby([\"Expected Author\", \"Predicted Author\", \"Model\"])\n",
        "        .size()\n",
        "        .reset_index(name=\"Count\")\n",
        "    )\n",
        "    pivot = grouped.pivot_table(\n",
        "        index=[\"Expected Author\", \"Predicted Author\"],\n",
        "        columns=\"Model\",\n",
        "        values=\"Count\",\n",
        "        fill_value=0\n",
        "    )\n",
        "\n",
        "    # Color map\n",
        "    model_colors = dict(zip(pivot.columns, plt.cm.Set2.colors[:len(pivot.columns)]))\n",
        "\n",
        "    # Figure setup\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    x_labels = sorted(pivot.index.get_level_values(0).unique())\n",
        "    y_labels = sorted(pivot.index.get_level_values(1).unique())\n",
        "    x_map = {name: i for i, name in enumerate(x_labels)}\n",
        "    y_map = {name: i for i, name in enumerate(y_labels)}\n",
        "\n",
        "    # Draw pies\n",
        "    for (x_label, y_label), row in pivot.iterrows():\n",
        "        total = row.sum()\n",
        "        if total == 0:\n",
        "            continue\n",
        "\n",
        "        x = x_map[x_label]\n",
        "        y = y_map[y_label]\n",
        "        radius = 0.1 + 0.2 * (total / pivot.values.max())\n",
        "\n",
        "        start_angle = 0\n",
        "        for model, count in row.items():\n",
        "            if count == 0:\n",
        "                continue\n",
        "            angle = 360 * count / total\n",
        "            wedge = Wedge(\n",
        "                center=(x, y), r=radius,\n",
        "                theta1=start_angle, theta2=start_angle + angle,\n",
        "                facecolor=model_colors[model],\n",
        "                edgecolor='gray', linewidth=0.5\n",
        "            )\n",
        "            ax.add_patch(wedge)\n",
        "            start_angle += angle\n",
        "\n",
        "        ax.text(\n",
        "            x, y, str(int(total)),\n",
        "            ha='center', va='center',\n",
        "            fontsize=9, weight='bold', color='black'\n",
        "        )\n",
        "\n",
        "    # Configure axis\n",
        "    ax.set_xlim(-0.8, len(x_map) - 0.2)\n",
        "    ax.set_ylim(-0.8, len(y_map) - 0.2)\n",
        "    ax.set_xticks(list(x_map.values()))\n",
        "    ax.set_yticks(list(y_map.values()))\n",
        "    ax.set_xticklabels(x_labels, rotation=30)\n",
        "    ax.set_yticklabels(y_labels)\n",
        "    ax.set_xlabel(\"Expected Author\")\n",
        "    ax.set_ylabel(\"Predicted Author\")\n",
        "    ax.set_title(f\"Expected vs. Predicted ({model_filter} only {title_suffix})\")\n",
        "\n",
        "    # Legend\n",
        "    legend_patches = [\n",
        "        plt.Line2D([0], [0], marker='o', color='w',\n",
        "                   label=model, markerfacecolor=color, markersize=10)\n",
        "        for model, color in model_colors.items()\n",
        "    ]\n",
        "    ax.legend(handles=legend_patches, title=\"Model\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "\n",
        "    ax.set_aspect('equal')\n",
        "    ax.grid(True, linestyle=\"--\", alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Create the two separate plots\n",
        "plot_pie_chart_for_model(generated_texts_merged, \"gpt3\", \"(GPT-3)\")\n",
        "plot_pie_chart_for_model(generated_texts_merged, \"gpt3_lora\", \"(GPT-3 LoRA)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xuhw5H5X3y4m"
      },
      "source": [
        "## ---- Analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVjdEtkR1UHA"
      },
      "outputs": [],
      "source": [
        "generated_texts_merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udnuX9T44MIe"
      },
      "outputs": [],
      "source": [
        "# Add diagnostic columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTPXDDHR1UJ0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "y_true = generated_texts_merged[\"expected_label\"].to_numpy()\n",
        "y_pred = generated_texts_merged[\"predicted_Label_roberta\"].to_numpy()\n",
        "\n",
        "# If you already have probs_np from earlier: shape [N,5]\n",
        "p_true = probs_np[np.arange(len(probs_np)), y_true]\n",
        "p_pred = probs_np[np.arange(len(probs_np)), y_pred]\n",
        "conf   = probs_np.max(axis=1)                       # model confidence\n",
        "entropy = -np.sum(probs_np * np.log(probs_np + 1e-12), axis=1)  # uncertainty\n",
        "margin = p_pred - p_true                            # how much more the model preferred pred over true\n",
        "\n",
        "generated_texts_merged[\"p_true\"] = p_true\n",
        "generated_texts_merged[\"p_pred\"] = p_pred\n",
        "generated_texts_merged[\"confidence\"] = conf\n",
        "generated_texts_merged[\"entropy\"] = entropy\n",
        "generated_texts_merged[\"margin\"] = margin\n",
        "generated_texts_merged[\"is_error\"] = (y_true != y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuacqXVJ1dIK"
      },
      "outputs": [],
      "source": [
        "generated_texts_merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fshK7JgE4T7h"
      },
      "outputs": [],
      "source": [
        "# 2) Where are errors concentrated?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnlUzEfEMlBh"
      },
      "outputs": [],
      "source": [
        "def author_confusion_table(df):\n",
        "    \"\"\"\n",
        "    Creates a table of confusion stats between Expected Author and Predicted Author.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        Must contain columns:\n",
        "        - \"Expected Author\" (string)\n",
        "        - \"Predicted Author\" (string)\n",
        "        - \"Text\" (any, used for count)\n",
        "        - \"p_true\" (float, probability assigned to the true label)\n",
        "        - \"confidence\" (float, max predicted probability)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pandas.DataFrame\n",
        "        Index: (Expected Author, Predicted Author)\n",
        "        Columns: n, mean_p_true, mean_conf\n",
        "        Sorted by n descending.\n",
        "    \"\"\"\n",
        "    tbl = (\n",
        "        df.groupby([\"Expected Author\", \"Predicted Author\"])\n",
        "          .agg(n=(\"Text\", \"size\"),\n",
        "               mean_p_true=(\"p_true\", \"mean\"),\n",
        "               mean_conf=(\"confidence\", \"mean\"))\n",
        "          .sort_values([\"n\"], ascending=False)\n",
        "    )\n",
        "    return tbl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVvk65lGMlrc"
      },
      "outputs": [],
      "source": [
        "# Full dataset\n",
        "full_tbl = author_confusion_table(generated_texts_merged)\n",
        "\n",
        "# Only GPT-3 samples\n",
        "gpt3_tbl = author_confusion_table(generated_texts_merged[generated_texts_merged[\"Model\"] == \"gpt3\"])\n",
        "\n",
        "# Only GPT-3 LoRA samples\n",
        "gpt3_lora_tbl = author_confusion_table(generated_texts_merged[generated_texts_merged[\"Model\"] == \"gpt3_lora\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsjkAhaKMluR"
      },
      "outputs": [],
      "source": [
        "gpt3_tbl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvvWr1Ad2_M6"
      },
      "outputs": [],
      "source": [
        "gpt3_lora_tbl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkwUQxYM4YBx"
      },
      "outputs": [],
      "source": [
        "# 3) Confidence & calibration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK8o8pET1030"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def calibration_table(df, bins=np.linspace(0, 1, 21)):\n",
        "    \"\"\"\n",
        "    Compute calibration stats (n, acc, mean_conf, gap) by confidence bin.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        Must contain:\n",
        "        - \"confidence\": float, predicted max probability\n",
        "        - \"is_error\": bool, True if prediction != actual label\n",
        "        - \"Text\": used for counting samples\n",
        "    bins : array-like, optional\n",
        "        Bin edges for confidence (default: 0.0 to 1.0 in steps of 0.1)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pandas.DataFrame\n",
        "        Index: conf_bin (int bin index)\n",
        "        Columns: n, acc, mean_conf, gap\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df[\"conf_bin\"] = np.digitize(df[\"confidence\"], bins)\n",
        "\n",
        "    calib = (\n",
        "        df.groupby(\"conf_bin\")\n",
        "          .agg(n=(\"Text\", \"size\"),\n",
        "               acc=(\"is_error\", lambda x: 1 - x.mean()),\n",
        "               mean_conf=(\"confidence\", \"mean\"))\n",
        "    )\n",
        "    calib[\"gap\"] = calib[\"mean_conf\"] - calib[\"acc\"]\n",
        "    return calib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BURQXnwdNNzh"
      },
      "outputs": [],
      "source": [
        "# Full dataset\n",
        "full_calib = calibration_table(generated_texts_merged)\n",
        "\n",
        "# GPT-3 only\n",
        "gpt3_calib = calibration_table(generated_texts_merged[generated_texts_merged[\"Model\"] == \"gpt3\"])\n",
        "\n",
        "# GPT-3 LoRA only\n",
        "gpt3_lora_calib = calibration_table(generated_texts_merged[generated_texts_merged[\"Model\"] == \"gpt3_lora\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqLuZdApNN2W"
      },
      "outputs": [],
      "source": [
        "gpt3_calib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaDqcoUP3EPW"
      },
      "outputs": [],
      "source": [
        "gpt3_lora_calib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZZzJzmvJgWJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "calib_for_plot = gpt3_calib.copy() # full_calib \\ gpt3_calib \\ gpt3_lora_calib\n",
        "\n",
        "plt.figure(figsize=(7,6))\n",
        "plt.plot(calib_for_plot[\"mean_conf\"], calib_for_plot[\"acc\"], marker='o', label=\"Actual accuracy\")\n",
        "plt.plot([0,1],[0,1],'--',color='gray',label=\"Perfect calibration\")\n",
        "\n",
        "sizes = (calib_for_plot[\"n\"] / calib_for_plot[\"n\"].max()) * 300\n",
        "plt.scatter(calib_for_plot[\"mean_conf\"], calib_for_plot[\"acc\"], s=sizes, alpha=0.6)\n",
        "\n",
        "plt.xlabel(\"Predicted confidence\")\n",
        "plt.ylabel(\"Percentage of Matching Labels\")\n",
        "plt.title(\"Reliability Curve (Calibration Plot)\")\n",
        "#plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_texts_merged[generated_texts_merged[\"Model\"] == \"gpt3\"]"
      ],
      "metadata": {
        "id": "fuHCy9EfIkfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_conf_samples = generated_texts_merged[generated_texts_merged[\"confidence\"] > 0.93]"
      ],
      "metadata": {
        "id": "sppOXeM8JQ6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_conf_samples = high_conf_samples.reset_index(drop=True)\n",
        "high_conf_samples"
      ],
      "metadata": {
        "id": "KDlsZiaYJWQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Wedge\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Map labels to author names\n",
        "label_map = {\n",
        "    0: \"Charles Dickens\",\n",
        "    1: \"Jane Austen\",\n",
        "    2: \"Mark Twain\",\n",
        "    3: \"Louisa May Alcott\",\n",
        "    4: \"Herman Melville\"\n",
        "}\n",
        "\n",
        "high_conf_samples[\"Expected Author\"] = high_conf_samples[\"expected_label\"].map(label_map)\n",
        "high_conf_samples[\"Predicted Author\"] = high_conf_samples[\"predicted_Label_roberta\"].map(label_map)\n",
        "\n",
        "# Prepare pie-chart data at each (expected, predicted) coordinate\n",
        "grouped = (\n",
        "    high_conf_samples.groupby([\"Expected Author\", \"Predicted Author\", \"Model\"])\n",
        "    .size()\n",
        "    .reset_index(name=\"Count\")\n",
        ")\n",
        "\n",
        "# Create a pivot table to get model proportions at each coordinate\n",
        "pivot = grouped.pivot_table(index=[\"Expected Author\", \"Predicted Author\"],\n",
        "                             columns=\"Model\",\n",
        "                             values=\"Count\",\n",
        "                             fill_value=0)\n",
        "\n",
        "# Color map for models\n",
        "model_colors = dict(zip(pivot.columns, plt.cm.Set2.colors[:len(pivot.columns)]))\n",
        "\n",
        "# Plot pies at each grid location\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "x_labels = sorted(pivot.index.get_level_values(0).unique())\n",
        "y_labels = sorted(pivot.index.get_level_values(1).unique())\n",
        "\n",
        "# Create mappings for positioning\n",
        "x_map = {name: i for i, name in enumerate(x_labels)}\n",
        "y_map = {name: i for i, name in enumerate(y_labels)}\n",
        "\n",
        "# Draw pies\n",
        "for (x_label, y_label), row in pivot.iterrows():\n",
        "    total = row.sum()\n",
        "    if total == 0:\n",
        "        continue\n",
        "\n",
        "    x = x_map[x_label]\n",
        "    y = y_map[y_label]\n",
        "    radius = 0.05 + 0.1 * (total / pivot.values.max())  # Adjust pie size\n",
        "\n",
        "    start_angle = 0\n",
        "    for model, count in row.items():\n",
        "        if count == 0:\n",
        "            continue\n",
        "        angle = 360 * count / total\n",
        "        wedge = Wedge(center=(x, y), r=radius,\n",
        "                      theta1=start_angle, theta2=start_angle + angle,\n",
        "                      facecolor=model_colors[model],\n",
        "                      edgecolor='gray', linewidth=0.5)\n",
        "        ax.add_patch(wedge)\n",
        "        start_angle += angle\n",
        "\n",
        "# Configure axis\n",
        "ax.set_xlim(-0.5 - 0.3, len(x_map) - 0.5 + 0.3)\n",
        "ax.set_ylim(-0.5 - 0.3, len(y_map) - 0.5 + 0.3)\n",
        "ax.set_xticks(list(x_map.values()))\n",
        "ax.set_yticks(list(y_map.values()))\n",
        "ax.set_xticklabels(x_labels, rotation=30)\n",
        "ax.set_yticklabels(y_labels)\n",
        "ax.set_xlabel(\"Expected Author\")\n",
        "ax.set_ylabel(\"Predicted Author\")\n",
        "ax.set_title(\"Expected vs. Predicted (Pie chart by Model)\")\n",
        "\n",
        "for (x_label, y_label), row in pivot.iterrows():\n",
        "    total = row.sum()\n",
        "    if total == 0:\n",
        "        continue\n",
        "\n",
        "    x = x_map[x_label]\n",
        "    y = y_map[y_label]\n",
        "    radius = 0.1 + 0.2 * (total / pivot.values.max())  # smaller pies\n",
        "\n",
        "    start_angle = 0\n",
        "    for model, count in row.items():\n",
        "        if count == 0:\n",
        "            continue\n",
        "        angle = 360 * count / total\n",
        "        wedge = Wedge(center=(x, y), r=radius,\n",
        "                      theta1=start_angle, theta2=start_angle + angle,\n",
        "                      facecolor=model_colors[model],\n",
        "                      edgecolor='gray', linewidth=0.5)\n",
        "        ax.add_patch(wedge)\n",
        "        start_angle += angle\n",
        "\n",
        "    ax.text(\n",
        "        x, y, str(int(total)),\n",
        "        ha='center', va='center',\n",
        "        fontsize=9, weight='bold', color='black'\n",
        "    )\n",
        "\n",
        "\n",
        "# Legend\n",
        "legend_patches = [plt.Line2D([0], [0], marker='o', color='w',\n",
        "                label=model, markerfacecolor=color, markersize=10)\n",
        "                for model, color in model_colors.items()]\n",
        "ax.legend(handles=legend_patches, title=\"Model\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "\n",
        "ax.set_aspect('equal')\n",
        "ax.grid(True, linestyle=\"--\", alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v6mGicNrJRE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_pie_chart_for_model(data, model_filter, title_suffix):\n",
        "    # Filter for a single model\n",
        "    data = data[data[\"Model\"] == model_filter]\n",
        "\n",
        "    # Group and pivot\n",
        "    grouped = (\n",
        "        data.groupby([\"Expected Author\", \"Predicted Author\", \"Model\"])\n",
        "        .size()\n",
        "        .reset_index(name=\"Count\")\n",
        "    )\n",
        "    pivot = grouped.pivot_table(\n",
        "        index=[\"Expected Author\", \"Predicted Author\"],\n",
        "        columns=\"Model\",\n",
        "        values=\"Count\",\n",
        "        fill_value=0\n",
        "    )\n",
        "\n",
        "    # Color map\n",
        "    model_colors = dict(zip(pivot.columns, plt.cm.Set2.colors[:len(pivot.columns)]))\n",
        "\n",
        "    # Figure setup\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    x_labels = sorted(pivot.index.get_level_values(0).unique())\n",
        "    y_labels = sorted(pivot.index.get_level_values(1).unique())\n",
        "    x_map = {name: i for i, name in enumerate(x_labels)}\n",
        "    y_map = {name: i for i, name in enumerate(y_labels)}\n",
        "\n",
        "    # Draw pies\n",
        "    for (x_label, y_label), row in pivot.iterrows():\n",
        "        total = row.sum()\n",
        "        if total == 0:\n",
        "            continue\n",
        "\n",
        "        x = x_map[x_label]\n",
        "        y = y_map[y_label]\n",
        "        radius = 0.1 + 0.2 * (total / pivot.values.max())\n",
        "\n",
        "        start_angle = 0\n",
        "        for model, count in row.items():\n",
        "            if count == 0:\n",
        "                continue\n",
        "            angle = 360 * count / total\n",
        "            wedge = Wedge(\n",
        "                center=(x, y), r=radius,\n",
        "                theta1=start_angle, theta2=start_angle + angle,\n",
        "                facecolor=model_colors[model],\n",
        "                edgecolor='gray', linewidth=0.5\n",
        "            )\n",
        "            ax.add_patch(wedge)\n",
        "            start_angle += angle\n",
        "\n",
        "        ax.text(\n",
        "            x, y, str(int(total)),\n",
        "            ha='center', va='center',\n",
        "            fontsize=9, weight='bold', color='black'\n",
        "        )\n",
        "\n",
        "    # Configure axis\n",
        "    ax.set_xlim(-0.8, len(x_map) - 0.2)\n",
        "    ax.set_ylim(-0.8, len(y_map) - 0.2)\n",
        "    ax.set_xticks(list(x_map.values()))\n",
        "    ax.set_yticks(list(y_map.values()))\n",
        "    ax.set_xticklabels(x_labels, rotation=30)\n",
        "    ax.set_yticklabels(y_labels)\n",
        "    ax.set_xlabel(\"Expected Author\")\n",
        "    ax.set_ylabel(\"Predicted Author\")\n",
        "    ax.set_title(f\"Expected vs. Predicted ({model_filter} only {title_suffix})\")\n",
        "\n",
        "    # Legend\n",
        "    legend_patches = [\n",
        "        plt.Line2D([0], [0], marker='o', color='w',\n",
        "                   label=model, markerfacecolor=color, markersize=10)\n",
        "        for model, color in model_colors.items()\n",
        "    ]\n",
        "    ax.legend(handles=legend_patches, title=\"Model\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "\n",
        "    ax.set_aspect('equal')\n",
        "    ax.grid(True, linestyle=\"--\", alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Create the two separate plots\n",
        "plot_pie_chart_for_model(high_conf_samples, \"gpt3\", \"(GPT-3)\")\n",
        "plot_pie_chart_for_model(high_conf_samples, \"gpt3_lora\", \"(GPT-3 LoRA)\")\n"
      ],
      "metadata": {
        "id": "Bn_uQNPmJRJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_percentages(df):\n",
        "    result = (\n",
        "        df.groupby(\"Model\")[\"is_error\"]\n",
        "        .agg(\n",
        "            total_samples=\"count\",\n",
        "            error_count=\"sum\"\n",
        "        )\n",
        "        .reset_index()\n",
        "    )\n",
        "    result[\"correct_percentage\"] = 1 - (result[\"error_count\"] / result[\"total_samples\"])\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "p8FupNxtKFvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For all data\n",
        "print(correct_percentages(generated_texts_merged))\n",
        "print()\n",
        "# For only high-confidence subset\n",
        "high_conf = generated_texts_merged[generated_texts_merged[\"confidence\"] > 0.93]\n",
        "print(correct_percentages(high_conf))"
      ],
      "metadata": {
        "id": "HGrnXByaKFzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OKvnddoNKF3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1fGLmATnJRMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYBhB6cS4bHd"
      },
      "outputs": [],
      "source": [
        "# 4) Hard vs easy mistakes (use the probabilities):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ucfk8kjw2qpy"
      },
      "outputs": [],
      "source": [
        "hi_conf_wrong = generated_texts_merged[\n",
        "    (generated_texts_merged[\"Model\"] == \"gpt3\") &\n",
        "    (generated_texts_merged[\"is_error\"]) &\n",
        "    (generated_texts_merged[\"confidence\"] > 0.93)]\n",
        "\n",
        "total_high_confidence = generated_texts_merged[\n",
        "    (generated_texts_merged[\"Model\"] == \"gpt3\") &\n",
        "    (generated_texts_merged[\"confidence\"] > 0.93)]\n",
        "\n",
        "lo_conf_wrong = generated_texts_merged[\n",
        "    (generated_texts_merged[\"Model\"] == \"gpt3\") &\n",
        "    (generated_texts_merged[\"is_error\"]) &\n",
        "    (generated_texts_merged[\"confidence\"] < 0.5)]\n",
        "\n",
        "total_low_confidence = generated_texts_merged[\n",
        "    (generated_texts_merged[\"Model\"] == \"gpt3\") &\n",
        "    (generated_texts_merged[\"confidence\"] < 0.5)]\n",
        "\n",
        "\n",
        "print(\"The number of mismatched sentences with high model confidence ( > 90% ):\")\n",
        "print(len(hi_conf_wrong),\"/\", len(total_high_confidence), \"=\", round(len(hi_conf_wrong)/len(total_high_confidence), 2))\n",
        "\n",
        "print(\"\\nThe number of mismatched sentences with low model confidence ( < 50% ):\")\n",
        "print(len(lo_conf_wrong),\"/\", len(total_low_confidence), \"=\", round(len(lo_conf_wrong)/len(total_low_confidence), 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNMFfbQu2J40"
      },
      "outputs": [],
      "source": [
        "hi_conf_wrong = generated_texts_merged[\n",
        "    (generated_texts_merged[\"Model\"] == \"gpt3_lora\") &\n",
        "    (generated_texts_merged[\"is_error\"]) &\n",
        "    (generated_texts_merged[\"confidence\"] > 0.93)]\n",
        "\n",
        "total_high_confidence = generated_texts_merged[\n",
        "    (generated_texts_merged[\"Model\"] == \"gpt3_lora\") &\n",
        "    (generated_texts_merged[\"confidence\"] > 0.93)]\n",
        "\n",
        "lo_conf_wrong = generated_texts_merged[\n",
        "    (generated_texts_merged[\"Model\"] == \"gpt3_lora\") &\n",
        "    (generated_texts_merged[\"is_error\"]) &\n",
        "    (generated_texts_merged[\"confidence\"] < 0.5)]\n",
        "\n",
        "total_low_confidence = generated_texts_merged[\n",
        "    (generated_texts_merged[\"Model\"] == \"gpt3_lora\") &\n",
        "    (generated_texts_merged[\"confidence\"] < 0.5)]\n",
        "\n",
        "\n",
        "print(\"The number of mismatched sentences with high model confidence ( > 90% ):\")\n",
        "print(len(hi_conf_wrong),\"/\", len(total_high_confidence), \"=\", round(len(hi_conf_wrong)/len(total_high_confidence), 2))\n",
        "\n",
        "print(\"\\nThe number of mismatched sentences with low model confidence ( < 50% ):\")\n",
        "print(len(lo_conf_wrong),\"/\", len(total_low_confidence), \"=\", round(len(lo_conf_wrong)/len(total_low_confidence), 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzHBoS1S4ewg"
      },
      "outputs": [],
      "source": [
        "# 5) Top-k signal (is the true class “close”?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN5iDzNj2Lmx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def topk_accuracy(df, probs, k_values=(1, 2, 3)):\n",
        "    \"\"\"\n",
        "    Compute top-k accuracy for given dataset and probability matrix.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        Must contain:\n",
        "        - \"expected_label\": int, true class index\n",
        "        - \"predicted_Label_roberta\": int, predicted class index\n",
        "    probs : np.ndarray\n",
        "        Shape (N, C) array of predicted probabilities for each class.\n",
        "        Rows must align with df rows.\n",
        "    k_values : tuple\n",
        "        Top-k values to compute (default: (1, 2, 3)).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Keys: \"top-1\", \"top-2\", ..., values: accuracy as float.\n",
        "    \"\"\"\n",
        "    y_true = df[\"expected_label\"].to_numpy()\n",
        "    results = {}\n",
        "\n",
        "    for k in k_values:\n",
        "        topk_right = (np.argsort(-probs, axis=1)[:, :k] == y_true[:, None]).any(axis=1).mean()\n",
        "        results[f\"top-{k}\"] = round(topk_right, 2)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_hVQJyjfnIc"
      },
      "outputs": [],
      "source": [
        "# Full dataset\n",
        "full_topk = topk_accuracy(generated_texts_merged, probs_np)\n",
        "\n",
        "# GPT-3 only\n",
        "mask_gpt3 = generated_texts_merged[\"Model\"] == \"gpt3\"\n",
        "gpt3_topk = topk_accuracy(generated_texts_merged[mask_gpt3], probs_np[mask_gpt3])\n",
        "\n",
        "# GPT-3 LoRA only\n",
        "mask_lora = generated_texts_merged[\"Model\"] == \"gpt3_lora\"\n",
        "gpt3_lora_topk = topk_accuracy(generated_texts_merged[mask_lora], probs_np[mask_lora])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36X03KBBfnLP"
      },
      "outputs": [],
      "source": [
        "gpt3_topk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2W6niKk3me9"
      },
      "outputs": [],
      "source": [
        "gpt3_lora_topk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter: confidence > 0.93\n",
        "high_conf_mask = generated_texts_merged[\"confidence\"] > 0.93\n",
        "\n",
        "# Full dataset (high confidence only)\n",
        "full_topk = topk_accuracy(\n",
        "    generated_texts_merged[high_conf_mask],\n",
        "    probs_np[high_conf_mask]\n",
        ")\n",
        "\n",
        "# GPT-3 only (high confidence only)\n",
        "mask_gpt3 = (generated_texts_merged[\"Model\"] == \"gpt3\") & high_conf_mask\n",
        "gpt3_topk = topk_accuracy(\n",
        "    generated_texts_merged[mask_gpt3],\n",
        "    probs_np[mask_gpt3]\n",
        ")\n",
        "\n",
        "# GPT-3 LoRA only (high confidence only)\n",
        "mask_lora = (generated_texts_merged[\"Model\"] == \"gpt3_lora\") & high_conf_mask\n",
        "gpt3_lora_topk = topk_accuracy(\n",
        "    generated_texts_merged[mask_lora],\n",
        "    probs_np[mask_lora]\n",
        ")"
      ],
      "metadata": {
        "id": "JxjdjhvRQxLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt3_topk"
      ],
      "metadata": {
        "id": "ZcItMMoLSXMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt3_lora_topk"
      ],
      "metadata": {
        "id": "-LfI5WVfSXPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ou8b_fzjSXSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWwK53qb4khT"
      },
      "outputs": [],
      "source": [
        "# 6) Margin analysis (which errors are “close”?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ccbPmeS2ohR"
      },
      "outputs": [],
      "source": [
        "def close_and_stubborn_errors(df, model=None, prob_cols_prefix=\"prob_\",\n",
        "                               true_label_col=\"expected_label\",\n",
        "                               pred_label_col=\"predicted_Label_roberta\"):\n",
        "    \"\"\"\n",
        "    Identify 'close call' errors (small gap between true and predicted prob)\n",
        "    and 'stubborn' errors (predicted much higher than true) for a dataset or subset.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        Must contain:\n",
        "        - probability columns starting with prob_cols_prefix (default: \"prob_\")\n",
        "        - true_label_col (default: \"expected_label\")\n",
        "        - pred_label_col (default: \"predicted_Label_roberta\")\n",
        "    model : str or None\n",
        "        If given, filters df where Model == model before analysis.\n",
        "    prob_cols_prefix : str\n",
        "        Prefix for the probability columns.\n",
        "    true_label_col : str\n",
        "        Name of the column with true class indices.\n",
        "    pred_label_col : str\n",
        "        Name of the column with predicted class indices.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple of pd.DataFrame\n",
        "        (close_calls_df, stubborn_errors_df)\n",
        "    \"\"\"\n",
        "    # Optional filtering by model\n",
        "    if model is not None:\n",
        "        dfx = df[df[\"Model\"] == model].copy()\n",
        "    else:\n",
        "        dfx = df.copy()\n",
        "\n",
        "    # Get probability matrix\n",
        "    prob_cols = [c for c in dfx.columns if c.startswith(prob_cols_prefix)]\n",
        "    if not prob_cols:\n",
        "        raise ValueError(f\"No probability columns found starting with '{prob_cols_prefix}'.\")\n",
        "\n",
        "    probs = dfx[prob_cols].to_numpy()\n",
        "    y_true = dfx[true_label_col].to_numpy()\n",
        "    y_pred = dfx[pred_label_col].to_numpy()\n",
        "\n",
        "    # Compute required metrics if missing\n",
        "    if \"p_true\" not in dfx.columns:\n",
        "        dfx[\"p_true\"] = probs[np.arange(len(probs)), y_true]\n",
        "    if \"confidence\" not in dfx.columns:\n",
        "        dfx[\"confidence\"] = probs.max(axis=1)\n",
        "    if \"is_error\" not in dfx.columns:\n",
        "        dfx[\"is_error\"] = y_true != y_pred\n",
        "    if \"margin\" not in dfx.columns:\n",
        "        dfx[\"margin\"] = dfx[\"confidence\"] - dfx[\"p_true\"]\n",
        "\n",
        "    # true_minus_best = p_true - confidence\n",
        "    dfx[\"true_minus_best\"] = dfx[\"p_true\"] - dfx[\"confidence\"]\n",
        "\n",
        "    # Define subsets\n",
        "    close_calls_df = dfx.query(\"is_error and true_minus_best > -0.1\")\n",
        "    stubborn_errors_df = dfx.query(\"is_error and margin > 0.3\")\n",
        "\n",
        "    return close_calls_df, stubborn_errors_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_JhFdIr2tv2"
      },
      "outputs": [],
      "source": [
        "# Full dataset\n",
        "close_calls_all, stubborn_errors_all = close_and_stubborn_errors(generated_texts_merged)\n",
        "\n",
        "# Only GPT-3\n",
        "close_calls_gpt3, stubborn_errors_gpt3 = close_and_stubborn_errors(generated_texts_merged, model=\"gpt3\")\n",
        "\n",
        "# Only GPT-3 LoRA\n",
        "close_calls_lora, stubborn_errors_lora = close_and_stubborn_errors(generated_texts_merged, model=\"gpt3_lora\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oyk0Vxp8w7Tg"
      },
      "outputs": [],
      "source": [
        "print(\"gpt3:\\n\")\n",
        "print(f\"Close calls:     {len(close_calls_gpt3)} samples\")\n",
        "print(f\"Stubborn errors: {len(stubborn_errors_gpt3)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fv2oilLC34pM"
      },
      "outputs": [],
      "source": [
        "print(\"gpt3_lora:\\n\")\n",
        "print(f\"Close calls:     {len(close_calls_lora)} samples\")\n",
        "print(f\"Stubborn errors: {len(stubborn_errors_lora)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga6oIcIo2wyt"
      },
      "outputs": [],
      "source": [
        "# 7) Length & truncation effects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xstvpNDJ4Vq5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHYM_vbI4qnc"
      },
      "outputs": [],
      "source": [
        "#8) Per-author difficulty & bias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXRHLStq4v3x"
      },
      "outputs": [],
      "source": [
        "def author_stats_table(df):\n",
        "    \"\"\"\n",
        "    Compute per-author performance statistics.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        Must contain:\n",
        "        - \"Expected Author\": str\n",
        "        - \"Text\": used for counting samples\n",
        "        - \"is_error\": bool, True if prediction != actual\n",
        "        - \"p_true\": float, probability assigned to the true label\n",
        "        - \"confidence\": float, max predicted probability\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pandas.DataFrame\n",
        "        Index: Expected Author\n",
        "        Columns: n, acc, mean_p_true, mean_conf\n",
        "        Sorted by acc ascending.\n",
        "    \"\"\"\n",
        "    stats = (\n",
        "        df.groupby(\"Expected Author\")\n",
        "          .agg(n=(\"Text\", \"size\"),\n",
        "               acc=(\"is_error\", lambda x: 1 - x.mean()),\n",
        "               mean_p_true=(\"p_true\", \"mean\"),\n",
        "               mean_conf=(\"confidence\", \"mean\"))\n",
        "          .sort_values(\"acc\")\n",
        "    )\n",
        "    return stats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggln3Gts2vJH"
      },
      "outputs": [],
      "source": [
        "# Full dataset\n",
        "full_author_stats = author_stats_table(generated_texts_merged)\n",
        "\n",
        "# GPT-3 only\n",
        "gpt3_author_stats = author_stats_table(generated_texts_merged[generated_texts_merged[\"Model\"] == \"gpt3\"])\n",
        "\n",
        "# GPT-3 LoRA only\n",
        "gpt3_lora_author_stats = author_stats_table(generated_texts_merged[generated_texts_merged[\"Model\"] == \"gpt3_lora\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArgzrIQ52vMS"
      },
      "outputs": [],
      "source": [
        "gpt3_author_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AtuLsrj4aul"
      },
      "outputs": [],
      "source": [
        "gpt3_lora_author_stats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full dataset\n",
        "full_author_stats_h_c = author_stats_table(generated_texts_merged[generated_texts_merged[\"confidence\"] > 0.93])\n",
        "\n",
        "# GPT-3 only\n",
        "gpt3_author_stats_h_c = author_stats_table(generated_texts_merged[(generated_texts_merged[\"Model\"] == \"gpt3\") & (generated_texts_merged[\"confidence\"] > 0.93)])\n",
        "\n",
        "# GPT-3 LoRA only\n",
        "gpt3_lora_author_stats_h_c = author_stats_table(generated_texts_merged[(generated_texts_merged[\"Model\"] == \"gpt3_lora\") & (generated_texts_merged[\"confidence\"] > 0.93)])\n"
      ],
      "metadata": {
        "id": "lmJ-XyIjWRDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt3_author_stats_h_c"
      ],
      "metadata": {
        "id": "aPkYFXMHWRGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt3_lora_author_stats_h_c"
      ],
      "metadata": {
        "id": "5Gy_nDsQWRJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8um8DD86WRMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbpX7VYq4xKo"
      },
      "outputs": [],
      "source": [
        "# 9) Compare generation sources (gpt3 vs gpt3_lora):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Skr7BGoH5PtY"
      },
      "outputs": [],
      "source": [
        "def author_accuracy_by_model(df):\n",
        "    \"\"\"\n",
        "    Compute per-author accuracy stats split by Model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        Must contain:\n",
        "        - \"Model\": str\n",
        "        - \"Expected Author\": str\n",
        "        - \"is_error\": bool\n",
        "        - \"p_true\": float\n",
        "        - \"confidence\": float\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pandas.DataFrame\n",
        "        Pivot table with Expected Author as index, Models as columns, and accuracy as values.\n",
        "        Also returns the grouped stats before pivoting.\n",
        "    \"\"\"\n",
        "    grouped = (\n",
        "        df.groupby([\"Model\", \"Expected Author\"])\n",
        "          .agg(acc=(\"is_error\", lambda x: 1 - x.mean()),\n",
        "               mean_p_true=(\"p_true\", \"mean\"),\n",
        "               mean_conf=(\"confidence\", \"mean\"))\n",
        "          .reset_index()\n",
        "    )\n",
        "    pivot_acc = grouped.pivot(index=\"Expected Author\", columns=\"Model\", values=\"acc\")\n",
        "    return grouped, pivot_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kGa1h0P5T7l"
      },
      "outputs": [],
      "source": [
        "# Full dataset\n",
        "grouped_stats, pivot_acc = author_accuracy_by_model(generated_texts_merged)\n",
        "\n",
        "# Show detailed grouped stats\n",
        "print(grouped_stats)\n",
        "\n",
        "# Show accuracy pivot table\n",
        "print(pivot_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJbp9JPH_TAq"
      },
      "source": [
        "## ---- Explainability (RoBERTa):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPTZeOchwxt-"
      },
      "outputs": [],
      "source": [
        "!pip install captum numpy>=2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnppFbZCCF9B"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- pick representative mistakes ---\n",
        "def select_representative_errors(df, top_k_per_pair=3, min_conf=0.93):\n",
        "    \"\"\"\n",
        "    High-confidence wrong predictions, grouped by (Expected Author -> Predicted Author).\n",
        "    \"\"\"\n",
        "    wrong = df.query(\"is_error == True and confidence >= @min_conf\").copy()\n",
        "    if wrong.empty:\n",
        "        print(\"No high-confidence errors found with the current threshold.\")\n",
        "        return wrong\n",
        "\n",
        "    wrong[\"pair\"] = list(zip(wrong[\"Expected Author\"], wrong[\"Predicted Author\"]))\n",
        "    reps = (\n",
        "        wrong.sort_values(\"confidence\", ascending=False)\n",
        "             .groupby(\"pair\", group_keys=False)\n",
        "             .head(top_k_per_pair)\n",
        "    )\n",
        "    return reps\n",
        "\n",
        "# --- RoBERTa BPE aggregation: merge subword pieces into words ---\n",
        "def merge_bpe_tokens(tokens, scores):\n",
        "    \"\"\"\n",
        "    RoBERTa uses byte-level BPE; tokens starting with 'Ġ' indicate word starts.\n",
        "    Merge subword pieces by summing their scores.\n",
        "    \"\"\"\n",
        "    words, w_scores = [], []\n",
        "    curr_word, curr_score = \"\", 0.0\n",
        "\n",
        "    for tok, sc in zip(tokens, scores):\n",
        "        # Skip special tokens\n",
        "        if tok in (\"<s>\", \"</s>\", \"<pad>\"):\n",
        "            continue\n",
        "\n",
        "        if tok.startswith(\"Ġ\"):  # new word\n",
        "            if curr_word:\n",
        "                words.append(curr_word)\n",
        "                w_scores.append(curr_score)\n",
        "            curr_word = tok[1:]  # drop the leading space marker\n",
        "            curr_score = float(sc)\n",
        "        else:\n",
        "            # continuation of the current word (subword piece)\n",
        "            curr_word += tok\n",
        "            curr_score += float(sc)\n",
        "\n",
        "    if curr_word:\n",
        "        words.append(curr_word)\n",
        "        w_scores.append(curr_score)\n",
        "\n",
        "    # Build a dataframe sorted by absolute contribution\n",
        "    out = pd.DataFrame({\"word\": words, \"attr\": w_scores})\n",
        "    out[\"attr_abs\"] = out[\"attr\"].abs()\n",
        "    out = out.sort_values(\"attr_abs\", ascending=False).reset_index(drop=True)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP1r5w0OCF_7"
      },
      "outputs": [],
      "source": [
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "def _forward_from_embeds(model, attention_mask, token_type_ids, inputs_embeds):\n",
        "    out = model(\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids if token_type_ids is not None else None,\n",
        "        inputs_embeds=inputs_embeds\n",
        "    )\n",
        "    return out.logits\n",
        "\n",
        "def attribute_example_with_ig(model, tokenizer, text, target_class, max_length=256, device=None, n_steps=50):\n",
        "    model.eval()\n",
        "    device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "    model.to(device)\n",
        "\n",
        "    enc = tokenizer(\n",
        "        text, truncation=True, max_length=max_length, return_tensors=\"pt\"\n",
        "    )\n",
        "    input_ids = enc[\"input_ids\"].to(device)\n",
        "    attention_mask = enc[\"attention_mask\"].to(device)\n",
        "    token_type_ids = enc.get(\"token_type_ids\", None)\n",
        "    if token_type_ids is not None:\n",
        "        token_type_ids = token_type_ids.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputs_embeds = model.roberta.embeddings.word_embeddings(input_ids)  # [1, L, 768]\n",
        "        pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 1\n",
        "        baseline_ids = torch.full_like(input_ids, pad_id)\n",
        "        baseline_embeds = model.roberta.embeddings.word_embeddings(baseline_ids)\n",
        "\n",
        "        logits = _forward_from_embeds(model, attention_mask, token_type_ids, inputs_embeds)\n",
        "        probs = torch.softmax(logits, dim=-1).detach().cpu().numpy()[0]\n",
        "        pred_class = int(probs.argmax())\n",
        "\n",
        "    def _target_logit_from_embeds(inputs_embeds_):\n",
        "        logits_ = _forward_from_embeds(model, attention_mask, token_type_ids, inputs_embeds_)\n",
        "        return logits_[:, target_class]\n",
        "\n",
        "    ig = IntegratedGradients(_target_logit_from_embeds)\n",
        "\n",
        "    # FIX: only one return value when return_convergence_delta=False\n",
        "    attributions = ig.attribute(\n",
        "        inputs=inputs_embeds,\n",
        "        baselines=baseline_embeds,\n",
        "        n_steps=n_steps,\n",
        "        return_convergence_delta=False\n",
        "    )\n",
        "\n",
        "    token_attr = attributions.sum(dim=-1).squeeze(0).detach().cpu().numpy()  # [L]\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0).detach().cpu().tolist())\n",
        "    return tokens, token_attr, probs, pred_class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsK-sIp_CGC2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def explain_row(df, row_idx, model, tokenizer, max_length=256, device=None, top_k=15):\n",
        "    \"\"\"\n",
        "    Run IG for both predicted and true classes on one example from df.\n",
        "    Returns:\n",
        "      dict with:\n",
        "        - meta (text, expected/predicted authors, probs)\n",
        "        - pred_df (word attributions toward predicted class)\n",
        "        - true_df (word attributions toward true class)\n",
        "    \"\"\"\n",
        "    row = df.iloc[row_idx]\n",
        "    text = row[\"Text\"]\n",
        "    true_cls = int(row[\"expected_label\"])\n",
        "    pred_cls = int(row[\"predicted_Label_roberta\"])\n",
        "\n",
        "    tokens_p, attr_p, probs_p, pred_check = attribute_example_with_ig(\n",
        "        model, tokenizer, text, target_class=pred_cls, max_length=max_length, device=device\n",
        "    )\n",
        "    tokens_t, attr_t, probs_t, _ = attribute_example_with_ig(\n",
        "        model, tokenizer, text, target_class=true_cls, max_length=max_length, device=device\n",
        "    )\n",
        "\n",
        "    # Merge subwords -> words\n",
        "    pred_df = merge_bpe_tokens(tokens_p, attr_p)\n",
        "    true_df = merge_bpe_tokens(tokens_t, attr_t)\n",
        "\n",
        "    # Keep only top_k by |attr|\n",
        "    pred_df_top = pred_df.head(top_k).copy()\n",
        "    true_df_top = true_df.head(top_k).copy()\n",
        "\n",
        "    # Meta\n",
        "    meta = {\n",
        "        \"text\": text,\n",
        "        \"expected_label\": true_cls,\n",
        "        \"predicted_label\": pred_cls,\n",
        "        \"Expected Author\": row[\"Expected Author\"],\n",
        "        \"Predicted Author\": row[\"Predicted Author\"],\n",
        "        \"probs\": probs_p,  # same as probs_t; recomputed per run but same input\n",
        "        \"confidence\": float(row[\"confidence\"]),\n",
        "        \"is_error\": bool(row[\"is_error\"])\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"meta\": meta,\n",
        "        \"pred_df\": pred_df_top,  # tokens driving the predicted class\n",
        "        \"true_df\": true_df_top   # tokens driving the true class\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example: pick representative errors (very confident but wrong)\n",
        "reps = select_representative_errors(generated_texts_merged, top_k_per_pair=2, min_conf=0.93)\n",
        "\n",
        "# If you want a specific confusion pair:\n",
        "# reps = generated_texts_merged.query(\"is_error and `Expected Author`=='Jane Austen' and `Predicted Author`=='Charles Dickens'\").nlargest(3, 'confidence')\n",
        "\n",
        "# Explain the first 3 examples\n",
        "results = []\n",
        "for idx in reps.index[:3]:\n",
        "    out = explain_row(generated_texts_merged, idx, model, tokenizer, max_length=256, top_k=15)\n",
        "    results.append(out)\n",
        "\n",
        "# Inspect one result\n",
        "r0 = results[0]\n",
        "print(\"Example meta:\", {k:v for k,v in r0[\"meta\"].items() if k!=\"probs\"})\n",
        "print(\"\\nTop tokens toward PREDICTED class:\")\n",
        "display(r0[\"pred_df\"])  # in Colab/Jupyter this shows a nice table\n",
        "\n",
        "print(\"\\nTop tokens toward TRUE class:\")\n",
        "display(r0[\"true_df\"])\n"
      ],
      "metadata": {
        "id": "T6sYM1Wyb9qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsEp6QSTGs1z"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "import matplotlib\n",
        "\n",
        "def highlight_text(tokens, scores, cmap=\"RdBu\", score_range=None):\n",
        "    \"\"\"\n",
        "    Render tokens with background color proportional to attribution score.\n",
        "    Positive scores = red, negative = blue (by default).\n",
        "    \"\"\"\n",
        "    # Normalize scores for consistent coloring\n",
        "    if score_range is None:\n",
        "        max_abs = max(abs(scores.min()), abs(scores.max())) or 1e-9\n",
        "    else:\n",
        "        max_abs = score_range\n",
        "    norm = matplotlib.colors.Normalize(vmin=-max_abs, vmax=max_abs)\n",
        "    cmap_obj = matplotlib.cm.get_cmap(cmap)\n",
        "\n",
        "    html_tokens = []\n",
        "    for tok, sc in zip(tokens, scores):\n",
        "        color = matplotlib.colors.rgb2hex(cmap_obj(norm(sc))[:3])\n",
        "        html_tokens.append(f\"<span style='background-color:{color}; padding:2px; margin:1px;'>{tok}</span>\")\n",
        "    return \" \".join(html_tokens)\n",
        "\n",
        "def show_attribution_text(text, tokenizer, word_attr_df, title=\"\", cmap=\"RdBu\"):\n",
        "    \"\"\"\n",
        "    Given raw text and word-level attributions (word_attr_df from merge_bpe_tokens),\n",
        "    re-tokenize to split into words for alignment with attr values.\n",
        "    \"\"\"\n",
        "    words = word_attr_df[\"word\"].tolist()\n",
        "    scores = word_attr_df[\"attr\"].tolist()\n",
        "    html_str = highlight_text(words, np.array(scores), cmap=cmap)\n",
        "    display(HTML(f\"<div><b>{title}</b><br>{html_str}</div>\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JE7oK4pqGun8"
      },
      "outputs": [],
      "source": [
        "# Predicted class visualization\n",
        "show_attribution_text(\n",
        "    r0[\"meta\"][\"text\"],\n",
        "    tokenizer,\n",
        "    r0[\"pred_df\"],\n",
        "    title=f\"Evidence for Predicted: {r0['meta']['Predicted Author']}\"\n",
        ")\n",
        "\n",
        "# True class visualization\n",
        "show_attribution_text(\n",
        "    r0[\"meta\"][\"text\"],\n",
        "    tokenizer,\n",
        "    r0[\"true_df\"],\n",
        "    title=f\"Evidence for True: {r0['meta']['Expected Author']}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLZWkKHyE51n"
      },
      "outputs": [],
      "source": [
        "def differential_attribution(pred_df, true_df, k=15):\n",
        "    # merge on word; fill missing with 0\n",
        "    df = pred_df[[\"word\",\"attr\"]].merge(\n",
        "        true_df[[\"word\",\"attr\"]], on=\"word\", how=\"outer\", suffixes=(\"_pred\",\"_true\")\n",
        "    ).fillna(0.0)\n",
        "    df[\"diff\"] = df[\"attr_pred\"] - df[\"attr_true\"]\n",
        "    df[\"diff_abs\"] = df[\"diff\"].abs()\n",
        "    df = df.sort_values(\"diff_abs\", ascending=False).head(k)\n",
        "    return df[[\"word\",\"attr_pred\",\"attr_true\",\"diff\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEx1wQ_IF3ey"
      },
      "outputs": [],
      "source": [
        "# Example:\n",
        "# diff > 0: evidence for the predicted class.\n",
        "# diff < 0: evidence for the true class.\n",
        "\n",
        "diff = differential_attribution(r0[\"pred_df\"], r0[\"true_df\"], k=20)\n",
        "display(diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oT9_1RUGIPh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def mask_and_score(model, tokenizer, text, target_class, max_length=256, device=None, words_to_mask=None):\n",
        "    \"\"\"\n",
        "    Replace chosen words with <mask> (roughly) and return target_class prob delta.\n",
        "    \"\"\"\n",
        "    device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "    model.to(device).eval()\n",
        "\n",
        "    # crude word masking: replace exact substrings; for robust masking, map tokens instead\n",
        "    masked_text = text\n",
        "    if words_to_mask:\n",
        "        for w in words_to_mask:\n",
        "            if w and w.strip():\n",
        "                masked_text = masked_text.replace(w, tokenizer.mask_token or \"<mask>\")\n",
        "\n",
        "    enc = tokenizer(masked_text, truncation=True, max_length=max_length, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**enc).logits\n",
        "        probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
        "    return probs[target_class], probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkqCKGFSGOSr"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "target_pred = r0[\"meta\"][\"predicted_label\"]\n",
        "p_with, _ = mask_and_score(model, tokenizer, r0[\"meta\"][\"text\"], target_pred, words_to_mask=[\"Marianne\"])\n",
        "print(\"P(pred class) after masking:\", p_with)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfv7H0MVGRjZ"
      },
      "outputs": [],
      "source": [
        "def explain_with_diff(df, row_idx, model, tokenizer, max_length=256, device=None, top_k=15, do_ablate=True):\n",
        "    out = explain_row(df, row_idx, model, tokenizer, max_length=max_length, device=device, top_k=top_k)\n",
        "    pred_df, true_df = out[\"pred_df\"], out[\"true_df\"]\n",
        "    diff = differential_attribution(pred_df, true_df, k=top_k)\n",
        "\n",
        "    print(\"META:\", {k:v for k,v in out[\"meta\"].items() if k not in (\"probs\",)})\n",
        "    print(\"\\n--- Top tokens toward PREDICTED class ---\")\n",
        "    display(pred_df)\n",
        "    print(\"\\n--- Top tokens toward TRUE class ---\")\n",
        "    display(true_df)\n",
        "    print(\"\\n--- Differential (pred - true) ---\")\n",
        "    display(diff)\n",
        "\n",
        "    if do_ablate:\n",
        "        text = out[\"meta\"][\"text\"]\n",
        "        pred_cls = out[\"meta\"][\"predicted_label\"]\n",
        "        # test top-3 separating tokens (positive diff)\n",
        "        top_sep = diff.sort_values(\"diff\", ascending=False).head(3)[\"word\"].tolist()\n",
        "        try:\n",
        "            p_after, _ = mask_and_score(model, tokenizer, text, pred_cls, max_length=max_length, device=device, words_to_mask=top_sep)\n",
        "            print(f\"\\nAblation: masking {top_sep} -> P(pred_class) = {p_after:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(\"Ablation error:\", e)\n",
        "\n",
        "    return out, diff\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmW_X3_hGZyU"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example:\n",
        "_ = explain_with_diff(generated_texts_merged, reps.index[0], model, tokenizer, max_length=256)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InFeNETHd6nL"
      },
      "source": [
        "## ---- Explainability (DeBERTa):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOWvf0ocd6nL"
      },
      "outputs": [],
      "source": [
        "!pip install captum numpy>=2.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_embeddings_module(model):\n",
        "    \"\"\"\n",
        "    Return the word embedding module for common HF encoder models.\n",
        "    Supports: roberta, deberta, bert, distilbert, albert, electra.\n",
        "    \"\"\"\n",
        "    for backbone_name in [\"roberta\", \"deberta\", \"bert\", \"distilbert\", \"albert\", \"electra\"]:\n",
        "        if hasattr(model, backbone_name):\n",
        "            backbone = getattr(model, backbone_name)\n",
        "            if hasattr(backbone, \"embeddings\") and hasattr(backbone.embeddings, \"word_embeddings\"):\n",
        "                return backbone.embeddings.word_embeddings\n",
        "    # Some models may have embeddings directly\n",
        "    if hasattr(model, \"embeddings\") and hasattr(model.embeddings, \"word_embeddings\"):\n",
        "        return model.embeddings.word_embeddings\n",
        "    raise AttributeError(\"Could not locate embeddings.word_embeddings on this model.\")\n"
      ],
      "metadata": {
        "id": "CRYT3-AZeN3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIDcWYnXd6nL"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- pick representative mistakes ---\n",
        "def select_representative_errors(df, top_k_per_pair=3, min_conf=0.93):\n",
        "    \"\"\"\n",
        "    High-confidence wrong predictions, grouped by (Expected Author -> Predicted Author).\n",
        "    \"\"\"\n",
        "    wrong = df.query(\"is_error == True and confidence >= @min_conf\").copy()\n",
        "    if wrong.empty:\n",
        "        print(\"No high-confidence errors found with the current threshold.\")\n",
        "        return wrong\n",
        "\n",
        "    wrong[\"pair\"] = list(zip(wrong[\"Expected Author\"], wrong[\"Predicted Author\"]))\n",
        "    reps = (\n",
        "        wrong.sort_values(\"confidence\", ascending=False)\n",
        "             .groupby(\"pair\", group_keys=False)\n",
        "             .head(top_k_per_pair)\n",
        "    )\n",
        "    return reps\n",
        "\n",
        "def merge_subword_tokens(tokens, scores):\n",
        "    \"\"\"\n",
        "    Merge subwords to words for both byte-BPE (Ġword) and SentencePiece (▁word).\n",
        "    - Starts of words: tokens starting with 'Ġ' (RoBERTa) or '▁' (SentencePiece).\n",
        "    - Special tokens skipped: <s>, </s>, <pad>, [CLS], [SEP], [PAD]\n",
        "    Aggregates scores by summing subpieces.\n",
        "    \"\"\"\n",
        "    specials = {\"<s>\", \"</s>\", \"<pad>\", \"[CLS]\", \"[SEP]\", \"[PAD]\"}\n",
        "    words, w_scores = [], []\n",
        "    curr_word, curr_score = \"\", 0.0\n",
        "\n",
        "    def flush():\n",
        "        nonlocal curr_word, curr_score\n",
        "        if curr_word:\n",
        "            words.append(curr_word)\n",
        "            w_scores.append(curr_score)\n",
        "            curr_word, curr_score = \"\", 0.0\n",
        "\n",
        "    for tok, sc in zip(tokens, scores):\n",
        "        if tok in specials:\n",
        "            continue\n",
        "        # Word starts: leading marker\n",
        "        if tok.startswith(\"Ġ\") or tok.startswith(\"▁\"):\n",
        "            flush()\n",
        "            base = tok[1:]  # drop marker\n",
        "            curr_word = base\n",
        "            curr_score = float(sc)\n",
        "        else:\n",
        "            # continuation piece\n",
        "            curr_word += tok\n",
        "            curr_score += float(sc)\n",
        "    flush()\n",
        "\n",
        "    out = pd.DataFrame({\"word\": words, \"attr\": w_scores})\n",
        "    out[\"attr_abs\"] = out[\"attr\"].abs()\n",
        "    out = out.sort_values(\"attr_abs\", ascending=False).reset_index(drop=True)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEoyBR8Od6nL"
      },
      "outputs": [],
      "source": [
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "def _forward_from_embeds(model, attention_mask, token_type_ids, inputs_embeds):\n",
        "    # Most HF sequence classification models accept inputs_embeds directly\n",
        "    out = model(\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids if token_type_ids is not None else None,\n",
        "        inputs_embeds=inputs_embeds\n",
        "    )\n",
        "    return out.logits\n",
        "\n",
        "\n",
        "def attribute_example_with_ig(model, tokenizer, text, target_class, max_length=256, device=None, n_steps=50):\n",
        "    \"\"\"\n",
        "    Compute IG token attributions for one text toward 'target_class' logit.\n",
        "    Works with DeBERTa/RoBERTa/BERT/etc.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "    model.to(device)\n",
        "\n",
        "    enc = tokenizer(text, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    input_ids = enc[\"input_ids\"].to(device)\n",
        "    attention_mask = enc.get(\"attention_mask\", None)\n",
        "    attention_mask = attention_mask.to(device) if attention_mask is not None else None\n",
        "    token_type_ids = enc.get(\"token_type_ids\", None)\n",
        "    token_type_ids = token_type_ids.to(device) if token_type_ids is not None else None\n",
        "\n",
        "    # Get embeddings module generically\n",
        "    we = get_word_embeddings_module(model)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputs_embeds = we(input_ids)  # [1, L, H]\n",
        "        pad_id = tokenizer.pad_token_id\n",
        "        if pad_id is None:\n",
        "            # Fallback: many SentencePiece models use 0 as pad (DeBERTa often does)\n",
        "            pad_id = 0\n",
        "        baseline_ids = torch.full_like(input_ids, pad_id)\n",
        "        baseline_embeds = we(baseline_ids)\n",
        "\n",
        "        logits = _forward_from_embeds(model, attention_mask, token_type_ids, inputs_embeds)\n",
        "        probs = torch.softmax(logits, dim=-1).detach().cpu().numpy()[0]\n",
        "        pred_class = int(probs.argmax())\n",
        "\n",
        "    def _target_logit_from_embeds(inputs_embeds_):\n",
        "        logits_ = _forward_from_embeds(model, attention_mask, token_type_ids, inputs_embeds_)\n",
        "        return logits_[:, target_class]\n",
        "\n",
        "    ig = IntegratedGradients(_target_logit_from_embeds)\n",
        "\n",
        "    attributions = ig.attribute(\n",
        "        inputs=inputs_embeds,\n",
        "        baselines=baseline_embeds,\n",
        "        n_steps=n_steps,\n",
        "        return_convergence_delta=False\n",
        "    )\n",
        "\n",
        "    token_attr = attributions.sum(dim=-1).squeeze(0).detach().cpu().numpy()  # [L]\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0).detach().cpu().tolist())\n",
        "    return tokens, token_attr, probs, pred_class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kBWp1w2d6nL"
      },
      "outputs": [],
      "source": [
        "def explain_row(df, row_idx, model, tokenizer, max_length=256, device=None, top_k=15):\n",
        "    row = df.iloc[row_idx]\n",
        "    text = row[\"Text\"]\n",
        "    true_cls = int(row[\"expected_label\"])\n",
        "    pred_cls = int(row[\"predicted_Label_roberta\"])\n",
        "\n",
        "    toks_p, attr_p, probs_p, _ = attribute_example_with_ig(\n",
        "        model, tokenizer, text, target_class=pred_cls, max_length=max_length, device=device\n",
        "    )\n",
        "    toks_t, attr_t, probs_t, _ = attribute_example_with_ig(\n",
        "        model, tokenizer, text, target_class=true_cls, max_length=max_length, device=device\n",
        "    )\n",
        "\n",
        "    # ORIGINAL ORDER (for visualization)\n",
        "    pred_df_ordered = merge_subword_tokens(toks_p, attr_p, sort_by_abs=False)\n",
        "    true_df_ordered = merge_subword_tokens(toks_t, attr_t, sort_by_abs=False)\n",
        "\n",
        "    # SORTED (for top-k tables, optional)\n",
        "    pred_df_sorted = merge_subword_tokens(toks_p, attr_p, sort_by_abs=True).head(top_k)\n",
        "    true_df_sorted = merge_subword_tokens(toks_t, attr_t, sort_by_abs=True).head(top_k)\n",
        "\n",
        "    meta = {\n",
        "        \"text\": text,\n",
        "        \"expected_label\": true_cls,\n",
        "        \"predicted_label\": pred_cls,\n",
        "        \"Expected Author\": row[\"Expected Author\"],\n",
        "        \"Predicted Author\": row[\"Predicted Author\"],\n",
        "        \"probs\": probs_p,\n",
        "        \"confidence\": float(row[\"confidence\"]),\n",
        "        \"is_error\": bool(row[\"is_error\"])\n",
        "    }\n",
        "    return {\n",
        "        \"meta\": meta,\n",
        "        \"pred_ordered\": pred_df_ordered,\n",
        "        \"true_ordered\": true_df_ordered,\n",
        "        \"pred_df\": pred_df_sorted,\n",
        "        \"true_df\": true_df_sorted\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def merge_subword_tokens(tokens, scores, *, sort_by_abs=False):\n",
        "    \"\"\"\n",
        "    Merge subwords to words for both byte-BPE (Ġword) and SentencePiece (▁word).\n",
        "    Preserves original order by default. If sort_by_abs=True, sorts by |attr|.\n",
        "    \"\"\"\n",
        "    specials = {\"<s>\", \"</s>\", \"<pad>\", \"[CLS]\", \"[SEP]\", \"[PAD]\"}\n",
        "    words, w_scores = [], []\n",
        "    curr_word, curr_score = \"\", 0.0\n",
        "\n",
        "    def flush():\n",
        "        nonlocal curr_word, curr_score\n",
        "        if curr_word:\n",
        "            words.append(curr_word)\n",
        "            w_scores.append(curr_score)\n",
        "            curr_word, curr_score = \"\", 0.0\n",
        "\n",
        "    for tok, sc in zip(tokens, scores):\n",
        "        if tok in specials:\n",
        "            continue\n",
        "        if tok.startswith(\"Ġ\") or tok.startswith(\"▁\"):  # new word\n",
        "            flush()\n",
        "            base = tok[1:]\n",
        "            curr_word = base\n",
        "            curr_score = float(sc)\n",
        "        else:  # continuation\n",
        "            curr_word += tok\n",
        "            curr_score += float(sc)\n",
        "    flush()\n",
        "\n",
        "    df = pd.DataFrame({\"word\": words, \"attr\": w_scores})\n",
        "    if sort_by_abs:\n",
        "        df[\"attr_abs\"] = df[\"attr\"].abs()\n",
        "        df = df.sort_values(\"attr_abs\", ascending=False).reset_index(drop=True)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "3lyCWZOCf_pR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: pick representative errors (very confident but wrong)\n",
        "reps = select_representative_errors(generated_texts_merged, top_k_per_pair=2, min_conf=0.93)\n",
        "\n",
        "# If you want a specific confusion pair:\n",
        "# reps = generated_texts_merged.query(\"is_error and `Expected Author`=='Jane Austen' and `Predicted Author`=='Charles Dickens'\").nlargest(3, 'confidence')\n",
        "\n",
        "# Explain the first 3 examples\n",
        "results = []\n",
        "for idx in reps.index[:3]:\n",
        "    out = explain_row(generated_texts_merged, idx, model, tokenizer, max_length=256, top_k=15)\n",
        "    results.append(out)\n",
        "\n",
        "# Inspect one result\n",
        "r0 = results[0]\n",
        "print(\"Example meta:\", {k:v for k,v in r0[\"meta\"].items() if k!=\"probs\"})\n",
        "print(\"\\nTop tokens toward PREDICTED class:\")\n",
        "display(r0[\"pred_df\"])  # in Colab/Jupyter this shows a nice table\n",
        "\n",
        "print(\"\\nTop tokens toward TRUE class:\")\n",
        "display(r0[\"true_df\"])\n"
      ],
      "metadata": {
        "id": "ION-fZrFd6nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHsWBoWqd6nM"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "\n",
        "def highlight_text(tokens, scores, cmap=\"RdBu\", score_range=None):\n",
        "    \"\"\"\n",
        "    Render tokens with background color proportional to attribution score.\n",
        "    Positive = red, negative = blue.\n",
        "    \"\"\"\n",
        "    # New: use the non-deprecated API\n",
        "    cmap_obj = matplotlib.colormaps.get_cmap(cmap)\n",
        "\n",
        "    scores = np.asarray(scores, dtype=float)\n",
        "    if score_range is None:\n",
        "        max_abs = float(np.max(np.abs(scores))) or 1e-9\n",
        "    else:\n",
        "        max_abs = float(score_range)\n",
        "\n",
        "    norm = matplotlib.colors.Normalize(vmin=-max_abs, vmax=max_abs)\n",
        "    html_tokens = []\n",
        "    for tok, sc in zip(tokens, scores):\n",
        "        color = matplotlib.colors.rgb2hex(cmap_obj(norm(sc))[:3])\n",
        "        safe_tok = tok.replace(\"&\",\"&amp;\").replace(\"<\",\"&lt;\").replace(\">\",\"&gt;\")\n",
        "        html_tokens.append(\n",
        "            f\"<span style='background-color:{color}; padding:2px 3px; margin:1px; border-radius:3px;'>{safe_tok}</span>\"\n",
        "        )\n",
        "    return \" \".join(html_tokens)\n",
        "\n",
        "def show_attribution_text_ordered(word_attr_df, title=\"\", cmap=\"RdBu\", score_range=None):\n",
        "    \"\"\"\n",
        "    Display words in their ORIGINAL order (no re-sorting).\n",
        "    Expects a DataFrame with columns ['word','attr'] in original order.\n",
        "    \"\"\"\n",
        "    html_str = highlight_text(word_attr_df[\"word\"].tolist(),\n",
        "                              word_attr_df[\"attr\"].tolist(),\n",
        "                              cmap=cmap, score_range=score_range)\n",
        "    display(HTML(f\"<div style='line-height:2'><b>{title}</b><br>{html_str}</div>\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nzHkkNnd6nM"
      },
      "outputs": [],
      "source": [
        "# Example with r0 (from your earlier results)\n",
        "show_attribution_text_ordered(\n",
        "    r0[\"pred_ordered\"],\n",
        "    title=f\"Evidence for Predicted (original order): {r0['meta']['Predicted Author']}\"\n",
        ")\n",
        "\n",
        "show_attribution_text_ordered(\n",
        "    r0[\"true_ordered\"],\n",
        "    title=f\"Evidence for True (original order): {r0['meta']['Expected Author']}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def side_by_side_ordered(pred_df_ordered, true_df_ordered, pred_title, true_title, cmap=\"RdBu\"):\n",
        "    html_left  = highlight_text(pred_df_ordered[\"word\"].tolist(), pred_df_ordered[\"attr\"].tolist(), cmap=cmap)\n",
        "    html_right = highlight_text(true_df_ordered[\"word\"].tolist(), true_df_ordered[\"attr\"].tolist(), cmap=cmap)\n",
        "    display(HTML(f\"\"\"\n",
        "    <div style=\"display:flex; gap:24px\">\n",
        "      <div style=\"flex:1; line-height:2\">\n",
        "        <div><b>{pred_title}</b></div>\n",
        "        <div>{html_left}</div>\n",
        "      </div>\n",
        "      <div style=\"flex:1; line-height:2\">\n",
        "        <div><b>{true_title}</b></div>\n",
        "        <div>{html_right}</div>\n",
        "      </div>\n",
        "    </div>\n",
        "    \"\"\"))\n",
        "\n",
        "# Usage:\n",
        "# side_by_side_ordered(\n",
        "#     r0[\"pred_ordered\"], r0[\"true_ordered\"],\n",
        "#     f\"Predicted: {r0['meta']['Predicted Author']}\",\n",
        "#     f\"True: {r0['meta']['Expected Author']}\"\n",
        "# )\n"
      ],
      "metadata": {
        "id": "aLW3WI1Xghev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage:\n",
        "side_by_side_ordered(\n",
        "     r0[\"pred_ordered\"], r0[\"true_ordered\"],\n",
        "     f\"Predicted: {r0['meta']['Predicted Author']}\",\n",
        "     f\"True: {r0['meta']['Expected Author']}\"\n",
        " )"
      ],
      "metadata": {
        "id": "Lp9_r_ICgkT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# your label map (same order you trained with)\n",
        "label_map = {\n",
        "    0: \"Charles Dickens\",\n",
        "    1: \"Jane Austen\",\n",
        "    2: \"Mark Twain\",\n",
        "    3: \"Louisa May Alcott\",\n",
        "    4: \"Herman Melville\",\n",
        "}\n",
        "\n",
        "text = \"made the place the scene of their holiday entertainment.\"\n",
        "\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# tokenize exactly as in training\n",
        "enc = tokenizer(text, truncation=True, max_length=256, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.inference_mode():\n",
        "    logits = model(**enc).logits            # [1, 5]\n",
        "    probs  = torch.softmax(logits, dim=-1)  # [1, 5]\n",
        "\n",
        "pred_idx = int(torch.argmax(probs, dim=-1).item())\n",
        "pred_author = label_map[pred_idx]\n",
        "probs_np = probs.squeeze(0).cpu().numpy()\n",
        "\n",
        "print(\"Predicted:\", pred_author, \"\\n\")\n",
        "for i, p in enumerate(probs_np):\n",
        "    print(f\"{label_map[i]:20s}  {p:.4f}\")\n"
      ],
      "metadata": {
        "id": "cAkT2959jxhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_texts(texts, model, tokenizer, max_length=256):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    enc = tokenizer(\n",
        "        texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "    with torch.inference_mode():\n",
        "        logits = model(**enc).logits\n",
        "        probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
        "    preds = probs.argmax(axis=1)\n",
        "    authors = [label_map[i] for i in preds]\n",
        "    return authors, probs  # list[str], np.ndarray [N,5]\n",
        "\n",
        "authors, probs = predict_texts([text], model, tokenizer)\n",
        "print(\"Predicted:\", authors[0])\n"
      ],
      "metadata": {
        "id": "H96wlyAtkWkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wnye2orTkWnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ilfNGPBykWr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1y7g-rWsd6nM"
      },
      "outputs": [],
      "source": [
        "def differential_attribution(pred_df, true_df, k=15):\n",
        "    # merge on word; fill missing with 0\n",
        "    df = pred_df[[\"word\",\"attr\"]].merge(\n",
        "        true_df[[\"word\",\"attr\"]], on=\"word\", how=\"outer\", suffixes=(\"_pred\",\"_true\")\n",
        "    ).fillna(0.0)\n",
        "    df[\"diff\"] = df[\"attr_pred\"] - df[\"attr_true\"]\n",
        "    df[\"diff_abs\"] = df[\"diff\"].abs()\n",
        "    df = df.sort_values(\"diff_abs\", ascending=False).head(k)\n",
        "    return df[[\"word\",\"attr_pred\",\"attr_true\",\"diff\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFfnBJHyd6nM"
      },
      "outputs": [],
      "source": [
        "# Example:\n",
        "# diff > 0: evidence for the predicted class.\n",
        "# diff < 0: evidence for the true class.\n",
        "\n",
        "diff = differential_attribution(r0[\"pred_df\"], r0[\"true_df\"], k=20)\n",
        "display(diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2yEB9HGd6nM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def mask_and_score(model, tokenizer, text, target_class, max_length=256, device=None, words_to_mask=None):\n",
        "    \"\"\"\n",
        "    Replace chosen words with <mask> (roughly) and return target_class prob delta.\n",
        "    \"\"\"\n",
        "    device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "    model.to(device).eval()\n",
        "\n",
        "    # crude word masking: replace exact substrings; for robust masking, map tokens instead\n",
        "    masked_text = text\n",
        "    if words_to_mask:\n",
        "        for w in words_to_mask:\n",
        "            if w and w.strip():\n",
        "                masked_text = masked_text.replace(w, tokenizer.mask_token or \"<mask>\")\n",
        "\n",
        "    enc = tokenizer(masked_text, truncation=True, max_length=max_length, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**enc).logits\n",
        "        probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
        "    return probs[target_class], probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yJ3mA_Rd6nM"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "target_pred = r0[\"meta\"][\"predicted_label\"]\n",
        "p_with, _ = mask_and_score(model, tokenizer, r0[\"meta\"][\"text\"], target_pred, words_to_mask=[\"Marianne\"])\n",
        "print(\"P(pred class) after masking:\", p_with)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNKmEf7Jd6nM"
      },
      "outputs": [],
      "source": [
        "def explain_with_diff(df, row_idx, model, tokenizer, max_length=256, device=None, top_k=15, do_ablate=True):\n",
        "    out = explain_row(df, row_idx, model, tokenizer, max_length=max_length, device=device, top_k=top_k)\n",
        "    pred_df, true_df = out[\"pred_df\"], out[\"true_df\"]\n",
        "    diff = differential_attribution(pred_df, true_df, k=top_k)\n",
        "\n",
        "    print(\"META:\", {k:v for k,v in out[\"meta\"].items() if k not in (\"probs\",)})\n",
        "    print(\"\\n--- Top tokens toward PREDICTED class ---\")\n",
        "    display(pred_df)\n",
        "    print(\"\\n--- Top tokens toward TRUE class ---\")\n",
        "    display(true_df)\n",
        "    print(\"\\n--- Differential (pred - true) ---\")\n",
        "    display(diff)\n",
        "\n",
        "    if do_ablate:\n",
        "        text = out[\"meta\"][\"text\"]\n",
        "        pred_cls = out[\"meta\"][\"predicted_label\"]\n",
        "        # test top-3 separating tokens (positive diff)\n",
        "        top_sep = diff.sort_values(\"diff\", ascending=False).head(3)[\"word\"].tolist()\n",
        "        try:\n",
        "            p_after, _ = mask_and_score(model, tokenizer, text, pred_cls, max_length=max_length, device=device, words_to_mask=top_sep)\n",
        "            print(f\"\\nAblation: masking {top_sep} -> P(pred_class) = {p_after:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(\"Ablation error:\", e)\n",
        "\n",
        "    return out, diff\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lYDhvI_d6nM"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example:\n",
        "_ = explain_with_diff(generated_texts_merged, reps.index[0], model, tokenizer, max_length=256)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05HHnVaSH2w5"
      },
      "source": [
        "## ---- Explainability (global):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1zWia8t19Tq"
      },
      "outputs": [],
      "source": [
        "# Differential attribution for the pair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT5SnUOJGb8T"
      },
      "outputs": [],
      "source": [
        "# ---- CONFIG ----\n",
        "PAIR_TRUE, PAIR_PRED = 1, 2      # Austen=1, Twain=2  (you can flip later)\n",
        "MAXLEN = 256\n",
        "N_PER_SIDE = 150                 # how many errors to sample per confusion direction\n",
        "TOP_K_TOKENS = 20                # how many tokens to show in the bars\n",
        "\n",
        "# sample errors for both directions\n",
        "a_to_t = generated_texts_merged.query(\"expected_label==@PAIR_TRUE and predicted_Label_roberta==@PAIR_PRED\")\n",
        "t_to_a = generated_texts_merged.query(\"expected_label==@PAIR_PRED and predicted_Label_roberta==@PAIR_TRUE\")\n",
        "\n",
        "a_to_t = a_to_t.nlargest(N_PER_SIDE, \"confidence\") if len(a_to_t)>N_PER_SIDE else a_to_t\n",
        "t_to_a = t_to_a.nlargest(N_PER_SIDE, \"confidence\") if len(t_to_a)>N_PER_SIDE else t_to_a\n",
        "\n",
        "# helper to run IG on many rows and aggregate word-level diff\n",
        "from collections import defaultdict\n",
        "\n",
        "def aggregate_pair_diff(rows, true_cls, pred_cls, model, tokenizer, max_length=256, device=None):\n",
        "    token2sum = defaultdict(float)\n",
        "    token2cnt = defaultdict(int)\n",
        "    for idx in rows.index:\n",
        "        out = explain_row(generated_texts_merged, idx, model, tokenizer, max_length=max_length, device=device, top_k=999)\n",
        "        pred_df, true_df = out[\"pred_df\"], out[\"true_df\"]\n",
        "        diff = (pred_df[[\"word\",\"attr\"]].merge(true_df[[\"word\",\"attr\"]], on=\"word\", how=\"outer\",\n",
        "                                               suffixes=(\"_pred\",\"_true\")).fillna(0.0))\n",
        "        diff[\"d\"] = diff[\"attr_pred\"] - diff[\"attr_true\"]\n",
        "        for w, d in zip(diff[\"word\"], diff[\"d\"]):\n",
        "            token2sum[w] += float(d)\n",
        "            token2cnt[w] += 1\n",
        "    # average diff per token\n",
        "    items = [(w, token2sum[w]/max(token2cnt[w],1), token2cnt[w]) for w in token2sum]\n",
        "    df = pd.DataFrame(items, columns=[\"word\",\"mean_diff\",\"count\"])\n",
        "    # filter trivial tokens\n",
        "    df = df[~df[\"word\"].str.match(r\"^(\\W+|)$\")]\n",
        "    return df.sort_values(\"mean_diff\", ascending=False)\n",
        "\n",
        "# aggregate for both directions\n",
        "df_a_to_t = aggregate_pair_diff(a_to_t, true_cls=PAIR_TRUE, pred_cls=PAIR_PRED, model=model, tokenizer=tokenizer, max_length=MAXLEN)\n",
        "df_t_to_a = aggregate_pair_diff(t_to_a, true_cls=PAIR_PRED, pred_cls=PAIR_TRUE, model=model, tokenizer=tokenizer, max_length=MAXLEN)\n",
        "\n",
        "# plot top tokens pushing toward Twain (positive) and toward Austen (negative)\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_top_tokens(df, title, k=TOP_K_TOKENS):\n",
        "    top_pos = df.head(k)\n",
        "    top_neg = df.tail(k).sort_values(\"mean_diff\")\n",
        "    fig, ax = plt.subplots(figsize=(8,6))\n",
        "    y = list(top_pos[\"word\"]) + list(top_neg[\"word\"])\n",
        "    x = list(top_pos[\"mean_diff\"]) + list(top_neg[\"mean_diff\"])\n",
        "    ax.barh(y, x)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"Mean differential attribution (pred - true)\")\n",
        "    ax.axvline(0, linestyle=\"--\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_top_tokens(df_a_to_t, \"Austen→Twain errors: tokens pushing Twain (+) vs Austen (-)\")\n",
        "plot_top_tokens(df_t_to_a, \"Twain→Austen errors: tokens pushing Austen (+) vs Twain (-)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlyL1LY5H5Em"
      },
      "outputs": [],
      "source": [
        "# Token frequency vs. differential attribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICvyhLbfH5Hi"
      },
      "outputs": [],
      "source": [
        "def scatter_freq_vs_diff(df_diff, title):\n",
        "    # approximate frequency from 'count' (how often token contributed across examples)\n",
        "    freq = df_diff[\"count\"].astype(float)\n",
        "    mean_diff = df_diff[\"mean_diff\"].astype(float)\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(7,6))\n",
        "    plt.scatter(freq, mean_diff, alpha=0.6)\n",
        "    plt.xlabel(\"Token frequency in confused examples\")\n",
        "    plt.ylabel(\"Mean differential attribution (pred - true)\")\n",
        "    plt.title(title)\n",
        "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "    plt.show()\n",
        "\n",
        "scatter_freq_vs_diff(df_a_to_t, \"Austen→Twain: frequency vs differential attribution\")\n",
        "scatter_freq_vs_diff(df_t_to_a, \"Twain→Austen: frequency vs differential attribution\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRdadg-UH5Kb"
      },
      "outputs": [],
      "source": [
        "# Prototype & counter-prototype snippets (gallery)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62fNApZx2ELI"
      },
      "outputs": [],
      "source": [
        "def top_prototypes(df_pair, k=6):\n",
        "    # already filtered to a confusion pair; pick by highest confidence\n",
        "    return df_pair.nlargest(k, \"confidence\")[[\"Text\",\"Expected Author\",\"Predicted Author\",\"confidence\"]]\n",
        "\n",
        "# Example galleries:\n",
        "proto_a_to_t = top_prototypes(a_to_t, k=6)\n",
        "proto_t_to_a = top_prototypes(t_to_a, k=6)\n",
        "proto_a_to_t.head(), proto_t_to_a.head()\n",
        "\n",
        "# (Optional) For each text in the gallery, call explain_row(...) then show_attribution_text(...) as you did.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OP2sNACt2EO7"
      },
      "outputs": [],
      "source": [
        "# Counterfactual impact of top tokens (average Δprob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2FHFptF2ETd"
      },
      "outputs": [],
      "source": [
        "def average_token_impact(df_rows, target_class, tokens, model, tokenizer, max_length=256, device=None, sample_n=100):\n",
        "    rows = df_rows.nlargest(sample_n, \"confidence\") if len(df_rows)>sample_n else df_rows\n",
        "    impacts = []\n",
        "    for t in tokens:\n",
        "        deltas = []\n",
        "        for _, r in rows.iterrows():\n",
        "            p0, _ = mask_and_score(model, tokenizer, r[\"Text\"], target_class, max_length=max_length, device=device, words_to_mask=[])\n",
        "            p1, _ = mask_and_score(model, tokenizer, r[\"Text\"], target_class, max_length=max_length, device=device, words_to_mask=[t])\n",
        "            deltas.append(p0 - p1)  # drop in prob when masking token t\n",
        "        impacts.append((t, float(np.mean(deltas)), len(deltas)))\n",
        "    out = pd.DataFrame(impacts, columns=[\"token\",\"mean_delta_prob\",\"n\"])\n",
        "    return out.sort_values(\"mean_delta_prob\", ascending=False)\n",
        "\n",
        "# Example: take top 15 Twain-pushing tokens from Austen→Twain errors and measure their avg impact on P(Twain)\n",
        "twain_tokens = df_a_to_t.head(15)[\"word\"].tolist()\n",
        "impact_tbl = average_token_impact(a_to_t, target_class=2, tokens=twain_tokens, model=model, tokenizer=tokenizer, max_length=MAXLEN, sample_n=80)\n",
        "impact_tbl.head(15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEKeE2Ww2Ucx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ---- IntegratedGradients:"
      ],
      "metadata": {
        "id": "0RrulZEBrwv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 – Setup\n",
        "\n",
        "!pip install captum\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from captum.attr import IntegratedGradients\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval().to(device)\n"
      ],
      "metadata": {
        "id": "02KYMetvrzCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_embeddings_module(model):\n",
        "    for name in [\"deberta\", \"roberta\", \"bert\", \"distilbert\", \"albert\", \"electra\"]:\n",
        "        if hasattr(model, name):\n",
        "            emb = getattr(model, name).embeddings\n",
        "            if hasattr(emb, \"word_embeddings\"):\n",
        "                return emb.word_embeddings\n",
        "    # Fallback\n",
        "    if hasattr(model, \"embeddings\") and hasattr(model.embeddings, \"word_embeddings\"):\n",
        "        return model.embeddings.word_embeddings\n",
        "    raise AttributeError(\"Could not locate embeddings.word_embeddings on this model.\")\n",
        "\n",
        "def _forward_from_embeds(model, attention_mask, token_type_ids, inputs_embeds):\n",
        "    out = model(\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids if token_type_ids is not None else None,\n",
        "        inputs_embeds=inputs_embeds,\n",
        "    )\n",
        "    return out.logits\n"
      ],
      "metadata": {
        "id": "lPcIttG4tWUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attribution for a Single Sentence:\n",
        "\n",
        "def get_token_attributions_embeds(model, tokenizer, text, target_class, max_length=128, n_steps=32, device=None):\n",
        "    model.eval()\n",
        "    device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "    model.to(device)\n",
        "\n",
        "    enc = tokenizer(text, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    input_ids = enc[\"input_ids\"].to(device)\n",
        "    attention_mask = enc.get(\"attention_mask\", None)\n",
        "    attention_mask = attention_mask.to(device) if attention_mask is not None else None\n",
        "    token_type_ids = enc.get(\"token_type_ids\", None)\n",
        "    token_type_ids = token_type_ids.to(device) if token_type_ids is not None else None\n",
        "\n",
        "    we = get_word_embeddings_module(model)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputs_embeds  = we(input_ids)                         # [1, L, H]\n",
        "        pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
        "        baseline_ids   = torch.full_like(input_ids, pad_id)\n",
        "        baseline_embeds = we(baseline_ids)\n",
        "\n",
        "    def target_logit_from_embeds(inp_emb):\n",
        "        logits = _forward_from_embeds(model, attention_mask, token_type_ids, inp_emb)\n",
        "        return logits[:, target_class]\n",
        "\n",
        "    ig = IntegratedGradients(target_logit_from_embeds)\n",
        "    # returns a single tensor when return_convergence_delta=False\n",
        "    attributions = ig.attribute(\n",
        "        inputs=inputs_embeds,\n",
        "        baselines=baseline_embeds,\n",
        "        n_steps=n_steps,\n",
        "        return_convergence_delta=False,\n",
        "    )\n",
        "\n",
        "    # reduce embedding-dim → per-token score\n",
        "    token_attr = attributions.sum(dim=-1).squeeze(0).detach().cpu().numpy()\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0).detach().cpu().tolist())\n",
        "    return tokens, token_attr\n"
      ],
      "metadata": {
        "id": "XrTIauCIsCv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "def global_author_attributions(df, model, tokenizer,\n",
        "                               sample_size=None,   # ← None = use ALL rows\n",
        "                               max_length=128, n_steps=32,\n",
        "                               by=\"Expected Author\"):\n",
        "    \"\"\"\n",
        "    Aggregate token attributions per author on df (optionally filtered before).\n",
        "    by: \"Expected Author\" (ground-truth view) or \"Predicted Author\" (model-view).\n",
        "    \"\"\"\n",
        "    import numpy as np, pandas as pd\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    authors = df[by].unique().tolist()\n",
        "    agg = {a: {} for a in authors}\n",
        "\n",
        "    if sample_size is None:\n",
        "        sampled = df\n",
        "    else:\n",
        "        sampled = df.sample(min(sample_size, len(df)), random_state=42)\n",
        "\n",
        "    for _, row in tqdm(sampled.iterrows(), total=len(sampled)):\n",
        "        text = row[\"Text\"]\n",
        "        target_cls = int(row[\"expected_label\"]) if by == \"Expected Author\" else int(row[\"predicted_Label_roberta\"])\n",
        "\n",
        "        toks, attrs = get_token_attributions_embeds(\n",
        "            model, tokenizer, text,\n",
        "            target_class=target_cls, max_length=max_length, n_steps=n_steps\n",
        "        )\n",
        "\n",
        "        for tok, val in zip(toks, attrs):\n",
        "            if tok in {\"<s>\", \"</s>\", \"<pad>\", \"[CLS]\", \"[SEP]\", \"[PAD]\"}:\n",
        "                continue\n",
        "            if tok.startswith(\"▁\") or tok.startswith(\"Ġ\"):\n",
        "                tok = tok[1:]\n",
        "            if not tok:\n",
        "                continue\n",
        "            a = row[by]\n",
        "            agg[a][tok] = agg[a].get(tok, 0.0) + float(val)\n",
        "\n",
        "    author_dfs = {}\n",
        "    for a in authors:\n",
        "        df_tok = pd.DataFrame(list(agg[a].items()), columns=[\"token\", \"total_attr\"])\n",
        "        df_tok[\"abs_attr\"] = df_tok[\"total_attr\"].abs()\n",
        "        author_dfs[a] = df_tok.sort_values(\"abs_attr\", ascending=False).reset_index(drop=True)\n",
        "    return author_dfs\n",
        "\n"
      ],
      "metadata": {
        "id": "WIBB5BqzsR7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_top_tokens(author_df, author_name, top_n=30):\n",
        "    top_df = author_df.head(top_n)\n",
        "    plt.figure(figsize=(4,6))\n",
        "    plt.barh(top_df[\"token\"], top_df[\"total_attr\"])\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.title(f\"Top tokens influencing: {author_name}\")\n",
        "    plt.xlabel(\"Aggregated attribution\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "dmAGPRM6sXP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_conf_df = generated_texts_merged[generated_texts_merged[\"confidence\"] > 0.93].copy()\n",
        "print(\"High‑confidence samples:\", len(high_conf_df))"
      ],
      "metadata": {
        "id": "sCF0X8L7xN71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5 – Run\n",
        "\n",
        "author_results_all = global_author_attributions(\n",
        "    high_conf_df, model, tokenizer,\n",
        "    sample_size=None,      # ← use ALL high‑confidence rows\n",
        "    max_length=128,        # increase to 256 if you can afford it\n",
        "    n_steps=32,            # 16–32 is a good trade‑off\n",
        "    by=\"Expected Author\"   # or \"Predicted Author\" for model‑centric view\n",
        ")"
      ],
      "metadata": {
        "id": "9uce165wsesT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save\n",
        "import pickle\n",
        "\n",
        "\n",
        "# Save dictionary to a pickle file\n",
        "with open(\"author_results_all.pkl\", \"wb\") as f:\n",
        "    pickle.dump(author_results_all, f)"
      ],
      "metadata": {
        "id": "CEn2WNr0Su6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load\n",
        "import pickle\n",
        "\n",
        "with open(\"author_results_all.pkl\", \"rb\") as f:\n",
        "    author_results_all = pickle.load(f)"
      ],
      "metadata": {
        "id": "YemkxZk1U8tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for author, df_tok in author_results_all.items():\n",
        "    plot_top_tokens(df_tok, author, top_n=15)"
      ],
      "metadata": {
        "id": "RKg8Cm7mspjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8r-Ue0rUUevj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ---- XAI Generators:"
      ],
      "metadata": {
        "id": "zt-TAXUMUhND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install captum"
      ],
      "metadata": {
        "id": "OFwMm6-1bANm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, math, random\n",
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
        "from torch.nn import functional as F\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Your paths\n",
        "model = GPTNeoForCausalLM.from_pretrained(drive_model_path).to(device).eval()\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(drive_tokenizer_path)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # safety\n",
        "\n",
        "TAGS = [\"<0>\", \"<1>\", \"<2>\", \"<3>\", \"<4>\"]  # Dickens, Austen, Twain, Alcott, Melville"
      ],
      "metadata": {
        "id": "gP0gEuCubAQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional) ensure the model is set to return attentions by default\n",
        "model.config.output_attentions = True\n",
        "\n",
        "@torch.no_grad()\n",
        "def attention_to_from_tag_better(prompt_text, max_new_tokens=40):\n",
        "    full = generate_text(prompt_text, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    enc = tokenizer(full, return_tensors=\"pt\").to(model.device)\n",
        "    out = model(**enc, output_attentions=True)\n",
        "    attns = out.attentions  # tuple of [B,H,T,T]\n",
        "    attn_mask = enc[\"attention_mask\"][0].bool()  # [T]\n",
        "    T = int(attn_mask.sum().item())              # exclude PAD at the tail\n",
        "    valid_idx = torch.arange(enc[\"input_ids\"].size(1), device=model.device)[attn_mask]\n",
        "\n",
        "    # Find tag span at the start (like before)\n",
        "    text_stripped = prompt_text.lstrip()\n",
        "    tag = next((t for t in TAGS if text_stripped.startswith(t)), None)\n",
        "    if tag is None:\n",
        "        tag = text_stripped.split()[0] if text_stripped else TAGS[0]\n",
        "    tag_ids = tokenizer(tag, add_special_tokens=False)[\"input_ids\"]\n",
        "    if prompt_text.startswith(tag + \" \"):\n",
        "        tag_ids_space = tokenizer(tag + \" \", add_special_tokens=False)[\"input_ids\"]\n",
        "        if tag_ids_space == enc[\"input_ids\"][0, :len(tag_ids_space)].tolist():\n",
        "            tag_ids = tag_ids_space\n",
        "\n",
        "    tag_start, tag_end = 0, len(tag_ids)  # exclusive\n",
        "    tag_len = tag_end - tag_start\n",
        "\n",
        "    layer_stats = []\n",
        "    for A in attns:  # [B,H,T,T]\n",
        "        A = A.squeeze(0)[:, attn_mask, :][:, :, attn_mask]  # [H, T, T] over valid tokens only\n",
        "        A_mean = A.mean(dim=0)  # average over heads -> [T,T]\n",
        "\n",
        "        # ---- TO TAG ----\n",
        "        to_tag_mass = A_mean[:, tag_start:tag_end].mean().item()\n",
        "        baseline_to_tag = tag_len / T\n",
        "        to_tag_enrichment = to_tag_mass / baseline_to_tag if baseline_to_tag > 0 else float('nan')\n",
        "\n",
        "        # ---- FROM TAG (meaningful) ----\n",
        "        tag_rows = A_mean[tag_start:tag_end, :]  # [tag_len, T]\n",
        "        # exclude the tag columns when measuring broadcast\n",
        "        non_tag_cols = torch.cat([torch.arange(0, tag_start, device=model.device),\n",
        "                                  torch.arange(tag_end, T, device=model.device)])\n",
        "        if non_tag_cols.numel() > 0:\n",
        "            from_tag_non_tag_mean = tag_rows[:, non_tag_cols].mean().item()\n",
        "            from_tag_max = tag_rows[:, non_tag_cols].max().item()\n",
        "        else:\n",
        "            from_tag_non_tag_mean = float('nan')\n",
        "            from_tag_max = float('nan')\n",
        "\n",
        "        layer_stats.append({\n",
        "            \"to_tag\": to_tag_mass,\n",
        "            \"to_tag_enrichment\": to_tag_enrichment,\n",
        "            \"from_tag_non_tag_mean\": from_tag_non_tag_mean,\n",
        "            \"from_tag_max\": from_tag_max,\n",
        "            \"T\": T,\n",
        "            \"tag_len\": tag_len\n",
        "        })\n",
        "\n",
        "    return full, layer_stats\n"
      ],
      "metadata": {
        "id": "Pu4QRZ4G0Vyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "full, stats = attention_to_from_tag_better(\"<0>\")\n",
        "for L, s in enumerate(stats):\n",
        "    print(f\"Layer {L:02d}: to_tag={s['to_tag']:.4f} (x{s['to_tag_enrichment']:.1f}), \"\n",
        "          f\"from_tag_mean(non-tag)={s['from_tag_non_tag_mean']:.4f}, \"\n",
        "          f\"from_tag_max={s['from_tag_max']:.4f}\")\n",
        "print(\"\\nSample:\\n\", full)\n"
      ],
      "metadata": {
        "id": "ECzueeZY3Jk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "full, stats = attention_to_from_tag_better(\"<1>\")\n",
        "for L, s in enumerate(stats):\n",
        "    print(f\"Layer {L:02d}: to_tag={s['to_tag']:.4f} (x{s['to_tag_enrichment']:.1f}), \"\n",
        "          f\"from_tag_mean(non-tag)={s['from_tag_non_tag_mean']:.4f}, \"\n",
        "          f\"from_tag_max={s['from_tag_max']:.4f}\")\n",
        "print(\"\\nSample:\\n\", full)\n"
      ],
      "metadata": {
        "id": "legN5kvVER6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "full, stats = attention_to_from_tag_better(\"<2> \")\n",
        "for L, s in enumerate(stats):\n",
        "    print(f\"Layer {L:02d}: to_tag={s['to_tag']:.4f} (x{s['to_tag_enrichment']:.1f}), \"\n",
        "          f\"from_tag_mean(non-tag)={s['from_tag_non_tag_mean']:.4f}, \"\n",
        "          f\"from_tag_max={s['from_tag_max']:.4f}\")\n",
        "print(\"\\nSample:\\n\", full)\n"
      ],
      "metadata": {
        "id": "NTS8kkjEEvL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "full, stats = attention_to_from_tag_better(\"<3> \")\n",
        "for L, s in enumerate(stats):\n",
        "    print(f\"Layer {L:02d}: to_tag={s['to_tag']:.4f} (x{s['to_tag_enrichment']:.1f}), \"\n",
        "          f\"from_tag_mean(non-tag)={s['from_tag_non_tag_mean']:.4f}, \"\n",
        "          f\"from_tag_max={s['from_tag_max']:.4f}\")\n",
        "print(\"\\nSample:\\n\", full)\n"
      ],
      "metadata": {
        "id": "cImNtdqzE0st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "full, stats = attention_to_from_tag_better(\"<4> \")\n",
        "for L, s in enumerate(stats):\n",
        "    print(f\"Layer {L:02d}: to_tag={s['to_tag']:.4f} (x{s['to_tag_enrichment']:.1f}), \"\n",
        "          f\"from_tag_mean(non-tag)={s['from_tag_non_tag_mean']:.4f}, \"\n",
        "          f\"from_tag_max={s['from_tag_max']:.4f}\")\n",
        "print(\"\\nSample:\\n\", full)\n"
      ],
      "metadata": {
        "id": "u8Hbg0tNE3JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################"
      ],
      "metadata": {
        "id": "UJ_iOi2pE476"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.eval();"
      ],
      "metadata": {
        "id": "atcxe8jdFNp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generation (explain one sample)\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_once(prompt, max_new_tokens=40, temperature=0.8, top_p=0.95, seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    out_ids = model.generate(\n",
        "        **enc,\n",
        "        do_sample=True,\n",
        "        top_p=top_p,\n",
        "        temperature=temperature,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )[0]\n",
        "    text = tokenizer.decode(out_ids, skip_special_tokens=True)\n",
        "    return out_ids.unsqueeze(0), text  # [1, T], string\n",
        "\n",
        "\n",
        "# Integrated Gradients\n",
        "\n",
        "def build_baseline_and_inputs(input_ids, prompt_len):\n",
        "    with torch.no_grad():\n",
        "        base_no_grad = model.transformer.wte(input_ids)  # [1, T, d]\n",
        "    # Captum will backprop through 'inputs' only; they must require_grad=True\n",
        "    inputs_embeds = base_no_grad.clone().detach().requires_grad_(True)\n",
        "\n",
        "    baseline = torch.zeros_like(inputs_embeds)\n",
        "    # keep generated part fixed in both baseline and inputs to focus attribution on prompt\n",
        "    baseline[:, prompt_len:, :] = base_no_grad[:, prompt_len:, :]\n",
        "    return inputs_embeds, baseline\n",
        "\n",
        "def token_logprob_from_embeds(embeds, attention_mask, target_index, target_token_id):\n",
        "    \"\"\"\n",
        "    Returns shape (1,) tensor with log P(x[target_index] | x[:target_index]).\n",
        "    \"\"\"\n",
        "    outputs = model(inputs_embeds=embeds, attention_mask=attention_mask)\n",
        "    logits = outputs.logits  # [1, T, V]\n",
        "    logprobs = F.log_softmax(logits[:, :-1, :], dim=-1)\n",
        "    pred_pos = target_index - 1\n",
        "    lp_scalar = logprobs[0, pred_pos, target_token_id]  # 0-dim scalar\n",
        "    return lp_scalar.unsqueeze(0)  # -> shape (1,)\n",
        "\n",
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "def compute_ig_matrix_for_generation(input_ids, prompt_len, n_steps=32, internal_bs=1):\n",
        "    attention_mask = torch.ones_like(input_ids, device=input_ids.device)\n",
        "    T = input_ids.size(1)\n",
        "    gen_start = prompt_len\n",
        "    gen_len = T - gen_start\n",
        "\n",
        "    all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
        "    prompt_tokens = all_tokens[:prompt_len]\n",
        "    gen_tokens = all_tokens[gen_start:]\n",
        "\n",
        "    # embeddings to attribute + baseline (both on device)\n",
        "    inputs_embeds, baseline = build_baseline_and_inputs(input_ids, prompt_len)\n",
        "\n",
        "    ig_matrix = np.zeros((prompt_len, gen_len), dtype=np.float32)\n",
        "\n",
        "    for j_abs in range(gen_start, T):\n",
        "        target_token_id = input_ids[0, j_abs].item()\n",
        "\n",
        "        # forward that returns (1,) tensor for Captum\n",
        "        def forward_scalar(embeds):\n",
        "            return token_logprob_from_embeds(\n",
        "                embeds, attention_mask, j_abs, target_token_id\n",
        "            )\n",
        "\n",
        "        ig = IntegratedGradients(forward_scalar)\n",
        "\n",
        "        # IMPORTANT: pass inputs that require grad; baseline need not require grad\n",
        "        attributions, _ = ig.attribute(\n",
        "            inputs=inputs_embeds,\n",
        "            baselines=baseline,\n",
        "            n_steps=n_steps,\n",
        "            internal_batch_size=internal_bs,\n",
        "            return_convergence_delta=True\n",
        "        )\n",
        "\n",
        "        tok_scores = attributions.norm(dim=-1).squeeze(0)  # [T]\n",
        "        ig_matrix[:, j_abs - gen_start] = tok_scores[:prompt_len].detach().cpu().numpy()\n",
        "\n",
        "    return ig_matrix, prompt_tokens, gen_tokens\n",
        "\n",
        "\n",
        "# Plotting utilities\n",
        "\n",
        "def plot_heatmap(ig_matrix, prompt_tokens, gen_tokens, title=\"Token Attribution Heatmap (IG)\"):\n",
        "    plt.figure(figsize=(min(18, 2 + 0.3 * len(gen_tokens)), min(10, 2 + 0.25 * len(prompt_tokens))))\n",
        "    im = plt.imshow(ig_matrix, aspect='auto', interpolation='nearest')\n",
        "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "    plt.xticks(ticks=np.arange(len(gen_tokens)), labels=gen_tokens, rotation=90)\n",
        "    plt.yticks(ticks=np.arange(len(prompt_tokens)), labels=prompt_tokens)\n",
        "    plt.xlabel(\"Generated tokens\")\n",
        "    plt.ylabel(\"Prompt tokens (incl. tag)\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_prompt_importance(ig_matrix, prompt_tokens, title=\"Prompt Token Importance (sum over generated)\"):\n",
        "    scores = ig_matrix.sum(axis=1)\n",
        "    plt.figure(figsize=(max(6, 0.4 * len(prompt_tokens)), 3.5))\n",
        "    plt.bar(range(len(prompt_tokens)), scores)\n",
        "    plt.xticks(range(len(prompt_tokens)), prompt_tokens, rotation=90)\n",
        "    plt.ylabel(\"IG magnitude (sum over generated)\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# One-call helper\n",
        "\n",
        "def explain_generation_with_ig(prompt, max_new_tokens=40, n_steps=32, seed=42):\n",
        "    input_ids, full_text = generate_once(prompt, max_new_tokens=max_new_tokens, seed=seed)\n",
        "    prompt_len = len(tokenizer(prompt)[\"input_ids\"])\n",
        "\n",
        "    ig_matrix, prompt_tokens, gen_tokens = compute_ig_matrix_for_generation(\n",
        "        input_ids.to(device), prompt_len, n_steps=n_steps\n",
        "    )\n",
        "    print(\"FULL TEXT:\\n\", tokenizer.decode(input_ids[0], skip_special_tokens=True))\n",
        "    plot_heatmap(ig_matrix, prompt_tokens, gen_tokens,\n",
        "                 title=f\"IG Heatmap | prompt='{prompt.strip()}'\")\n",
        "    plot_prompt_importance(ig_matrix, prompt_tokens)"
      ],
      "metadata": {
        "id": "FfmN1y1DFTKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run it (example)\n",
        "\n",
        "# Use your tags as the prompt start (e.g., \"<0> \", \"<1> \", etc.)\n",
        "\n",
        "explain_generation_with_ig(\"<0>\", max_new_tokens=40, n_steps=32, seed=123)"
      ],
      "metadata": {
        "id": "tY93O530GFYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run it (example)\n",
        "\n",
        "# Use your tags as the prompt start (e.g., \"<0> \", \"<1> \", etc.)\n",
        "\n",
        "explain_generation_with_ig(\"<1>\", max_new_tokens=40, n_steps=32, seed=123)"
      ],
      "metadata": {
        "id": "dZrNtnXDGFbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run it (example)\n",
        "\n",
        "# Use your tags as the prompt start (e.g., \"<0> \", \"<1> \", etc.)\n",
        "\n",
        "explain_generation_with_ig(\"<2>\", max_new_tokens=40, n_steps=32, seed=123)"
      ],
      "metadata": {
        "id": "nGb-Z3yiLwSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run it (example)\n",
        "\n",
        "# Use your tags as the prompt start (e.g., \"<0> \", \"<1> \", etc.)\n",
        "\n",
        "explain_generation_with_ig(\"<3>\", max_new_tokens=40, n_steps=32, seed=123)"
      ],
      "metadata": {
        "id": "DXFfmZ2cLxjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run it (example)\n",
        "\n",
        "# Use your tags as the prompt start (e.g., \"<0> \", \"<1> \", etc.)\n",
        "\n",
        "explain_generation_with_ig(\"<4>\", max_new_tokens=40, n_steps=32, seed=123)"
      ],
      "metadata": {
        "id": "eYeziC8GLynZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention Head Visualizations"
      ],
      "metadata": {
        "id": "zWtQv8rFN0Vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTNeoForCausalLM.from_pretrained(drive_model_path).to(device).eval()\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(drive_tokenizer_path)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ],
      "metadata": {
        "id": "utBHgcWvOCoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_once(prompt, max_new_tokens=40, temperature=0.8, top_p=0.95, seed=123):\n",
        "    torch.manual_seed(seed)\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    out = model.generate(\n",
        "        **enc,\n",
        "        do_sample=True, top_p=top_p, temperature=temperature,\n",
        "        max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id\n",
        "    )[0]\n",
        "    text = tokenizer.decode(out, skip_special_tokens=True)\n",
        "    return out.unsqueeze(0), text  # [1, T], string\n",
        "\n",
        "@torch.no_grad()\n",
        "def forward_with_attn(input_ids):\n",
        "    out = model(input_ids=input_ids, output_attentions=True, output_hidden_states=False)\n",
        "    # out.attentions: tuple(len=num_layers) of [1, num_heads, T, T]\n",
        "    return out.attentions\n"
      ],
      "metadata": {
        "id": "1SA_gc7XOEBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tag_span_indices(prompt_text):\n",
        "    # Tokenize the exact prompt; the tag is at the start of the prompt\n",
        "    ids = tokenizer(prompt_text, return_tensors=\"pt\")[\"input_ids\"][0].tolist()\n",
        "    # If you pass prompt like \"<0> \" this returns however many subtokens that is.\n",
        "    return list(range(len(ids)))  # tag is the whole prompt here\n"
      ],
      "metadata": {
        "id": "1QqUujyNOF02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tag_len_for(tag_text):\n",
        "    return len(tokenizer(tag_text)[\"input_ids\"])\n"
      ],
      "metadata": {
        "id": "J4F62ovfPdKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attn_matrix(mat, tokens, title):\n",
        "    plt.figure(figsize=(min(18, 2 + 0.25*len(tokens)), min(18, 2 + 0.25*len(tokens))))\n",
        "    im = plt.imshow(mat, interpolation=\"nearest\", aspect=\"auto\", cmap=\"Blues\")\n",
        "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "    plt.xticks(range(len(tokens)), tokens, rotation=90)\n",
        "    plt.yticks(range(len(tokens)), tokens)\n",
        "    plt.xlabel(\"Key (attended-to) tokens →\")\n",
        "    plt.ylabel(\"Query (attending) tokens →\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def tokens_for(ids):\n",
        "    return tokenizer.convert_ids_to_tokens(ids[0].tolist())\n"
      ],
      "metadata": {
        "id": "bIRmurpAYsdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def visualize_attention_overview(prompt, max_new_tokens=40, seed=123, layers_to_show=(0, 6, 12, 18, 23)):\n",
        "    ids, text = generate_once(prompt, max_new_tokens=max_new_tokens, seed=seed)\n",
        "    ids, toks = trim_to_nonpad(ids)   # <-- trim out [PAD]s\n",
        "    attns = forward_with_attn(ids.to(device))  # [1,H,T,T]\n",
        "\n",
        "    print(\"FULL TEXT:\\n\", tokenizer.decode(ids[0], skip_special_tokens=True))\n",
        "\n",
        "    tag_idx = tag_span_indices(prompt)\n",
        "    tag_mask = np.zeros(len(toks), dtype=bool)\n",
        "    tag_mask[tag_idx] = True\n",
        "\n",
        "    # Layer-avg heatmaps\n",
        "    for L in layers_to_show:\n",
        "        A = attns[L].mean(dim=1).squeeze(0).detach().cpu().numpy()\n",
        "        plot_attn_matrix(A, toks, title=f\"Layer {L} — attention (avg over heads)\")\n",
        "\n",
        "    # Attention to/from tag profile\n",
        "    to_tag, from_tag = [], []\n",
        "    for L in range(len(attns)):\n",
        "        A = attns[L].mean(dim=1).squeeze(0).detach().cpu().numpy()\n",
        "        to_tag.append(A[:, tag_mask].mean())\n",
        "        from_tag.append(A[tag_mask, :].mean())\n",
        "\n",
        "    Ls = np.arange(len(attns))\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.plot(Ls, to_tag, marker=\"o\", label=\"to tag (←)\")\n",
        "    plt.plot(Ls, from_tag, marker=\"o\", label=\"from tag (→)\")\n",
        "    plt.xlabel(\"Layer\"); plt.ylabel(\"Mean attention weight\")\n",
        "    plt.title(\"Attention to/from TAG across layers\")\n",
        "    plt.legend(); plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout(); plt.show()\n"
      ],
      "metadata": {
        "id": "8ueu_IXXYtsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def visualize_layer_heads(prompt, layer=12, max_new_tokens=40, seed=123, head_limit=None):\n",
        "    ids, _ = generate_once(prompt, max_new_tokens=max_new_tokens, seed=seed)\n",
        "    ids, toks = trim_to_nonpad(ids)   # <-- trim out [PAD]s\n",
        "    attns = forward_with_attn(ids.to(device))\n",
        "    A = attns[layer].squeeze(0).detach().cpu().numpy()  # [H,T,T]\n",
        "    H, T, _ = A.shape\n",
        "    if head_limit is not None:\n",
        "        H = min(H, head_limit)\n",
        "        A = A[:H]\n",
        "\n",
        "    cols = min(4, H)\n",
        "    rows = int(np.ceil(H / cols))\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n",
        "    axes = np.array(axes).reshape(-1)\n",
        "\n",
        "    for h in range(H):\n",
        "        ax = axes[h]\n",
        "        im = ax.imshow(A[h], interpolation=\"nearest\", aspect=\"auto\", cmap=\"Blues\")\n",
        "        ax.set_title(f\"Layer {layer} · Head {h}\")\n",
        "        ax.set_xticks(range(T)); ax.set_xticklabels(toks, rotation=90, fontsize=8)\n",
        "        ax.set_yticks(range(T)); ax.set_yticklabels(toks, fontsize=8)\n",
        "    for k in range(H, len(axes)): axes[k].axis(\"off\")\n",
        "    fig.colorbar(im, ax=axes[:H].tolist(), fraction=0.015, pad=0.01)\n",
        "    plt.tight_layout(); plt.show()\n"
      ],
      "metadata": {
        "id": "gEkBUMqrYvpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trim_to_nonpad(input_ids):\n",
        "    \"\"\"Trim tokens and IDs at the first [PAD].\"\"\"\n",
        "    toks = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
        "    if \"[PAD]\" in toks:\n",
        "        cutoff = toks.index(\"[PAD]\")\n",
        "        toks = toks[:cutoff]\n",
        "        input_ids = input_ids[:, :cutoff]\n",
        "    return input_ids, toks"
      ],
      "metadata": {
        "id": "LV-lV5u3aOSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def plot_to_tag_for_head(prompt, layer=12, head=0, max_new_tokens=40, seed=123):\n",
        "    ids, _ = generate_once(prompt, max_new_tokens=max_new_tokens, seed=seed)\n",
        "    ids, toks = trim_to_nonpad(ids)   # <-- trim out [PAD]s\n",
        "    attn = forward_with_attn(ids.to(device))[layer][0, head]  # [T,T]\n",
        "    tag_idx = tag_span_indices(prompt)\n",
        "    tag_mask = torch.zeros(attn.size(1), dtype=torch.bool, device=attn.device)\n",
        "    tag_mask[tag_idx] = True\n",
        "    to_tag = attn[:, tag_mask].mean(dim=1).detach().cpu().numpy()\n",
        "\n",
        "    plt.figure(figsize=(max(8, 0.35*len(toks)), 3.5))\n",
        "    plt.bar(range(len(toks)), to_tag)\n",
        "    plt.xticks(range(len(toks)), toks, rotation=90)\n",
        "    plt.ylabel(\"Attention → TAG\")\n",
        "    plt.title(f\"Layer {layer}, Head {head}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "o5gKsxD_Yx84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A) Overview across selected layers + tag profile\n",
        "visualize_attention_overview(\"<1> \", max_new_tokens=40, seed=123, layers_to_show=(0, 8, 16, 23))\n",
        "\n",
        "# B) Grid of heads for one layer\n",
        "visualize_layer_heads(\"<1> \", layer=12, head_limit=6)\n",
        "\n",
        "# C) Focused: which tokens attend most to the tag in a specific head?\n",
        "plot_to_tag_for_head(\"<1> \", layer=12, head=3)\n"
      ],
      "metadata": {
        "id": "Scax8ZDlYzJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counterfactual Comparison Plots"
      ],
      "metadata": {
        "id": "8GYCOyyMY0VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(drive_tokenizer_path, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "aZI7EUzPc4Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate_once(prompt, max_new_tokens=60, temperature=0.8, top_p=0.95, seed=123):\n",
        "    torch.manual_seed(seed)\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    out = model.generate(\n",
        "        **enc,\n",
        "        do_sample=True, top_p=top_p, temperature=temperature,\n",
        "        max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id\n",
        "    )[0]\n",
        "    text = tokenizer.decode(out, skip_special_tokens=True)\n",
        "    return text\n",
        "\n",
        "def _token_strings(input_ids):\n",
        "    return tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
        "\n",
        "def _group_tokens_into_words(token_strs):\n",
        "    \"\"\"\n",
        "    Heuristic for GPT-2 byte-BPE:\n",
        "    - Start a new word when a token string begins with a space ' '.\n",
        "    - Otherwise, append to the current word.\n",
        "    Returns: words, word_indices (len=number of tokens, mapping token->word_id)\n",
        "    \"\"\"\n",
        "    words, word_indices = [], []\n",
        "    cur = \"\"\n",
        "    wid = -1\n",
        "    for s in token_strs:\n",
        "        # Decode byte-BPE artifacts for display only\n",
        "        piece = tokenizer.convert_tokens_to_string([s])\n",
        "        starts_new = piece.startswith(\" \")\n",
        "        if starts_new or wid == -1:\n",
        "            if cur:\n",
        "                words.append(cur)\n",
        "            cur = piece.lstrip()\n",
        "            wid += 1\n",
        "        else:\n",
        "            cur += piece\n",
        "        word_indices.append(wid)\n",
        "    if cur:\n",
        "        words.append(cur)\n",
        "    return words, word_indices\n",
        "\n",
        "def score_with_tag(tag, continuation_text):\n",
        "    \"\"\"\n",
        "    Score exact string: [tag + ' ' + continuation_text]\n",
        "    Aggregate token log-probs into words via byte-BPE spacing heuristic.\n",
        "    \"\"\"\n",
        "    text = f\"{tag} {continuation_text}\".strip()\n",
        "    enc = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    input_ids = enc[\"input_ids\"]    # [1, T]\n",
        "    attn = enc[\"attention_mask\"]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids=input_ids, attention_mask=attn).logits  # [1,T,V]\n",
        "        logprobs = F.log_softmax(logits[:, :-1, :], dim=-1)\n",
        "        tgt = input_ids[:, 1:]\n",
        "        tok_lp = logprobs.gather(-1, tgt.unsqueeze(-1)).squeeze(-1)[0]   # [T-1]\n",
        "\n",
        "    # Figure out token positions belonging to the continuation (exclude tag tokens)\n",
        "    tag_len = len(tokenizer(tag)[\"input_ids\"])\n",
        "    # Prediction positions corresponding to continuation targets are indices [tag_len-1 ... T-2]\n",
        "    cont_tok_lp = tok_lp[tag_len-1:]                   # [N_cont_tokens]\n",
        "    cont_token_ids = input_ids[:, tag_len:].cpu()      # tokens that were predicted\n",
        "\n",
        "    token_strs = _token_strings(cont_token_ids)\n",
        "    words, word_idx = _group_tokens_into_words(token_strs)\n",
        "\n",
        "    # Sum token log-probs into their word bins\n",
        "    contrib = np.zeros(len(words), dtype=float)\n",
        "    for k, lp in enumerate(cont_tok_lp.cpu().numpy()):\n",
        "        w = word_idx[k]\n",
        "        if 0 <= w < len(words):\n",
        "            contrib[w] += lp\n",
        "\n",
        "    total_logprob = float(cont_tok_lp.sum())\n",
        "    token_count = int(cont_tok_lp.numel())\n",
        "    return list(zip(words, contrib.tolist())), total_logprob, token_count\n",
        "\n",
        "def make_counterfactual_plots(tag_a, tag_b, suffix=\"\", max_new_tokens=60, seed=123):\n",
        "    # Generate with Tag A; keep the continuation fixed\n",
        "    gen_text = generate_once(f\"{tag_a} {suffix}\".strip(),\n",
        "                             max_new_tokens=max_new_tokens, seed=seed)\n",
        "    cont = gen_text.replace(tag_a, \"\", 1).strip()\n",
        "\n",
        "    # Score same continuation under A and B\n",
        "    words_A, total_A, nA = score_with_tag(tag_a, cont)\n",
        "    words_B, total_B, nB = score_with_tag(tag_b, cont)\n",
        "\n",
        "    words = [w for w,_ in words_A]\n",
        "    a_vals = np.array([v for _,v in words_A])\n",
        "    b_vals = np.array([v for _,v in words_B])\n",
        "    delta = a_vals - b_vals\n",
        "\n",
        "    # Plots\n",
        "    plt.figure(figsize=(min(14, 0.5*len(words)+4), 4))\n",
        "    plt.bar(range(len(words)), delta)\n",
        "    plt.xticks(range(len(words)), words, rotation=90)\n",
        "    plt.ylabel(\"Δ logP (A − B)\")\n",
        "    plt.title(f\"Word-level preference: {tag_a} vs {tag_b}\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "    plt.figure(figsize=(min(14, 0.5*len(words)+4), 3.5))\n",
        "    plt.plot(np.cumsum(delta))\n",
        "    plt.xlabel(\"Word position →\")\n",
        "    plt.ylabel(\"Cumulative Δ logP (A − B)\")\n",
        "    plt.title(\"Cumulative preference along the continuation\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "    total_delta = total_A - total_B\n",
        "    ppl_A = np.exp(-total_A / max(1, nA))\n",
        "    ppl_B = np.exp(-total_B / max(1, nB))\n",
        "    print(\"=== Counterfactual summary (same continuation) ===\")\n",
        "    print(f\"Continuation (first 200 chars): {cont[:200]!r}\")\n",
        "    print(f\"Total logP (A={tag_a}): {total_A:.2f}   | tokens: {nA}   | approx ppl: {ppl_A:.2f}\")\n",
        "    print(f\"Total logP (B={tag_b}): {total_B:.2f}   | tokens: {nB}   | approx ppl: {ppl_B:.2f}\")\n",
        "    print(f\"Δ logP (A − B): {total_delta:.2f}  →  {'A preferred' if total_delta>0 else 'B preferred'}\")\n",
        "\n",
        "    return {\n",
        "        \"continuation\": cont,\n",
        "        \"words\": words,\n",
        "        \"delta_per_word\": delta.tolist(),\n",
        "        \"total_logprob_A\": total_A,\n",
        "        \"total_logprob_B\": total_B,\n",
        "        \"total_delta\": float(total_delta),\n",
        "        \"ppl_A\": float(ppl_A),\n",
        "        \"ppl_B\": float(ppl_B),\n",
        "    }"
      ],
      "metadata": {
        "id": "izKBSWZocMWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare Dickens (<0>) vs Austen (<1>) on the same continuation\n",
        "res = make_counterfactual_plots(\"<0>\", \"<1>\", suffix=\"\", max_new_tokens=60, seed=123)\n",
        "\n",
        "# Try other pairs:\n",
        "# make_counterfactual_plots(\"<2>\", \"<4>\", suffix=\"On the river\", seed=7)"
      ],
      "metadata": {
        "id": "MY0IRMbxclFS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8dY3iUswnt1K",
        "p9Rx2Emknt1P",
        "K6onOi8Snt1Q",
        "DlKbynBgK4rk",
        "nj4DUuUjIJOp",
        "vbCHvludIJOr",
        "NIDn7bRFIJOr",
        "s9jswrgpzW8o",
        "QR7YVgvGIJOt",
        "5mLS-xhQfGqH",
        "exp8dzo73fMs",
        "Xuhw5H5X3y4m",
        "qJbp9JPH_TAq",
        "InFeNETHd6nL",
        "05HHnVaSH2w5",
        "0RrulZEBrwv8",
        "zt-TAXUMUhND"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
