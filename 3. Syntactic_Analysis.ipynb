{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egzIVY7VioI2"
      },
      "source": [
        "# Connecting and Uploading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpGmBZ0tn_fv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MV-AIENVwGJP"
      },
      "outputs": [],
      "source": [
        "cd drive/MyDrive/Colab/LLMs Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DnNZm3q41vf"
      },
      "outputs": [],
      "source": [
        "#loading:\n",
        "import pandas as pd\n",
        "\n",
        "Dataset      = pd.read_pickle(\"Dataset_generated_texts_gpt3_09.13.2025.pkl\")\n",
        "Dataset_lora = pd.read_pickle(\"Dataset_generated_texts_gpt3_lora_09.13.2025.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbsjaUtY486y"
      },
      "outputs": [],
      "source": [
        "print(Dataset.shape)\n",
        "Dataset.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nylyIamX5z49"
      },
      "outputs": [],
      "source": [
        "print(Dataset_lora.shape)\n",
        "Dataset_lora.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEPdPc5wnnEI"
      },
      "source": [
        "# Running CoreNLP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading:\n",
        "import pandas as pd\n",
        "\n",
        "generated_texts_gpt3      = pd.read_csv(\"generated_texts_gpt3.csv\")\n",
        "generated_texts_gpt3_lora = pd.read_csv(\"generated_texts_gpt3_lora.csv\")"
      ],
      "metadata": {
        "id": "Q_Ev-bba25t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning:"
      ],
      "metadata": {
        "id": "MJcQNUeTKqkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Rename \"Text\" -> \"Taged_Text\"\n",
        "generated_texts_gpt3 = generated_texts_gpt3.rename(columns={\"Text\": \"Taged_Text\"})\n",
        "\n",
        "# 2) Create \"Label\" from the leading <number>\n",
        "generated_texts_gpt3[\"Label\"] = (\n",
        "    generated_texts_gpt3[\"Taged_Text\"]\n",
        "    .str.extract(r'^\\s*<\\s*(\\d+)\\s*>')        # grab the number at start like <0>\n",
        "    .astype(\"Int64\")                           # nullable integer dtype\n",
        ")\n",
        "\n",
        "# 3) Map label -> author name\n",
        "author_map = {\n",
        "    0: \"Charles Dickens\",\n",
        "    1: \"Mark Twain\",\n",
        "    2: \"Herman Melville\",\n",
        "    3: \"Jane Austen\",\n",
        "    4: \"Louisa May Alcott\",\n",
        "}\n",
        "generated_texts_gpt3[\"Author\"] = generated_texts_gpt3[\"Label\"].map(author_map)\n",
        "\n",
        "# 4) Make a cleaned \"Text\" column:\n",
        "#    - remove the leading <number> tag\n",
        "#    - remove \"<end>\" and everything after it (case-insensitive)\n",
        "#    - trim whitespace\n",
        "generated_texts_gpt3[\"Text\"] = (\n",
        "    generated_texts_gpt3[\"Taged_Text\"]\n",
        "    .str.replace(r'^\\s*<\\s*\\d+\\s*>\\s*', '', regex=True)\n",
        "    .str.replace(r'\\s*<end>.*$', '', regex=True, case=False)\n",
        "    .str.strip()\n",
        ")"
      ],
      "metadata": {
        "id": "B7gVamTYKqlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Rename \"Text\" -> \"Taged_Text\"\n",
        "generated_texts_gpt3_lora = generated_texts_gpt3_lora.rename(columns={\"Text\": \"Taged_Text\"})\n",
        "\n",
        "# 2) Create \"Label\" from the leading <number>\n",
        "generated_texts_gpt3_lora[\"Label\"] = (\n",
        "    generated_texts_gpt3_lora[\"Taged_Text\"]\n",
        "    .str.extract(r'^\\s*<\\s*(\\d+)\\s*>')        # grab the number at start like <0>\n",
        "    .astype(\"Int64\")                           # nullable integer dtype\n",
        ")\n",
        "\n",
        "# 3) Map label -> author name\n",
        "author_map = {\n",
        "    0: \"Charles Dickens\",\n",
        "    1: \"Mark Twain\",\n",
        "    2: \"Herman Melville\",\n",
        "    3: \"Jane Austen\",\n",
        "    4: \"Louisa May Alcott\",\n",
        "}\n",
        "generated_texts_gpt3_lora[\"Author\"] = generated_texts_gpt3_lora[\"Label\"].map(author_map)\n",
        "\n",
        "# 4) Make a cleaned \"Text\" column:\n",
        "#    - remove the leading <number> tag\n",
        "#    - remove \"<end>\" and everything after it (case-insensitive)\n",
        "#    - trim whitespace\n",
        "generated_texts_gpt3_lora[\"Text\"] = (\n",
        "    generated_texts_gpt3_lora[\"Taged_Text\"]\n",
        "    .str.replace(r'^\\s*<\\s*\\d+\\s*>\\s*', '', regex=True)\n",
        "    .str.replace(r'\\s*<end>.*$', '', regex=True, case=False)\n",
        "    .str.strip()\n",
        ")"
      ],
      "metadata": {
        "id": "Nqzp7xG0M72v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3v0uNseHJF2z"
      },
      "outputs": [],
      "source": [
        "# https://colab.research.google.com/github/stanfordnlp/stanza/blob/master/demo/Stanza_CoreNLP_Interface.ipynb#scrollTo=WP4Dz6PIJHeL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLovw1vTemiX"
      },
      "outputs": [],
      "source": [
        "!pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Zv-Jj_2emlC"
      },
      "outputs": [],
      "source": [
        "import stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEr_G_hpSON5"
      },
      "outputs": [],
      "source": [
        "rm -rf corenlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxadu_JIemoH"
      },
      "outputs": [],
      "source": [
        "# Download the Stanford CoreNLP package with Stanza's installation command\n",
        "corenlp_dir = './corenlp'\n",
        "stanza.install_corenlp(dir=corenlp_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_KX8vCqemtb"
      },
      "outputs": [],
      "source": [
        "# Set the CORENLP_HOME environment variable to point to the installation location\n",
        "import os\n",
        "os.environ[\"CORENLP_HOME\"] = corenlp_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GArOFol0emwe"
      },
      "outputs": [],
      "source": [
        "!ls $CORENLP_HOME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSgn9BwIUlme"
      },
      "outputs": [],
      "source": [
        "!export CORENLP_HOME=path_to_corenlp_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipCFszPiUlpX"
      },
      "outputs": [],
      "source": [
        "# Import client module\n",
        "from stanza.server import CoreNLPClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3qnhzm5Uls0"
      },
      "outputs": [],
      "source": [
        "# Construct a CoreNLPClient with some basic annotators, a memory allocation of 4GB, and port number 9001\n",
        "client = CoreNLPClient(\n",
        "    timeout=70000,\n",
        "    #annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner'],\n",
        "    annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],\n",
        "    memory='4G',\n",
        "    endpoint='http://localhost:9001',\n",
        "    be_quiet=True)\n",
        "print(client)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Px-ta_eUlvg"
      },
      "outputs": [],
      "source": [
        "# Start the background server and wait for some time\n",
        "# Note that in practice this is totally optional, as by default the server will be started when the first annotation is performed\n",
        "client.start()\n",
        "import time; time.sleep(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z0Tux9PGsPe"
      },
      "outputs": [],
      "source": [
        "# Print background processes and look for java\n",
        "!ps -o pid,cmd | grep java"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3nbp-Yn3nSl"
      },
      "outputs": [],
      "source": [
        "# CorNLP on dataset:\n",
        "\n",
        "#generated_texts_gpt3\n",
        "\n",
        "text_sentences_gpt3 = generated_texts_gpt3['Text'].tolist()\n",
        "\n",
        "text_sentences_gpt3_pared = []\n",
        "index_of_error_gpt3 = []\n",
        "\n",
        "print(\"Total number of Samples:\", len(generated_texts_gpt3))\n",
        "\n",
        "for i in range(len(text_sentences_gpt3)):\n",
        "  try:\n",
        "    if i % 1000 == 0:\n",
        "        print(i)\n",
        "    document = client.annotate(text_sentences_gpt3[i])\n",
        "    text_sentences_gpt3_pared.append(document)\n",
        "  except:\n",
        "    index_of_error_gpt3.append(i)\n",
        "    text_sentences_gpt3_pared.append(i)\n",
        "\n",
        "text_sentences_lora_pared = []\n",
        "index_of_error_lora = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLyA2DLK5P5W"
      },
      "outputs": [],
      "source": [
        "#generated_texts_gpt3_lora\n",
        "\n",
        "text_sentences_gpt3_lora = generated_texts_gpt3_lora['Text'].tolist()\n",
        "\n",
        "text_sentences_gpt3_lora_pared = []\n",
        "index_of_error_gpt3_lora = []\n",
        "\n",
        "print(\"Total number of Samples:\", len(generated_texts_gpt3_lora))\n",
        "\n",
        "for i in range(len(text_sentences_gpt3_lora)):\n",
        "  try:\n",
        "    if i % 1000 == 0:\n",
        "        print(i)\n",
        "    document = client.annotate(text_sentences_gpt3_lora[i])\n",
        "    text_sentences_gpt3_lora_pared.append(document)\n",
        "  except:\n",
        "    text_sentences_gpt3_lora_pared.append(i)\n",
        "    index_of_error_gpt3_lora.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMvXJvUHxqAL"
      },
      "outputs": [],
      "source": [
        "# Removing errors:\n",
        "\n",
        "print(index_of_error_gpt3)\n",
        "print(index_of_error_gpt3_lora)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TsMMJmjRJmY"
      },
      "outputs": [],
      "source": [
        "# Remove from Dataset:\n",
        "generated_texts_gpt3 = generated_texts_gpt3.drop(index=index_of_error_gpt3)\n",
        "generated_texts_gpt3.reset_index(drop=True, inplace=True)\n",
        "\n",
        "generated_texts_gpt3_lora = generated_texts_gpt3_lora.drop(index=index_of_error_gpt3_lora)\n",
        "generated_texts_gpt3_lora.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riPmmAqHj5bi"
      },
      "outputs": [],
      "source": [
        "# Remove the elements with biger index first:\n",
        "\n",
        "index_of_error_gpt3.reverse()\n",
        "index_of_error_gpt3_lora.reverse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vk3qeFJYj9HT"
      },
      "outputs": [],
      "source": [
        "print(index_of_error_gpt3)\n",
        "print(index_of_error_gpt3_lora)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Tc2EWcfibDm"
      },
      "outputs": [],
      "source": [
        "for x in index_of_error_gpt3:\n",
        "  del text_sentences_gpt3_pared[x]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIlfCIYikK4j"
      },
      "outputs": [],
      "source": [
        "for x in index_of_error_gpt3_lora:\n",
        "  del text_sentences_gpt3_lora_pared[x]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeLjp_bYHwSy"
      },
      "outputs": [],
      "source": [
        "# Adding CoreNLP to Dataset:\n",
        "\n",
        "for i in range(len(generated_texts_gpt3)):\n",
        "    generated_texts_gpt3.loc[i, 'parse_tree'] = str(text_sentences_gpt3_pared[i].sentence[0].parseTree)\n",
        "    generated_texts_gpt3.loc[i, 'pos']        = str([t.pos for t in text_sentences_gpt3_pared[i].sentence[0].token])\n",
        "    generated_texts_gpt3.loc[i, 'dependency'] = text_sentences_gpt3_pared[i].sentence[0].enhancedPlusPlusDependencies.edge\n",
        "\n",
        "\n",
        "for i in range(len(generated_texts_gpt3_lora)):\n",
        "    generated_texts_gpt3_lora.loc[i, 'parse_tree'] = str(text_sentences_gpt3_lora_pared[i].sentence[0].parseTree)\n",
        "    generated_texts_gpt3_lora.loc[i, 'pos']        = str([t.pos for t in text_sentences_gpt3_lora_pared[i].sentence[0].token])\n",
        "    generated_texts_gpt3_lora.loc[i, 'dependency'] = text_sentences_gpt3_lora_pared[i].sentence[0].enhancedPlusPlusDependencies.edge"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_texts_gpt3.head(1)"
      ],
      "metadata": {
        "id": "OMsTNv3JVGu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvD1Y_kE0ffy"
      },
      "outputs": [],
      "source": [
        "generated_texts_gpt3_lora.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"pos\"           string --> list\n",
        "# \"dependency\"    string --> list of dictionary"
      ],
      "metadata": {
        "id": "yrb1i1ijcZ8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "generated_texts_gpt3[\"pos\"] = generated_texts_gpt3[\"pos\"].apply(lambda x: ast.literal_eval(x))\n",
        "generated_texts_gpt3_lora[\"pos\"] = generated_texts_gpt3_lora[\"pos\"].apply(lambda x: ast.literal_eval(x))"
      ],
      "metadata": {
        "id": "cgv85e1146YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def parse_dependency(dep_str):\n",
        "    \"\"\"\n",
        "    Convert a dependency string into a list of dictionaries.\n",
        "    \"\"\"\n",
        "    if pd.isna(dep_str) or not isinstance(dep_str, str):\n",
        "        return []\n",
        "\n",
        "    # Remove outer square brackets if present\n",
        "    dep_str = dep_str.strip().lstrip(\"[\").rstrip(\"]\")\n",
        "\n",
        "    # Split into blocks by ', source:' while keeping the first 'source:'\n",
        "    parts = re.split(r',\\s*source:', dep_str)\n",
        "    parts = [p if p.strip().startswith(\"source:\") else \"source: \" + p.strip() for p in parts]\n",
        "\n",
        "    dep_list = []\n",
        "    for block in parts:\n",
        "        entry = {}\n",
        "        # Match key: value (handles quoted strings)\n",
        "        matches = re.findall(r'(\\w+):\\s*(\"?[^\"\\n]+\"?|\\w+)', block)\n",
        "        for k, v in matches:\n",
        "            v = v.strip('\"')\n",
        "            # Convert booleans and ints\n",
        "            if v.lower() == \"true\":\n",
        "                v = True\n",
        "            elif v.lower() == \"false\":\n",
        "                v = False\n",
        "            else:\n",
        "                try:\n",
        "                    v = int(v)\n",
        "                except ValueError:\n",
        "                    pass\n",
        "            entry[k] = v\n",
        "        if entry:\n",
        "            dep_list.append(entry)\n",
        "\n",
        "    return dep_list\n"
      ],
      "metadata": {
        "id": "UMY7PwzKCCmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_texts_gpt3[\"dependency\"] = generated_texts_gpt3[\"dependency\"].astype(str)   # or convert to dict/JSON if you have a better serializer\n",
        "generated_texts_gpt3_lora[\"dependency\"] = generated_texts_gpt3_lora[\"dependency\"].astype(str)   # or convert to dict/JSON if you have a better serializer"
      ],
      "metadata": {
        "id": "m3wAJ3rrUtsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_texts_gpt3[\"dependency\"] = generated_texts_gpt3[\"dependency\"].apply(lambda x: parse_dependency(x))\n",
        "generated_texts_gpt3_lora[\"dependency\"] = generated_texts_gpt3_lora[\"dependency\"].apply(lambda x: parse_dependency(x))"
      ],
      "metadata": {
        "id": "78kuTdJEDNX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving"
      ],
      "metadata": {
        "id": "1qd2Ji2YcqM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_texts_gpt3.to_pickle(\"generated_texts_gpt3_09.13.2025.pkl\")\n",
        "generated_texts_gpt3_lora.to_pickle(\"generated_texts_gpt3_lora_09.13.2025.pkl\")"
      ],
      "metadata": {
        "id": "KYXVdtBxU43o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Saved (^_^)\")"
      ],
      "metadata": {
        "id": "RVzNL5bYVRU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shut down the background CoreNLP server\n",
        "client.stop()\n",
        "\n",
        "time.sleep(10)\n",
        "!ps -o pid,cmd | grep java"
      ],
      "metadata": {
        "id": "xvZMFZFH6NRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oBgDGOwM9hS"
      },
      "source": [
        "# Features (Paper 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W142EXrlm2rP"
      },
      "outputs": [],
      "source": [
        "Dataset = generated_texts_gpt3.copy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_lora = generated_texts_gpt3_lora.copy()"
      ],
      "metadata": {
        "id": "305AXMyoueKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_len = []\n",
        "sentences_words = []\n",
        "\n",
        "for sentence in Dataset[\"Text\"]:\n",
        "        sentences_len.append(len(sentence))\n",
        "        sentences_words.append(len(sentence.split()))\n",
        "\n",
        "# Add as new columns to the existing DataFrame\n",
        "Dataset[\"Length\"] = sentences_len\n",
        "Dataset[\"Words\"] = sentences_words"
      ],
      "metadata": {
        "id": "zvA7OjUVuiDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_len = []\n",
        "sentences_words = []\n",
        "\n",
        "for sentence in Dataset_lora[\"Text\"]:\n",
        "        sentences_len.append(len(sentence))\n",
        "        sentences_words.append(len(sentence.split()))\n",
        "\n",
        "# Add as new columns to the existing DataFrame\n",
        "Dataset_lora[\"Length\"] = sentences_len\n",
        "Dataset_lora[\"Words\"] = sentences_words"
      ],
      "metadata": {
        "id": "qKkow2o8sHVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W91B6j-Qm8TC"
      },
      "outputs": [],
      "source": [
        "Dataset.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_lora.head(1)"
      ],
      "metadata": {
        "id": "-QGWaaEdu6bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YueBEFLLwb_4"
      },
      "source": [
        "## Part of Speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOSzd8fXWWTq"
      },
      "outputs": [],
      "source": [
        "for i in range(len(Dataset)):\n",
        "\n",
        "    tokens = Dataset.loc[i, \"pos\"]\n",
        "\n",
        "    Dataset.at[i, 'verb_count']   = sum(1 for t in tokens if t.startswith('V'))\n",
        "    Dataset.at[i, 'adverb_count'] = sum(1 for t in tokens if t.startswith('RB'))\n",
        "    Dataset.at[i, 'noun_count']   = sum(1 for t in tokens if t.startswith('N'))\n",
        "    Dataset.at[i, 'coord_count']  = sum(1 for t in tokens if t.startswith('CC'))\n",
        "    Dataset.at[i, 'subord_count'] = sum(1 for t in tokens if t.startswith('IN'))\n",
        "\n",
        "Dataset['verb_percent']        =  round(Dataset['verb_count']/Dataset['Words'], 2)\n",
        "Dataset['adverb_percent']      =  round(Dataset['adverb_count']/Dataset['Words'], 2)\n",
        "Dataset['noun_percent']        =  round(Dataset['noun_count']/Dataset['Words'], 2)\n",
        "Dataset['coord_percent']  =  round(Dataset['coord_count']/Dataset['Words'], 2)\n",
        "Dataset['subord_percent'] =  round(Dataset['subord_count']/Dataset['Words'], 2)\n",
        "\n",
        "del i, tokens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(Dataset_lora)):\n",
        "\n",
        "    tokens = Dataset_lora.loc[i, \"pos\"]\n",
        "\n",
        "    Dataset_lora.at[i, 'verb_count']   = sum(1 for t in tokens if t.startswith('V'))\n",
        "    Dataset_lora.at[i, 'adverb_count'] = sum(1 for t in tokens if t.startswith('RB'))\n",
        "    Dataset_lora.at[i, 'noun_count']   = sum(1 for t in tokens if t.startswith('N'))\n",
        "    Dataset_lora.at[i, 'coord_count']  = sum(1 for t in tokens if t.startswith('CC'))\n",
        "    Dataset_lora.at[i, 'subord_count'] = sum(1 for t in tokens if t.startswith('IN'))\n",
        "\n",
        "Dataset_lora['verb_percent']        =  round(Dataset_lora['verb_count']/Dataset_lora['Words'], 2)\n",
        "Dataset_lora['adverb_percent']      =  round(Dataset_lora['adverb_count']/Dataset_lora['Words'], 2)\n",
        "Dataset_lora['noun_percent']        =  round(Dataset_lora['noun_count']/Dataset_lora['Words'], 2)\n",
        "Dataset_lora['coord_percent']  =  round(Dataset_lora['coord_count']/Dataset_lora['Words'], 2)\n",
        "Dataset_lora['subord_percent'] =  round(Dataset_lora['subord_count']/Dataset_lora['Words'], 2)\n",
        "\n",
        "del i, tokens"
      ],
      "metadata": {
        "id": "QS6XrcKLvAv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xImt1gM-CzWt"
      },
      "source": [
        "## Path and Depth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2KhBx9n6tj8"
      },
      "outputs": [],
      "source": [
        "# Calculate the longest path between two leaves in the tree.\n",
        "\n",
        "def longest_path(node):\n",
        "\n",
        "    if not node.get(\"children\"):\n",
        "        # Leaf node, no children\n",
        "        return 0, 0\n",
        "\n",
        "    max_depth = 0\n",
        "    second_max_depth = 0\n",
        "    max_path = 0\n",
        "\n",
        "    for child in node[\"children\"]:\n",
        "        path, depth = longest_path(child)\n",
        "        if depth > max_depth:\n",
        "            second_max_depth = max_depth\n",
        "            max_depth = depth\n",
        "        elif depth > second_max_depth:\n",
        "            second_max_depth = depth\n",
        "\n",
        "        max_path = max(max_path, path)\n",
        "\n",
        "    # Longest path through this node\n",
        "    longest_through_root = max_depth + second_max_depth + 2\n",
        "\n",
        "    return max(max_path, longest_through_root), max_depth + 1\n",
        "\n",
        "\n",
        "def tree_depth(node):\n",
        "    \"\"\"\n",
        "    Calculate the depth of the tree.\n",
        "    \"\"\"\n",
        "    if not node.get(\"children\"):\n",
        "        # Leaf node, no children\n",
        "        return 1\n",
        "\n",
        "    return 1 + max(tree_depth(child) for child in node[\"children\"])\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "def build_tree(data):\n",
        "    \"\"\"\n",
        "    Build a tree from the string representation.\n",
        "    \"\"\"\n",
        "    # Split the data into lines\n",
        "    lines = data.split('\\n')\n",
        "\n",
        "    # Create a root node\n",
        "    tree = {\"children\": []}\n",
        "    current_node = tree\n",
        "    stack = []\n",
        "\n",
        "    # Regex pattern to identify node attributes\n",
        "    pattern = re.compile(r'(value|score): \"?(.*?)\"?\\s*$')\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.startswith(\"child {\"):\n",
        "            # Start a new child node\n",
        "            new_node = {\"children\": []}\n",
        "            if current_node is not None:\n",
        "                current_node[\"children\"].append(new_node)\n",
        "            stack.append(current_node)\n",
        "            current_node = new_node\n",
        "        elif line == \"}\":\n",
        "            # End of the current node, pop from stack\n",
        "            current_node = stack.pop()\n",
        "        else:\n",
        "            # Extract attributes\n",
        "            match = pattern.search(line)\n",
        "            if match:\n",
        "                current_node[match.group(1)] = match.group(2)\n",
        "\n",
        "    return tree[\"children\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xzlYip21ya-"
      },
      "outputs": [],
      "source": [
        "def longest_path_and_depth(node):\n",
        "    data = str(node)\n",
        "    tree = build_tree(data)\n",
        "    longest_path_length, _ = longest_path(tree)\n",
        "    depth = tree_depth(tree)\n",
        "\n",
        "    return longest_path_length, depth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeZkb-tR0Gcu"
      },
      "outputs": [],
      "source": [
        "Dataset['longest_path_parstree'], Dataset['depth_parstree'] = zip(*Dataset['parse_tree'].apply(longest_path_and_depth))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_lora['longest_path_parstree'], Dataset_lora['depth_parstree'] = zip(*Dataset_lora['parse_tree'].apply(longest_path_and_depth))"
      ],
      "metadata": {
        "id": "ChKY6jtlvSi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b49V9Fwds2SB"
      },
      "source": [
        "## Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zm9T_rI15uIX"
      },
      "outputs": [],
      "source": [
        "def questions (node):\n",
        "    S_question    = 1 if \"value: \\\"SQ\\\"\"    in str(node) else 0\n",
        "    S_question_wh = 1 if \"value: \\\"SBARQ\\\"\" in str(node) else 0\n",
        "    return S_question, S_question_wh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0nVhOnTPE7r"
      },
      "outputs": [],
      "source": [
        "Dataset['questions'], Dataset['questions_wh'] = zip(*Dataset['parse_tree'].apply(questions))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_lora['questions'], Dataset_lora['questions_wh'] = zip(*Dataset_lora['parse_tree'].apply(questions))"
      ],
      "metadata": {
        "id": "1fc5ThqsvmfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6VjtCveGkeS"
      },
      "source": [
        "## Pronouns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4toed0BwPVQs"
      },
      "outputs": [],
      "source": [
        "def pronouns  (node):\n",
        "    PRP_first  = node.lower().count(\"value: \\\"i\\\"\") + node.lower().count(\"value: \\\"we\\\"\")\n",
        "    PRP_second = node.lower().count(\"value: \\\"you\\\"\")\n",
        "    PRP_third  = node.lower().count(\"value: \\\"he\\\"\") + node.lower().count(\"value: \\\"she\\\"\") + node.lower().count(\"value: \\\"it\\\"\") + node.lower().count(\"value: \\\"they\\\"\")\n",
        "\n",
        "    return PRP_first, PRP_second, PRP_third"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWdcsi5SZSiw"
      },
      "outputs": [],
      "source": [
        "Dataset['PRP_first'], Dataset['PRP_second'], Dataset['PRP_third'] = zip(*Dataset['parse_tree'].apply(pronouns))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_lora['PRP_first'], Dataset_lora['PRP_second'], Dataset_lora['PRP_third'] = zip(*Dataset_lora['parse_tree'].apply(pronouns))"
      ],
      "metadata": {
        "id": "h7yLYwW_vsAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcN3yxG4Zv36"
      },
      "outputs": [],
      "source": [
        "Dataset['PRP_first_percent']   =  round(Dataset['PRP_first']/Dataset['Words'], 2)\n",
        "Dataset['PRP_second_percent']  =  round(Dataset['PRP_second']/Dataset['Words'], 2)\n",
        "Dataset['PRP_third_percent']   =  round(Dataset['PRP_third']/Dataset['Words'], 2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_lora['PRP_first_percent']   =  round(Dataset_lora['PRP_first']/Dataset_lora['Words'], 2)\n",
        "Dataset_lora['PRP_second_percent']  =  round(Dataset_lora['PRP_second']/Dataset_lora['Words'], 2)\n",
        "Dataset_lora['PRP_third_percent']   =  round(Dataset_lora['PRP_third']/Dataset_lora['Words'], 2)"
      ],
      "metadata": {
        "id": "VqBHqF5QvyIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROUvAI1wRhXe"
      },
      "source": [
        "## phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4gDARlCcP_h"
      },
      "outputs": [],
      "source": [
        "def phrases (node):\n",
        "    S_noun       = node.count(\"value: \\\"NP\\\"\")\n",
        "    S_quantifier = node.count(\"value: \\\"QP\\\"\")\n",
        "    return S_noun, S_quantifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yxxihwWqxCD"
      },
      "outputs": [],
      "source": [
        "Dataset['noun_phrase'], Dataset['quantifier_phrase'] = zip(*Dataset['parse_tree'].apply(phrases))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_lora['noun_phrase'], Dataset_lora['quantifier_phrase'] = zip(*Dataset_lora['parse_tree'].apply(phrases))"
      ],
      "metadata": {
        "id": "URz6yzCyv8Di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFSlCfCCjYCv"
      },
      "outputs": [],
      "source": [
        "# Droping quantifier_phrase\n",
        "Dataset.drop(columns=['quantifier_phrase'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Droping quantifier_phrase\n",
        "Dataset_lora.drop(columns=['quantifier_phrase'], inplace=True)"
      ],
      "metadata": {
        "id": "b1KNQUFWwE4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxIyAfWdLIjk"
      },
      "source": [
        "# Features (Paper 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWBi9C-n1Tow"
      },
      "source": [
        "#### Number of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2CkoFr61TMx"
      },
      "outputs": [],
      "source": [
        "Dataset['Words'] = Dataset['pos'].apply(lambda x: len(x))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_lora['Words'] = Dataset_lora['pos'].apply(lambda x: len(x))"
      ],
      "metadata": {
        "id": "0FoEN0LIwJ0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uhEh2wtnA47"
      },
      "source": [
        "#### 1. Passive Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nHZE_9sTOzr"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the language model\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n-T-ky0nK7W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def is_passive(sentence):\n",
        "    # Parse the sentence using SpaCy\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # Check for passive voice\n",
        "    for token in doc:\n",
        "        if token.dep_ == \"auxpass\":\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def is_agentless_passive(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    has_auxpass = False\n",
        "    has_agent = False\n",
        "\n",
        "    for token in doc:\n",
        "        if token.dep_ == \"auxpass\":\n",
        "            has_auxpass = True\n",
        "        if token.dep_ == \"agent\":\n",
        "            has_agent = True\n",
        "\n",
        "    # Check for passive voice and absence of an agent\n",
        "    return has_auxpass and not has_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwolFz1QnK-j"
      },
      "outputs": [],
      "source": [
        "def passive_detector(sentence):\n",
        "    if is_agentless_passive(sentence):\n",
        "        return \"Agentless Passive\"\n",
        "    elif is_passive(sentence):\n",
        "        return \"Passive\"\n",
        "    else:\n",
        "        return \"Active\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "289DnQqtnLBQ"
      },
      "outputs": [],
      "source": [
        "# Apply the function on the 'Sentence' column\n",
        "Dataset.loc[:, 'Passive'] = Dataset['Text'].apply(passive_detector)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function on the 'Sentence' column\n",
        "Dataset_lora.loc[:, 'Passive'] = Dataset_lora['Text'].apply(passive_detector)"
      ],
      "metadata": {
        "id": "tiXld9MNwX15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.groupby('Label')['Passive'].value_counts()"
      ],
      "metadata": {
        "id": "jwdkXr--2yaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_lora.groupby('Label')['Passive'].value_counts()"
      ],
      "metadata": {
        "id": "AOVS3QoTyZKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfwkDt_pH9Kd"
      },
      "source": [
        "#### 2. Comparative & Superlative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8D7Ss5aCH9Ke"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def parse_tree_structure(tree_string):\n",
        "    # Function to convert the custom tree structure to a dictionary\n",
        "    def parse_node(text):\n",
        "        stack = []\n",
        "        current_node = {}\n",
        "        current_key = None\n",
        "        current_value = None\n",
        "\n",
        "        for token in re.finditer(r'(\\w+)|[{}]', text):\n",
        "            if token.group() == '{':\n",
        "                stack.append((current_node, current_key))\n",
        "                current_node = {}\n",
        "                current_key = None\n",
        "            elif token.group() == '}':\n",
        "                if current_key:\n",
        "                    current_node[current_key] = current_value\n",
        "                if stack:\n",
        "                    parent_node, parent_key = stack.pop()\n",
        "                    if parent_key not in parent_node:\n",
        "                        parent_node[parent_key] = []\n",
        "                    parent_node[parent_key].append(current_node)\n",
        "                    current_node = parent_node\n",
        "                    current_key = None\n",
        "            else:\n",
        "                if current_key is None:\n",
        "                    current_key = token.group()\n",
        "                else:\n",
        "                    current_value = token.group()\n",
        "                    current_node[current_key] = current_value\n",
        "                    current_key = None\n",
        "                    current_value = None\n",
        "\n",
        "        return current_node\n",
        "\n",
        "    return parse_node(tree_string)\n",
        "\n",
        "\n",
        "\n",
        "def Comparative_Superlative(tree_string):\n",
        "\n",
        "    # Parse the string representation of the tree into a dictionary\n",
        "    tree_dict = parse_tree_structure(tree_string)\n",
        "\n",
        "    stack = [tree_dict]\n",
        "\n",
        "    while stack:\n",
        "        current = stack.pop()\n",
        "\n",
        "        if isinstance(current, dict):\n",
        "            for key, value in current.items():\n",
        "                if key == 'value' and (value == 'JJR' or value == 'RBR'):\n",
        "                    return \"Comparative\"\n",
        "                if key == 'value' and (value == 'JJS' or value == 'RBS'):\n",
        "                    return \"Superlative\"\n",
        "                if key == 'child':\n",
        "                    stack.extend(value)\n",
        "\n",
        "    return \"Not\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnsRsmqZH9Ke"
      },
      "outputs": [],
      "source": [
        "# Apply the function on the 'Sentence' column\n",
        "Dataset.loc[:, 'Compare_Super'] = Dataset['parse_tree'].apply(Comparative_Superlative)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function on the 'Sentence' column\n",
        "Dataset_lora.loc[:, 'Compare_Super'] = Dataset_lora['parse_tree'].apply(Comparative_Superlative)"
      ],
      "metadata": {
        "id": "xZSf1BwM22dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuVnWV9FGe9f"
      },
      "outputs": [],
      "source": [
        "Dataset.groupby('Label')['Compare_Super'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_lora.groupby('Label')['Compare_Super'].value_counts()"
      ],
      "metadata": {
        "id": "A5mf78xh3ASj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IA7bDN4z8uD"
      },
      "source": [
        "#### 3. Search for CONJP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLdX1nCDz8G_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Function to check if 'CONJP' exists in the 'parsstree' JSON\n",
        "def contains_conjp(json_str):\n",
        "    try:\n",
        "        # Check if 'CONJP' is present in the values of the JSON structure\n",
        "        return 'CONJP' in json_str\n",
        "    except json.JSONDecodeError:\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rsb78O9dHF61"
      },
      "outputs": [],
      "source": [
        "# Apply the function to the 'parsstree' column\n",
        "Dataset.loc[:, 'CONJP'] = Dataset['parse_tree'].apply(contains_conjp)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_lora.loc[:, 'CONJP'] = Dataset_lora['parse_tree'].apply(contains_conjp)"
      ],
      "metadata": {
        "id": "G5In4wXE9eKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.groupby('Label')['CONJP'].value_counts()"
      ],
      "metadata": {
        "id": "sD0K3YJA_hWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_lora.groupby('Label')['CONJP'].value_counts()"
      ],
      "metadata": {
        "id": "KR4rj4cF_hZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIzeUJXaJsbr"
      },
      "source": [
        "#### 4. Imperative Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRBQZgQPLjay"
      },
      "outputs": [],
      "source": [
        "#!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNWFIS-BLja8"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3M81UOx8Jsbx"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import ast\n",
        "\n",
        "def imperative_detector(pos):\n",
        "    if pos[0] == \"VB\" and pos[-1] == \".\":\n",
        "        return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PA_ZhdRHfXOV"
      },
      "outputs": [],
      "source": [
        "# Apply the function to the 'parsstree' column\n",
        "Dataset.loc[:, 'Imperative'] = Dataset['pos'].apply(imperative_detector)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_lora.loc[:, 'Imperative'] = Dataset_lora['pos'].apply(imperative_detector)"
      ],
      "metadata": {
        "id": "4rn545J-_yjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9LJZDL1Jsbz"
      },
      "outputs": [],
      "source": [
        "Dataset['Imperative'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_lora['Imperative'].value_counts()"
      ],
      "metadata": {
        "id": "4feipuFj_2qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk6PDv7iINYP"
      },
      "source": [
        "#### 5. Nominal Subject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plLAS_1iJb2B"
      },
      "outputs": [],
      "source": [
        "'''import spacy\n",
        "\n",
        "def Nsubj(sentence):\n",
        "\n",
        "    doc = client.annotate(sentence)\n",
        "    edges = doc.sentence[0].enhancedPlusPlusDependencies.edge\n",
        "\n",
        "\n",
        "    # 0 -> There is not a nsubj\n",
        "    # 1 -> nsubj is pronoun\n",
        "    # 2 -> nsubj is noun\n",
        "\n",
        "    for edge in edges:\n",
        "        if edge.dep == \"nsubj\":\n",
        "            index = edge.target - 1\n",
        "            if doc.sentence[0].token[index].pos == \"PRP\":\n",
        "                return 1 #pronoun\n",
        "            else:\n",
        "                return 2 #noun\n",
        "    return 0 #not'''\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unSVTvA-dkoR"
      },
      "outputs": [],
      "source": [
        "'''# Apply the function to the 'sentence' column\n",
        "Dataset.loc[:, 'Nsubj'] = Dataset['Sentence'].apply(Nsubj)'''\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6zSvTn7ePYE"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "def Nsubj(deps_list, pos):\n",
        "    print(len(deps_list))\n",
        "\n",
        "    # 0 -> There is not a nsubj\n",
        "    # 1 -> nsubj is pronoun\n",
        "    # 2 -> nsubj is noun\n",
        "\n",
        "    if len(pos) == 1:\n",
        "        return 0\n",
        "\n",
        "    for dep_info in deps_list:\n",
        "        if dep_info[\"dep\"] == \"nsubj\":\n",
        "            index = dep_info[\"target\"] - 1\n",
        "            if pos[index] == \"PRP\":\n",
        "                return 1 #pronoun\n",
        "            else:\n",
        "                return 2 #noun\n",
        "    return 0 #not"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset[\"Nsubj\"] = Dataset.apply(lambda row: Nsubj(row[\"dependency\"], row[\"pos\"]), axis=1)\n",
        "Dataset_lora[\"Nsubj\"] = Dataset_lora.apply(lambda row: Nsubj(row[\"dependency\"], row[\"pos\"]), axis=1)"
      ],
      "metadata": {
        "id": "hsVnohn2Edp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1nNqDc4kU7s"
      },
      "outputs": [],
      "source": [
        "Dataset['Nsubj'] = Dataset['Nsubj'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_lora['Nsubj'] = Dataset_lora['Nsubj'].astype(int)"
      ],
      "metadata": {
        "id": "DIXYC9E-APsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.groupby('Label')['Nsubj'].value_counts()"
      ],
      "metadata": {
        "id": "69hrAJ1pATyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_lora.groupby('Label')['Nsubj'].value_counts()"
      ],
      "metadata": {
        "id": "KGyXyI7xAT09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmTi4SdKJEqH"
      },
      "source": [
        "#### Prepositional Phrase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fAO91x2NvKq"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Function to convert the custom tree structure to a dictionary\n",
        "\n",
        "def parse_tree_structure(tree_string):\n",
        "    stack = []\n",
        "    current_node = {}\n",
        "    current_key = None\n",
        "    current_value = None\n",
        "\n",
        "    for token in re.finditer(r'(\\w+)|[{}]', tree_string):\n",
        "        if token.group() == '{':\n",
        "            stack.append((current_node, current_key))\n",
        "            current_node = {}\n",
        "            current_key = None\n",
        "        elif token.group() == '}':\n",
        "            if current_key:\n",
        "                current_node[current_key] = current_value\n",
        "            if stack:\n",
        "                parent_node, parent_key = stack.pop()\n",
        "                if parent_key not in parent_node:\n",
        "                    parent_node[parent_key] = []\n",
        "                parent_node[parent_key].append(current_node)\n",
        "                current_node = parent_node\n",
        "                current_key = None\n",
        "        else:\n",
        "            if current_key is None:\n",
        "                current_key = token.group()\n",
        "            else:\n",
        "                current_value = token.group()\n",
        "                current_node[current_key] = current_value\n",
        "                current_key = None\n",
        "                current_value = None\n",
        "    return current_node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADORPaUoQQc8"
      },
      "outputs": [],
      "source": [
        "def find_leaves_with_pp_ancestor(tree, parent_has_pp=False):\n",
        "    leaves = []\n",
        "\n",
        "    # Check if the current node has 'value': 'PP'\n",
        "    if tree.get('value') == 'PP':\n",
        "        parent_has_pp = True\n",
        "\n",
        "    # If this node has children, recurse\n",
        "    if 'child' in tree and isinstance(tree['child'], list):\n",
        "        for child in tree['child']:\n",
        "            leaves.extend(find_leaves_with_pp_ancestor(child, parent_has_pp))\n",
        "    else:\n",
        "        # If this is a leaf node and its parent or ancestor had 'value': 'PP'\n",
        "        if (parent_has_pp and tree.get('value') is not None) and (parent_has_pp and tree.get('value') != '342'):\n",
        "            leaves.append(tree.get('value'))\n",
        "\n",
        "    return leaves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTLB_oEDbAjh"
      },
      "outputs": [],
      "source": [
        "def percentage_of_PP(tree_string,words):\n",
        "    dic_tree = parse_tree_structure(tree_string)\n",
        "    list_of_pp = find_leaves_with_pp_ancestor(dic_tree)\n",
        "\n",
        "    return round(len(list_of_pp)/words, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zS63kQ32cny5"
      },
      "outputs": [],
      "source": [
        "# Runing on the all of Samples:\n",
        "\n",
        "Dataset['PP_Percent'] = 0.0\n",
        "\n",
        "for i in range(len(Dataset)):\n",
        "    percent = percentage_of_PP(Dataset.loc[i]['parse_tree'], Dataset.loc[i]['Words'])\n",
        "    Dataset.loc[i, 'PP_Percent'] = percent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Runing on the all of Samples:\n",
        "\n",
        "Dataset_lora['PP_Percent'] = 0.0\n",
        "\n",
        "for i in range(len(Dataset_lora)):\n",
        "    percent = percentage_of_PP(Dataset_lora.loc[i]['parse_tree'], Dataset_lora.loc[i]['Words'])\n",
        "    Dataset_lora.loc[i, 'PP_Percent'] = percent"
      ],
      "metadata": {
        "id": "jQIzENLBAdXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "735-tp8IVbty"
      },
      "outputs": [],
      "source": [
        "# Checking the reults:\n",
        "smaler_than_0 = Dataset[Dataset['PP_Percent'] < 0].index\n",
        "bigger_than_1 = Dataset[Dataset['PP_Percent'] > 1].index\n",
        "\n",
        "print(len(smaler_than_0), len(bigger_than_1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the reults:\n",
        "smaler_than_0 = Dataset_lora[Dataset_lora['PP_Percent'] < 0].index\n",
        "bigger_than_1 = Dataset_lora[Dataset_lora['PP_Percent'] > 1].index\n",
        "\n",
        "print(len(smaler_than_0), len(bigger_than_1))"
      ],
      "metadata": {
        "id": "F9xyw7pCCNi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Kjn_kii_lbP"
      },
      "outputs": [],
      "source": [
        "# Function to find words based on the type of parent node\n",
        "def find_leaves_with_pp_parent_type(tree, parent_tag=None, parent_of_pp=None):\n",
        "    leaves_vp = []\n",
        "    leaves_np = []\n",
        "    leaves_other = []\n",
        "\n",
        "    # Check if the current node has 'value': 'PP'\n",
        "    if tree.get('value') == 'PP':\n",
        "        parent_of_pp = parent_tag\n",
        "\n",
        "    # If this node has children, recurse\n",
        "    if 'child' in tree and isinstance(tree['child'], list):\n",
        "        for child in tree['child']:\n",
        "            vp, np, other = find_leaves_with_pp_parent_type(child, tree.get('value'), parent_of_pp)\n",
        "            leaves_vp.extend(vp)\n",
        "            leaves_np.extend(np)\n",
        "            leaves_other.extend(other)\n",
        "    else:\n",
        "        # If this is a leaf node and its parent or ancestor had 'value': 'PP'\n",
        "        if parent_of_pp == 'VP' and tree.get('value') is not None and tree.get('value') != '342':\n",
        "            leaves_vp.append(tree.get('value'))\n",
        "        elif parent_of_pp == 'NP' and tree.get('value') is not None and tree.get('value') != '342':\n",
        "            leaves_np.append(tree.get('value'))\n",
        "        elif parent_of_pp is not None and tree.get('value') is not None and tree.get('value') != '342':\n",
        "            leaves_other.append(tree.get('value'))\n",
        "\n",
        "    return leaves_vp, leaves_np, leaves_other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZcTG6X7_lqH"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "sample = 0\n",
        "\n",
        "leaves_vp, leaves_np, leaves_other = find_leaves_with_pp_parent_type(parse_tree_structure(Dataset.loc[sample][\"parse_tree\"]))\n",
        "\n",
        "print(\"Leaves with PP parent under VP:\", leaves_vp)\n",
        "print(\"Leaves with PP parent under NP:\", leaves_np)\n",
        "print(\"Leaves with PP parent under other:\", leaves_other)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgIZR5Sy_lxl"
      },
      "outputs": [],
      "source": [
        "def percentage_of_PP_for_VP_NP_Other(tree_string,words):\n",
        "    dic_tree = parse_tree_structure(tree_string)\n",
        "    leaves_vp, leaves_np, leaves_other = find_leaves_with_pp_parent_type(dic_tree)\n",
        "\n",
        "    return round(len(leaves_vp)/words, 2), round(len(leaves_np)/words, 2), round(len(leaves_other)/words, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-H4zxs3Djolr"
      },
      "outputs": [],
      "source": [
        "percentage_of_PP_for_VP_NP_Other(Dataset.loc[sample][\"parse_tree\"],Dataset.loc[sample][\"Words\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "percentage_of_PP_for_VP_NP_Other(Dataset_lora.loc[sample][\"parse_tree\"],Dataset_lora.loc[sample][\"Words\"])"
      ],
      "metadata": {
        "id": "E_7m_6NuCc8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHv6iSMPiKzm"
      },
      "outputs": [],
      "source": [
        "# Runing on the all of Samples:\n",
        "\n",
        "Dataset['PP_VP_Percent'] = 0.0\n",
        "Dataset['PP_NP_Percent'] = 0.0\n",
        "Dataset['PP_O_Percent'] = 0.0\n",
        "\n",
        "for i in range(len(Dataset)):\n",
        "    pp_vp_percent, pp_np_percent, pp_other_percent = percentage_of_PP_for_VP_NP_Other(Dataset.loc[i]['parse_tree'], Dataset.loc[i]['Words'])\n",
        "    Dataset.loc[i, 'PP_VP_Percent'] = pp_vp_percent\n",
        "    Dataset.loc[i, 'PP_NP_Percent'] = pp_np_percent\n",
        "    Dataset.loc[i, 'PP_O_Percent']  = pp_other_percent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Runing on the all of Samples:\n",
        "\n",
        "Dataset_lora['PP_VP_Percent'] = 0.0\n",
        "Dataset_lora['PP_NP_Percent'] = 0.0\n",
        "Dataset_lora['PP_O_Percent'] = 0.0\n",
        "\n",
        "for i in range(len(Dataset_lora)):\n",
        "    pp_vp_percent, pp_np_percent, pp_other_percent = percentage_of_PP_for_VP_NP_Other(Dataset_lora.loc[i]['parse_tree'], Dataset_lora.loc[i]['Words'])\n",
        "    Dataset_lora.loc[i, 'PP_VP_Percent'] = pp_vp_percent\n",
        "    Dataset_lora.loc[i, 'PP_NP_Percent'] = pp_np_percent\n",
        "    Dataset_lora.loc[i, 'PP_O_Percent']  = pp_other_percent"
      ],
      "metadata": {
        "id": "IqJq1HpeChlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.loc[0,['PP_Percent', 'PP_VP_Percent', 'PP_NP_Percent', 'PP_O_Percent']]"
      ],
      "metadata": {
        "id": "IOT6IRcuCnpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_lora.loc[0,['PP_Percent', 'PP_VP_Percent', 'PP_NP_Percent', 'PP_O_Percent']]"
      ],
      "metadata": {
        "id": "Jms7FLoFCnxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Make \"Dependency\" saveable (cahnge to string)\n",
        "\n",
        "import pickle\n",
        "\n",
        "bad_cols = []\n",
        "for col in generated_texts_gpt3.columns:\n",
        "    try:\n",
        "        # test serializability of the whole column\n",
        "        pickle.dumps(generated_texts_gpt3[col].tolist())\n",
        "    except Exception as e:\n",
        "        bad_cols.append(col)\n",
        "bad_cols\"\"\"\n",
        "\n",
        "pass"
      ],
      "metadata": {
        "id": "3FncdGogEMLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving"
      ],
      "metadata": {
        "id": "W0Q39OUYFIkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.to_pickle(\"Dataset_generated_texts_gpt3_09.13.2025.pkl\")\n",
        "Dataset_lora.to_pickle(\"Dataset_generated_texts_gpt3_lora_09.13.2025.pkl\")"
      ],
      "metadata": {
        "id": "v9HgHmNZEmAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HONC7JIU1t_t"
      },
      "source": [
        "# Analyzing Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpLKuxJaCWXz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset.columns"
      ],
      "metadata": {
        "id": "pHg90rlwDQzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset_lora.columns"
      ],
      "metadata": {
        "id": "oA6xtW5YDgJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm6CSfsvwRRf"
      },
      "outputs": [],
      "source": [
        "columns_num =  ['Words',\n",
        "                'depth_parstree',\n",
        "                #'longest_word_length',\n",
        "                'longest_path_parstree',\n",
        "                'verb_percent',\n",
        "                'adverb_percent',\n",
        "                'noun_percent',\n",
        "                'coord_percent',\n",
        "                'subord_percent',\n",
        "                'PRP_first_percent',\n",
        "                'PRP_second_percent',\n",
        "                'PRP_third_percent',\n",
        "                'noun_phrase',\n",
        "                'PP_Percent',\n",
        "                'PP_VP_Percent',\n",
        "                'PP_NP_Percent',\n",
        "                'PP_O_Percent']\n",
        "\n",
        "columns_cat =  ['questions',\n",
        "                'questions_wh',\n",
        "                'Passive',\n",
        "                'Compare_Super',\n",
        "                'CONJP',\n",
        "                'Imperative',\n",
        "                'Nsubj']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4th8QBO1-JX"
      },
      "outputs": [],
      "source": [
        "print(\"Numerical Data: \\n\")\n",
        "\n",
        "for col in columns_num:\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    sns.kdeplot(data=Dataset,\n",
        "                x=col,\n",
        "                hue='Author',\n",
        "                fill=True,\n",
        "                hue_order=[\n",
        "                    \"Charles Dickens\",\n",
        "                    \"Jane Austen\",\n",
        "                    \"Louisa May Alcott\",\n",
        "                    \"Mark Twain\",\n",
        "                    \"Herman Melville\"\n",
        "                ])\n",
        "    plt.title('')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Quantity')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Numerical Data: \\n\")\n",
        "\n",
        "for col in columns_num:\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    sns.kdeplot(data=Dataset_lora,\n",
        "                x=col,\n",
        "                hue='Author',\n",
        "                fill=True,\n",
        "                hue_order=[\n",
        "                    \"Charles Dickens\",\n",
        "                    \"Jane Austen\",\n",
        "                    \"Louisa May Alcott\",\n",
        "                    \"Mark Twain\",\n",
        "                    \"Herman Melville\"\n",
        "                ])\n",
        "    plt.title('')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Quantity')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "nFnW_5ojmcZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgEDf96i2-gm"
      },
      "outputs": [],
      "source": [
        "\"\"\"print(\"\\n Catgorical Data: \\n\")\n",
        "\n",
        "for col in columns_cat:\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    sns.histplot(data=Dataset, x=col, hue='Author', element='step', multiple='dodge')\n",
        "    plt.title('')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Quantity')\n",
        "    plt.show()\"\"\"\n",
        "pass"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "egzIVY7VioI2",
        "KEPdPc5wnnEI",
        "7oBgDGOwM9hS",
        "BxIyAfWdLIjk",
        "YWBi9C-n1Tow",
        "2uhEh2wtnA47",
        "QfwkDt_pH9Kd",
        "8IA7bDN4z8uD",
        "AIzeUJXaJsbr",
        "Bk6PDv7iINYP",
        "AmTi4SdKJEqH",
        "HONC7JIU1t_t"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
